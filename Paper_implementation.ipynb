{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b8ce67f6-0b00-448b-885d-b7d22bce4ff6",
      "metadata": {
        "id": "b8ce67f6-0b00-448b-885d-b7d22bce4ff6"
      },
      "source": [
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-george-hotz-quote.png\" width=600 alt=\"george hotz quote saying to get better at being a machine learning engineer, download a paper, implement it and keep going until you have skills\"/>\n",
        "\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebe46d77-6c4d-4102-9994-2cb89f633f18",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebe46d77-6c4d-4102-9994-2cb89f633f18",
        "outputId": "155b0037-7477-4c61-cc1f-0a4ccf42a760"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch version: 2.5.1+cu121\n",
            "torchvision version: 0.20.1+cu121\n"
          ]
        }
      ],
      "source": [
        "\n",
        "    import torch\n",
        "    import torchvision\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "    print(f\"torchvision version: {torchvision.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "960eb156-c1b1-4e76-a812-01bf045835bd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "960eb156-c1b1-4e76-a812-01bf045835bd",
        "outputId": "7583d229-ad85-4fe0-d159-752620d14645"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Couldn't find torchinfo... installing it.\n"
          ]
        }
      ],
      "source": [
        "# Continue with regular imports\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "# Try to get torchinfo, install it if it doesn't work\n",
        "try:\n",
        "    from torchinfo import summary\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "    !pip install -q torchinfo\n",
        "    from torchinfo import summary\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7878472-6680-43bf-981b-b7fa722ef9aa",
      "metadata": {
        "id": "a7878472-6680-43bf-981b-b7fa722ef9aa"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Contains functionality for creating PyTorch DataLoaders for\n",
        "image classification data using CIFAR-10.\n",
        "\"\"\"\n",
        "import os\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "def create_dataloaders(train_data, test_data,\n",
        "    transform: transforms.Compose,\n",
        "    batch_size: int,\n",
        "    num_workers: int=NUM_WORKERS\n",
        "):\n",
        "    \"\"\"Creates training and testing DataLoaders for CIFAR-10.\n",
        "\n",
        "    Args:\n",
        "        train_data: PyTorch Dataset object for training data (e.g., CIFAR10).\n",
        "        test_data: PyTorch Dataset object for testing data (e.g., CIFAR10).\n",
        "        transform: torchvision transforms to perform on training and testing data.\n",
        "        batch_size: Number of samples per batch in each of the DataLoaders.\n",
        "        num_workers: An integer for number of workers per DataLoader.\n",
        "\n",
        "    Returns:\n",
        "        A tuple of (train_dataloader, test_dataloader, class_names).\n",
        "        Where class_names is a list of the target classes.\n",
        "    \"\"\"\n",
        "    # Apply transforms to the datasets\n",
        "    train_data.transform = transform\n",
        "    test_data.transform = transform\n",
        "\n",
        "    # Get class names\n",
        "    class_names = train_data.classes\n",
        "\n",
        "    # Turn datasets into data loaders\n",
        "    train_dataloader = DataLoader(\n",
        "        train_data,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "    test_dataloader = DataLoader(\n",
        "        test_data,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "    return train_dataloader, test_dataloader, class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2170adf7-a5f9-46ac-ada6-99e64fb04f7b",
      "metadata": {
        "id": "2170adf7-a5f9-46ac-ada6-99e64fb04f7b"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Contains functions for training and testing a PyTorch model.\n",
        "\"\"\"\n",
        "import torch\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "def train_step(model: torch.nn.Module,\n",
        "               dataloader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               device: torch.device) -> Tuple[float, float]:\n",
        "    \"\"\"Trains a PyTorch model for a single epoch.\n",
        "\n",
        "    Turns a target PyTorch model to training mode and then\n",
        "    runs through all of the required training steps (forward\n",
        "    pass, loss calculation, optimizer step).\n",
        "\n",
        "    Args:\n",
        "    model: A PyTorch model to be trained.\n",
        "    dataloader: A DataLoader instance for the model to be trained on.\n",
        "    loss_fn: A PyTorch loss function to minimize.\n",
        "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "    Returns:\n",
        "    A tuple of training loss and training accuracy metrics.\n",
        "    In the form (train_loss, train_accuracy). For example:\n",
        "\n",
        "    (0.1112, 0.8743)\n",
        "    \"\"\"\n",
        "    # Put model in train mode\n",
        "    model.train()\n",
        "\n",
        "    # Setup train loss and train accuracy values\n",
        "    train_loss, train_acc = 0, 0\n",
        "\n",
        "    # Loop through data loader data batches\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Send data to target device\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # 1. Forward pass\n",
        "        y_pred = model(X)\n",
        "\n",
        "        # 2. Calculate  and accumulate loss\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # 3. Optimizer zero grad\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 4. Loss backward\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. Optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate and accumulate accuracy metric across all batches\n",
        "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
        "        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
        "\n",
        "    # Adjust metrics to get average loss and accuracy per batch\n",
        "    train_loss = train_loss / len(dataloader)\n",
        "    train_acc = train_acc / len(dataloader)\n",
        "    return train_loss, train_acc\n",
        "\n",
        "def test_step(model: torch.nn.Module,\n",
        "              dataloader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              device: torch.device) -> Tuple[float, float]:\n",
        "    \"\"\"Tests a PyTorch model for a single epoch.\n",
        "\n",
        "    Turns a target PyTorch model to \"eval\" mode and then performs\n",
        "    a forward pass on a testing dataset.\n",
        "\n",
        "    Args:\n",
        "    model: A PyTorch model to be tested.\n",
        "    dataloader: A DataLoader instance for the model to be tested on.\n",
        "    loss_fn: A PyTorch loss function to calculate loss on the test data.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "    Returns:\n",
        "    A tuple of testing loss and testing accuracy metrics.\n",
        "    In the form (test_loss, test_accuracy). For example:\n",
        "\n",
        "    (0.0223, 0.8985)\n",
        "    \"\"\"\n",
        "    # Put model in eval mode\n",
        "    model.eval()\n",
        "\n",
        "    # Setup test loss and test accuracy values\n",
        "    test_loss, test_acc = 0, 0\n",
        "\n",
        "    # Turn on inference context manager\n",
        "    with torch.inference_mode():\n",
        "        # Loop through DataLoader batches\n",
        "        for batch, (X, y) in enumerate(dataloader):\n",
        "            # Send data to target device\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            # 1. Forward pass\n",
        "            test_pred_logits = model(X)\n",
        "\n",
        "            # 2. Calculate and accumulate loss\n",
        "            loss = loss_fn(test_pred_logits, y)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            # Calculate and accumulate accuracy\n",
        "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
        "            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
        "\n",
        "    # Adjust metrics to get average loss and accuracy per batch\n",
        "    test_loss = test_loss / len(dataloader)\n",
        "    test_acc = test_acc / len(dataloader)\n",
        "    return test_loss, test_acc\n",
        "\n",
        "def train(model: torch.nn.Module,\n",
        "          train_dataloader: torch.utils.data.DataLoader,\n",
        "          test_dataloader: torch.utils.data.DataLoader,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          loss_fn: torch.nn.Module,\n",
        "          epochs: int,\n",
        "          device: torch.device) -> Dict[str, List]:\n",
        "    \"\"\"Trains and tests a PyTorch model.\n",
        "\n",
        "    Passes a target PyTorch models through train_step() and test_step()\n",
        "    functions for a number of epochs, training and testing the model\n",
        "    in the same epoch loop.\n",
        "\n",
        "    Calculates, prints and stores evaluation metrics throughout.\n",
        "\n",
        "    Args:\n",
        "    model: A PyTorch model to be trained and tested.\n",
        "    train_dataloader: A DataLoader instance for the model to be trained on.\n",
        "    test_dataloader: A DataLoader instance for the model to be tested on.\n",
        "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
        "    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
        "    epochs: An integer indicating how many epochs to train for.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "    Returns:\n",
        "    A dictionary of training and testing loss as well as training and\n",
        "    testing accuracy metrics. Each metric has a value in a list for\n",
        "    each epoch.\n",
        "    In the form: {train_loss: [...],\n",
        "              train_acc: [...],\n",
        "              test_loss: [...],\n",
        "              test_acc: [...]}\n",
        "    For example if training for epochs=2:\n",
        "             {train_loss: [2.0616, 1.0537],\n",
        "              train_acc: [0.3945, 0.3945],\n",
        "              test_loss: [1.2641, 1.5706],\n",
        "              test_acc: [0.3400, 0.2973]}\n",
        "    \"\"\"\n",
        "    # Create empty results dictionary\n",
        "    results = {\"train_loss\": [],\n",
        "               \"train_acc\": [],\n",
        "               \"test_loss\": [],\n",
        "               \"test_acc\": []\n",
        "    }\n",
        "\n",
        "    # Make sure model on target device\n",
        "    model.to(device)\n",
        "\n",
        "    # Loop through training and testing steps for a number of epochs\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        train_loss, train_acc = train_step(model=model,\n",
        "                                          dataloader=train_dataloader,\n",
        "                                          loss_fn=loss_fn,\n",
        "                                          optimizer=optimizer,\n",
        "                                          device=device)\n",
        "        test_loss, test_acc = test_step(model=model,\n",
        "          dataloader=test_dataloader,\n",
        "          loss_fn=loss_fn,\n",
        "          device=device)\n",
        "\n",
        "        # Print out what's happening\n",
        "        print(\n",
        "          f\"Epoch: {epoch+1} | \"\n",
        "          f\"train_loss: {train_loss:.4f} | \"\n",
        "          f\"train_acc: {train_acc:.4f} | \"\n",
        "          f\"test_loss: {test_loss:.4f} | \"\n",
        "          f\"test_acc: {test_acc:.4f}\"\n",
        "        )\n",
        "\n",
        "        # Update results dictionary\n",
        "        results[\"train_loss\"].append(train_loss)\n",
        "        results[\"train_acc\"].append(train_acc)\n",
        "        results[\"test_loss\"].append(test_loss)\n",
        "        results[\"test_acc\"].append(test_acc)\n",
        "\n",
        "    # Return the filled results at the end of the epochs\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a8f7425-d490-468b-b908-2b03af1e8aec",
      "metadata": {
        "id": "6a8f7425-d490-468b-b908-2b03af1e8aec"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Contains various utility functions for PyTorch model training and saving.\n",
        "\"\"\"\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "def save_model(model: torch.nn.Module,\n",
        "               target_dir: str,\n",
        "               model_name: str):\n",
        "    \"\"\"Saves a PyTorch model to a target directory.\n",
        "\n",
        "    Args:\n",
        "    model: A target PyTorch model to save.\n",
        "    target_dir: A directory for saving the model to.\n",
        "    model_name: A filename for the saved model. Should include\n",
        "      either \".pth\" or \".pt\" as the file extension.\n",
        "\n",
        "    Example usage:\n",
        "    save_model(model=model_0,\n",
        "               target_dir=\"models\",\n",
        "               model_name=\"05_going_modular_tingvgg_model.pth\")\n",
        "    \"\"\"\n",
        "    # Create target directory\n",
        "    target_dir_path = Path(target_dir)\n",
        "    target_dir_path.mkdir(parents=True,\n",
        "                        exist_ok=True)\n",
        "\n",
        "    # Create model save path\n",
        "    assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n",
        "    model_save_path = target_dir_path / model_name\n",
        "\n",
        "    # Save the model state_dict()\n",
        "    print(f\"[INFO] Saving model to: {model_save_path}\")\n",
        "    torch.save(obj=model.state_dict(),\n",
        "             f=model_save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "daa87536-2317-4c51-bdac-f098f23a0ff0",
      "metadata": {
        "id": "daa87536-2317-4c51-bdac-f098f23a0ff0"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "A series of helper functions used throughout the course.\n",
        "\n",
        "If a function gets defined once and could be used over and over, it'll go in here.\n",
        "\"\"\"\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import requests\n",
        "\n",
        "# Walk through an image classification directory and find out how many files (images)\n",
        "# are in each subdirectory.\n",
        "import os\n",
        "\n",
        "def walk_through_dir(dir_path):\n",
        "    \"\"\"\n",
        "    Walks through dir_path returning its contents.\n",
        "    Args:\n",
        "    dir_path (str): target directory\n",
        "\n",
        "    Returns:\n",
        "    A print out of:\n",
        "      number of subdiretories in dir_path\n",
        "      number of images (files) in each subdirectory\n",
        "      name of each subdirectory\n",
        "    \"\"\"\n",
        "    for dirpath, dirnames, filenames in os.walk(dir_path):\n",
        "        print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")\n",
        "\n",
        "def plot_decision_boundary(model: torch.nn.Module, X: torch.Tensor, y: torch.Tensor):\n",
        "    \"\"\"Plots decision boundaries of model predicting on X in comparison to y.\n",
        "\n",
        "    Source - https://madewithml.com/courses/foundations/neural-networks/ (with modifications)\n",
        "    \"\"\"\n",
        "    # Put everything to CPU (works better with NumPy + Matplotlib)\n",
        "    model.to(\"cpu\")\n",
        "    X, y = X.to(\"cpu\"), y.to(\"cpu\")\n",
        "\n",
        "    # Setup prediction boundaries and grid\n",
        "    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
        "    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 101), np.linspace(y_min, y_max, 101))\n",
        "\n",
        "    # Make features\n",
        "    X_to_pred_on = torch.from_numpy(np.column_stack((xx.ravel(), yy.ravel()))).float()\n",
        "\n",
        "    # Make predictions\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        y_logits = model(X_to_pred_on)\n",
        "\n",
        "    # Test for multi-class or binary and adjust logits to prediction labels\n",
        "    if len(torch.unique(y)) > 2:\n",
        "        y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1)  # mutli-class\n",
        "    else:\n",
        "        y_pred = torch.round(torch.sigmoid(y_logits))  # binary\n",
        "\n",
        "    # Reshape preds and plot\n",
        "    y_pred = y_pred.reshape(xx.shape).detach().numpy()\n",
        "    plt.contourf(xx, yy, y_pred, cmap=plt.cm.RdYlBu, alpha=0.7)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)\n",
        "    plt.xlim(xx.min(), xx.max())\n",
        "    plt.ylim(yy.min(), yy.max())\n",
        "\n",
        "\n",
        "# Plot linear data or training and test and predictions (optional)\n",
        "def plot_predictions(\n",
        "    train_data, train_labels, test_data, test_labels, predictions=None\n",
        "):\n",
        "    \"\"\"\n",
        "  Plots linear training data and test data and compares predictions.\n",
        "  \"\"\"\n",
        "    plt.figure(figsize=(10, 7))\n",
        "\n",
        "    # Plot training data in blue\n",
        "    plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")\n",
        "\n",
        "    # Plot test data in green\n",
        "    plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")\n",
        "\n",
        "    if predictions is not None:\n",
        "        # Plot the predictions in red (predictions were made on the test data)\n",
        "        plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")\n",
        "\n",
        "    # Show the legend\n",
        "    plt.legend(prop={\"size\": 14})\n",
        "\n",
        "\n",
        "# Calculate accuracy (a classification metric)\n",
        "def accuracy_fn(y_true, y_pred):\n",
        "    \"\"\"Calculates accuracy between truth labels and predictions.\n",
        "\n",
        "    Args:\n",
        "        y_true (torch.Tensor): Truth labels for predictions.\n",
        "        y_pred (torch.Tensor): Predictions to be compared to predictions.\n",
        "\n",
        "    Returns:\n",
        "        [torch.float]: Accuracy value between y_true and y_pred, e.g. 78.45\n",
        "    \"\"\"\n",
        "    correct = torch.eq(y_true, y_pred).sum().item()\n",
        "    acc = (correct / len(y_pred)) * 100\n",
        "    return acc\n",
        "\n",
        "\n",
        "def print_train_time(start, end, device=None):\n",
        "    \"\"\"Prints difference between start and end time.\n",
        "\n",
        "    Args:\n",
        "        start (float): Start time of computation (preferred in timeit format).\n",
        "        end (float): End time of computation.\n",
        "        device ([type], optional): Device that compute is running on. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        float: time between start and end in seconds (higher is longer).\n",
        "    \"\"\"\n",
        "    total_time = end - start\n",
        "    print(f\"\\nTrain time on {device}: {total_time:.3f} seconds\")\n",
        "    return total_time\n",
        "\n",
        "\n",
        "# Plot loss curves of a model\n",
        "def plot_loss_curves(results):\n",
        "    \"\"\"Plots training curves of a results dictionary.\n",
        "\n",
        "    Args:\n",
        "        results (dict): dictionary containing list of values, e.g.\n",
        "            {\"train_loss\": [...],\n",
        "             \"train_acc\": [...],\n",
        "             \"test_loss\": [...],\n",
        "             \"test_acc\": [...]}\n",
        "    \"\"\"\n",
        "    loss = results[\"train_loss\"]\n",
        "    test_loss = results[\"test_loss\"]\n",
        "\n",
        "    accuracy = results[\"train_acc\"]\n",
        "    test_accuracy = results[\"test_acc\"]\n",
        "\n",
        "    epochs = range(len(results[\"train_loss\"]))\n",
        "\n",
        "    plt.figure(figsize=(15, 7))\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, loss, label=\"train_loss\")\n",
        "    plt.plot(epochs, test_loss, label=\"test_loss\")\n",
        "    plt.title(\"Loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, accuracy, label=\"train_accuracy\")\n",
        "    plt.plot(epochs, test_accuracy, label=\"test_accuracy\")\n",
        "    plt.title(\"Accuracy\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.legend()\n",
        "\n",
        "\n",
        "# Pred and plot image function from notebook 04\n",
        "# See creation: https://www.learnpytorch.io/04_pytorch_custom_datasets/#113-putting-custom-image-prediction-together-building-a-function\n",
        "from typing import List\n",
        "import torchvision\n",
        "\n",
        "\n",
        "def pred_and_plot_image(\n",
        "    model: torch.nn.Module,\n",
        "    image_path: str,\n",
        "    class_names: List[str] = None,\n",
        "    transform=None,\n",
        "    device: torch.device = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "):\n",
        "    \"\"\"Makes a prediction on a target image with a trained model and plots the image.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): trained PyTorch image classification model.\n",
        "        image_path (str): filepath to target image.\n",
        "        class_names (List[str], optional): different class names for target image. Defaults to None.\n",
        "        transform (_type_, optional): transform of target image. Defaults to None.\n",
        "        device (torch.device, optional): target device to compute on. Defaults to \"cuda\" if torch.cuda.is_available() else \"cpu\".\n",
        "\n",
        "    Returns:\n",
        "        Matplotlib plot of target image and model prediction as title.\n",
        "\n",
        "    Example usage:\n",
        "        pred_and_plot_image(model=model,\n",
        "                            image=\"some_image.jpeg\",\n",
        "                            class_names=[\"class_1\", \"class_2\", \"class_3\"],\n",
        "                            transform=torchvision.transforms.ToTensor(),\n",
        "                            device=device)\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Load in image and convert the tensor values to float32\n",
        "    target_image = torchvision.io.read_image(str(image_path)).type(torch.float32)\n",
        "\n",
        "    # 2. Divide the image pixel values by 255 to get them between [0, 1]\n",
        "    target_image = target_image / 255.0\n",
        "\n",
        "    # 3. Transform if necessary\n",
        "    if transform:\n",
        "        target_image = transform(target_image)\n",
        "\n",
        "    # 4. Make sure the model is on the target device\n",
        "    model.to(device)\n",
        "\n",
        "    # 5. Turn on model evaluation mode and inference mode\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        # Add an extra dimension to the image\n",
        "        target_image = target_image.unsqueeze(dim=0)\n",
        "\n",
        "        # Make a prediction on image with an extra dimension and send it to the target device\n",
        "        target_image_pred = model(target_image.to(device))\n",
        "\n",
        "    # 6. Convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)\n",
        "    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n",
        "\n",
        "    # 7. Convert prediction probabilities -> prediction labels\n",
        "    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n",
        "\n",
        "    # 8. Plot the image alongside the prediction and prediction probability\n",
        "    plt.imshow(\n",
        "        target_image.squeeze().permute(1, 2, 0)\n",
        "    )  # make sure it's the right size for matplotlib\n",
        "    if class_names:\n",
        "        title = f\"Pred: {class_names[target_image_pred_label.cpu()]} | Prob: {target_image_pred_probs.max().cpu():.3f}\"\n",
        "    else:\n",
        "        title = f\"Pred: {target_image_pred_label} | Prob: {target_image_pred_probs.max().cpu():.3f}\"\n",
        "    plt.title(title)\n",
        "    plt.axis(False)\n",
        "\n",
        "def set_seeds(seed: int=42):\n",
        "    \"\"\"Sets random sets for torch operations.\n",
        "\n",
        "    Args:\n",
        "        seed (int, optional): Random seed to set. Defaults to 42.\n",
        "    \"\"\"\n",
        "    # Set the seed for general torch operations\n",
        "    torch.manual_seed(seed)\n",
        "    # Set the seed for CUDA torch operations (ones that happen on the GPU)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "def download_data(source: str,\n",
        "                  destination: str,\n",
        "                  remove_source: bool = True) -> Path:\n",
        "    \"\"\"Downloads a zipped dataset from source and unzips to destination.\n",
        "\n",
        "    Args:\n",
        "        source (str): A link to a zipped file containing data.\n",
        "        destination (str): A target directory to unzip data to.\n",
        "        remove_source (bool): Whether to remove the source after downloading and extracting.\n",
        "\n",
        "    Returns:\n",
        "        pathlib.Path to downloaded data.\n",
        "\n",
        "    Example usage:\n",
        "        download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
        "                      destination=\"pizza_steak_sushi\")\n",
        "    \"\"\"\n",
        "    # Setup path to data folder\n",
        "    data_path = Path(\"data/\")\n",
        "    image_path = data_path / destination\n",
        "\n",
        "    # If the image folder doesn't exist, download it and prepare it...\n",
        "    if image_path.is_dir():\n",
        "        print(f\"[INFO] {image_path} directory exists, skipping download.\")\n",
        "    else:\n",
        "        print(f\"[INFO] Did not find {image_path} directory, creating one...\")\n",
        "        image_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Download pizza, steak, sushi data\n",
        "        target_file = Path(source).name\n",
        "        with open(data_path / target_file, \"wb\") as f:\n",
        "            request = requests.get(source)\n",
        "            print(f\"[INFO] Downloading {target_file} from {source}...\")\n",
        "            f.write(request.content)\n",
        "\n",
        "        # Unzip pizza, steak, sushi data\n",
        "        with zipfile.ZipFile(data_path / target_file, \"r\") as zip_ref:\n",
        "            print(f\"[INFO] Unzipping {target_file} data...\")\n",
        "            zip_ref.extractall(image_path)\n",
        "\n",
        "        # Remove .zip file\n",
        "        if remove_source:\n",
        "            os.remove(data_path / target_file)\n",
        "\n",
        "    return image_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e246f92-e509-474e-b6c7-c82cf11cb8ca",
      "metadata": {
        "id": "5e246f92-e509-474e-b6c7-c82cf11cb8ca",
        "outputId": "d9c6ce27-976b-4856-9e0a-4591a1d0589a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7336d3fb-8fc0-4f1e-902c-46591bb4be8f",
      "metadata": {
        "id": "7336d3fb-8fc0-4f1e-902c-46591bb4be8f"
      },
      "outputs": [],
      "source": [
        "# What i first tried was to actually not use the cifar dataset in the torchvision datasets library and make download it from the official cifar-10 repo but unpickling it was way too much of a hassle\n",
        "\n",
        "def unpickle(file):\n",
        "    import pickle\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37b5ffc0-7093-481e-8081-dbdfac4c24f0",
      "metadata": {
        "id": "37b5ffc0-7093-481e-8081-dbdfac4c24f0",
        "outputId": "4b888a11-1b07-4f77-e757-77b46bb4a2da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 42.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torchvision\n",
        "\n",
        "# Download training data\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                      download=True)\n",
        "\n",
        "# Download test data\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                     download=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55a047b1-9f12-4dcf-8d97-83b7cbb37392",
      "metadata": {
        "id": "55a047b1-9f12-4dcf-8d97-83b7cbb37392"
      },
      "source": [
        "Beautiful! Data downloaded, let's setup the training and test directories."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6fce58f-0f0b-48ef-a9b6-1ef3ca09380d",
      "metadata": {
        "id": "d6fce58f-0f0b-48ef-a9b6-1ef3ca09380d"
      },
      "source": [
        "## 2. Create Datasets and DataLoaders\n",
        "\n",
        "\n",
        "\n",
        "In Table 3, the training resolution is mentioned as being 224 (height=224, width=224), but as the cifar-10 dataset uses 32x32 images, augmenting then to 224x 224 may introduce unexpected behaviours so in the final implementation i decided to just go with 32 x 32 res\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "And since I'll be training my model from scratch (no transfer learning to begin with), I technically shouldnt  provide a `normalize` transform but at the end i decided to normalize the dataset as it resulted in a substatial leap in perf\n",
        "\n",
        "### 2.1 Prepare transforms for images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a45ea650-c3fa-479c-8767-48bc3a1f1267",
      "metadata": {
        "id": "a45ea650-c3fa-479c-8767-48bc3a1f1267",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8dda384-0942-4522-ebb2-20742497c9df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manually created transforms: Compose(\n",
            "    RandomCrop(size=(32, 32), padding=4)\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    ToTensor()\n",
            "    Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Create image size (from Table 3 in the ViT paper)\n",
        "IMG_SIZE = 32\n",
        "\n",
        "manual_transforms = transforms.Compose([\n",
        "    #transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010]) # standard values for cifar-10 dataset\n",
        "\n",
        "])\n",
        "print(f\"Manually created transforms: {manual_transforms}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92a426b6-df22-4a58-9d5e-b382c73c6048",
      "metadata": {
        "id": "92a426b6-df22-4a58-9d5e-b382c73c6048"
      },
      "outputs": [],
      "source": [
        "# Setup directory paths to train and test images\n",
        "train_dir = trainset\n",
        "test_dir = testset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "437078c2-eb42-471f-8561-94845d0a878d",
      "metadata": {
        "id": "437078c2-eb42-471f-8561-94845d0a878d"
      },
      "source": [
        "### 2.2 Turn images into `DataLoader`'s\n",
        "\n",
        "\n",
        "The ViT paper states the use of a batch size of 4096 which is 128x the size of the batch size I've been using (32) because of the GPU VRAM limitations on my machine.\n",
        "\n",
        "Im using `pin_memory = True` although cifar-10 is not the biggest of datasets, i still believe the pros almost always outweigh the cons\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0ac8145-f89a-490f-82e3-d4b22225d163",
      "metadata": {
        "id": "d0ac8145-f89a-490f-82e3-d4b22225d163",
        "outputId": "7928d7cb-1502-4c68-d6e3-8796e03f93b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7a3a582e67d0>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x7a3a5830db50>,\n",
              " ['airplane',\n",
              "  'automobile',\n",
              "  'bird',\n",
              "  'cat',\n",
              "  'deer',\n",
              "  'dog',\n",
              "  'frog',\n",
              "  'horse',\n",
              "  'ship',\n",
              "  'truck'])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Set the batch size\n",
        "BATCH_SIZE = 32# this is lower than the ViT paper\n",
        "\n",
        "# Create data loaders\n",
        "train_dataloader, test_dataloader, class_names = create_dataloaders(\n",
        "    train_data=train_dir,\n",
        "    test_data=test_dir,\n",
        "    transform=manual_transforms, # use manually created transforms\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "train_dataloader, test_dataloader, class_names"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a980a5b-0c3b-440d-87f5-c54f1ab143ab",
      "metadata": {
        "id": "4a980a5b-0c3b-440d-87f5-c54f1ab143ab"
      },
      "source": [
        "### 2.3 Visualize a single image\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5734a22-ded5-403e-84f5-d7a90ed3f085",
      "metadata": {
        "id": "b5734a22-ded5-403e-84f5-d7a90ed3f085",
        "outputId": "9c89c783-7a76-458e-ad40-07ab7fc868b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([3, 32, 32]), tensor(5))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Get a batch of images\n",
        "image_batch, label_batch = next(iter(train_dataloader))\n",
        "\n",
        "# Get a single image from the batch\n",
        "image, label = image_batch[0], label_batch[0]\n",
        "\n",
        "# View the batch shapes\n",
        "image.shape, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afe85fae-38fd-4f34-a52c-29d02cce09c1",
      "metadata": {
        "id": "afe85fae-38fd-4f34-a52c-29d02cce09c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "outputId": "07701256-fe7a-43fa-bb31-c641dd1f24b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.4290657..2.3590088].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "# Plot image with matplotlib\n",
        "plt.imshow(image.permute(1, 2, 0)) # rearrange image dimensions to suit matplotlib [color_channels, height, width] -> [height, width, color_channels]\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False);\n",
        "\n",
        "## I know matplotlib is not exactly happy about being passed in normalized images but im too lazy to fix it plus it doesnt actually matter for the actual model training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c90c4c0-0039-4790-a25b-f14754e6b468",
      "metadata": {
        "id": "4c90c4c0-0039-4790-a25b-f14754e6b468"
      },
      "source": [
        "#### 3.2.1 Figure 1\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-figure-1-inputs-and-outputs.png\" width=900 alt=\"figure 1 from the original vision transformer paper\"/>\n",
        "\n",
        "\n",
        "\n",
        "The ViT architecture is comprised of several stages:\n",
        "* **Patch + Position Embedding (inputs)** - Turns the input image into a sequence of image patches and adds a position number to specify in what order the patch comes in.\n",
        "* **Linear projection of flattened patches (Embedded Patches)** - The image patches get turned into an **embedding**, the benefit of using an embedding rather than just the image values is that an embedding is a *learnable* representation (typically in the form of a vector) of the image that can improve with training.\n",
        "* **Norm** - This is short for \"[Layer Normalization](https://paperswithcode.com/method/layer-normalization)\" or \"LayerNorm\", a technique for regularizing (reducing overfitting) a neural network, you can use LayerNorm via the PyTorch layer [`torch.nn.LayerNorm()`](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html).\n",
        "* **Multi-Head Attention** - This is a [Multi-Headed Self-Attention layer](https://paperswithcode.com/method/multi-head-attention) or \"MSA\" for short. You can create an MSA layer via the PyTorch layer [`torch.nn.MultiheadAttention()`](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html).\n",
        "* **MLP (or [Multilayer perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron))** - A MLP can often refer to any collection of feedforward layers (or in PyTorch's case, a collection of layers with a `forward()` method). In the ViT Paper, the authors refer to the MLP as \"MLP block\" and it contains two [`torch.nn.Linear()`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) layers with a [`torch.nn.GELU()`](https://pytorch.org/docs/stable/generated/torch.nn.GELU.html) non-linearity activation in between them (section 3.1) and a [`torch.nn.Dropout()`](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html) layer after each (Appendix B.1).\n",
        "* **Transformer Encoder** - The Transformer Encoder, is a collection of the layers listed above. There are two skip connections inside the Transformer encoder (the \"+\" symbols) meaning the layer's inputs are fed directly to immediate layers as well as subsequent layers. The overall ViT architecture is comprised of a number of Transformer encoders stacked on top of eachother.\n",
        "* **MLP Head** - This is the output layer of the architecture, it converts the learned features of an input to a class output. Since we're working on image classification, you could also call this the \"classifier head\". The structure of the MLP Head is similar to the MLP block.\n",
        "\n",
        "###Note:\n",
        "The above textual explaination is not my intellectual property. It has just been added to give the reader some context of what I am tryna rebuild\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ee6c9bf-e40f-4511-9325-498251f5b998",
      "metadata": {
        "id": "7ee6c9bf-e40f-4511-9325-498251f5b998"
      },
      "source": [
        "### 4.2 Turning a image into patches\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "336e1b36-9849-4104-8cb9-bb64a20ffc48",
      "metadata": {
        "id": "336e1b36-9849-4104-8cb9-bb64a20ffc48",
        "outputId": "5f079f4f-2dc8-426e-fe7c-60bced79e5f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.4290657..2.3590088].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# View single image\n",
        "plt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcd2e784-7989-40e5-b8f5-64de18f1fe3d",
      "metadata": {
        "id": "bcd2e784-7989-40e5-b8f5-64de18f1fe3d",
        "outputId": "3fd196fe-fb34-4c76-9632-5d92de0b04aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.4290657..2.3590088].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x1600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Change image shape to be compatible with matplotlib (color_channels, height, width) -> (height, width, color_channels)\n",
        "image_permuted = image.permute(1, 2, 0)\n",
        "\n",
        "# Index to plot the top row of patched pixels\n",
        "patch_size = 16\n",
        "plt.figure(figsize=(patch_size, patch_size))\n",
        "plt.imshow(image_permuted[:patch_size, :, :]);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d45e15a-eb50-4c46-8055-2acaaacb881c",
      "metadata": {
        "id": "7d45e15a-eb50-4c46-8055-2acaaacb881c",
        "outputId": "76c3596a-89d7-4608-cffe-315d57e5ec40",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of patches per row: 8.0        \n",
            "Number of patches per column: 8.0        \n",
            "Total patches: 64.0        \n",
            "Patch size: 4 pixels x 4 pixels\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.4290657..0.32359254].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.4290657..-0.89765733].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.4290657..-1.3047407].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.4290657..-1.925058].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.4290657..-0.5293439].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.4290657..-1.8862882].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.4290657..-2.1823723].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.4290657..-2.221393].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.084821..1.8550009].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.2018826..1.8743858].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.1823723..1.6611518].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.221393..-1.1496612].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.221393..2.0876198].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.3002539..1.9325405].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.4182549..0.110358454].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.4290657..-2.221393].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.9677593..1.7774612].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.084821..1.7580763].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.2458785..2.0294652].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.221393..2.04885].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.148327..2.262084].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [1.1733978..2.3590088].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.221393..2.2233143].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.4290657..-2.221393].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.280587..-0.0059511834].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.378921..0.88575506].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.948249..1.9325405].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.44595647..2.1845443].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.5775534..1.8937707].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [1.2124183..2.1845443].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.1823723..2.1845443].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.4290657..-2.221393].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.4182549..0.80821544].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.4182549..1.1183741].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.753146..1.2540685].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.5970637..1.8550009].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.6946151..1.1959138].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.17837283..2.04885].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.4800019..2.1651595].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.4290657..-2.221393].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.2653887..0.73067576].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.3434299..0.88575506].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.0312653..1.1959138].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.4800019..1.9325405].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.1043313..0.7500607].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.0458004..1.350993].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.221393..1.6417668].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.4290657..-2.221393].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.1238415..0.16851321].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.0458004..0.55621153].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.889718..1.1959138].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.2068578..1.5254573].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.1043313..0.63375115].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.00678..1.0408344].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.2018826..1.1183741].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.4290657..-2.221393].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.162862..-0.8449072].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.3709111..-0.8055735].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.4182549..0.73067576].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.9092283..1.0989891].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.162862..0.6143663].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.1823723..0.71129084].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.1823723..-0.27456877].\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.4290657..-2.221393].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 64 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Setup hyperparameters and make sure img_size and patch_size are compatible\n",
        "img_size = 32\n",
        "patch_size = 4\n",
        "num_patches = img_size/patch_size\n",
        "assert img_size % patch_size == 0, \"Image size must be divisible by patch size\"\n",
        "print(f\"Number of patches per row: {num_patches}\\\n",
        "        \\nNumber of patches per column: {num_patches}\\\n",
        "        \\nTotal patches: {num_patches*num_patches}\\\n",
        "        \\nPatch size: {patch_size} pixels x {patch_size} pixels\")\n",
        "\n",
        "# Create a series of subplots\n",
        "fig, axs = plt.subplots(nrows=img_size // patch_size, # need int not float\n",
        "                        ncols=img_size // patch_size,\n",
        "                        figsize=(num_patches, num_patches),\n",
        "                        sharex=True,\n",
        "                        sharey=True)\n",
        "\n",
        "# Loop through height and width of image\n",
        "for i, patch_height in enumerate(range(0, img_size, patch_size)): # iterate through height\n",
        "    for j, patch_width in enumerate(range(0, img_size, patch_size)): # iterate through width\n",
        "\n",
        "        # Plot the permuted image patch (image_permuted -> (Height, Width, Color Channels))\n",
        "        axs[i, j].imshow(image_permuted[patch_height:patch_height+patch_size, # iterate through height\n",
        "                                        patch_width:patch_width+patch_size, # iterate through width\n",
        "                                        :]) # get all color channels\n",
        "\n",
        "        # Set up label information, remove the ticks for clarity and set labels to outside\n",
        "        axs[i, j].set_ylabel(i+1,\n",
        "                             rotation=\"horizontal\",\n",
        "                             horizontalalignment=\"right\",\n",
        "                             verticalalignment=\"center\")\n",
        "        axs[i, j].set_xlabel(j+1)\n",
        "        axs[i, j].set_xticks([])\n",
        "        axs[i, j].set_yticks([])\n",
        "        axs[i, j].label_outer()\n",
        "\n",
        "# Set a super title\n",
        "fig.suptitle(f\"{class_names[label]} -> Patchified\", fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f774b58d-7095-4272-aba3-fd9a2db4f28f",
      "metadata": {
        "id": "f774b58d-7095-4272-aba3-fd9a2db4f28f"
      },
      "source": [
        "### 4.3 Creating image patches using conv2d\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d4fd046-6b51-4ac0-8d39-e67fb333a18a",
      "metadata": {
        "id": "3d4fd046-6b51-4ac0-8d39-e67fb333a18a"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "# Set the patch size\n",
        "patch_size=2 ## The patch size is smaller than the ViT paper cause our image dataset has smaller images\n",
        "\n",
        "# Create the Conv2d layer with hyperparameters from the ViT paper\n",
        "conv2d = nn.Conv2d(in_channels=3, # number of color channels\n",
        "                   out_channels=768, # from Table 1: Hidden size D, this is the embedding size\n",
        "                   kernel_size=patch_size, # could also use (patch_size, patch_size)\n",
        "                   stride=patch_size,\n",
        "                   padding=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03dec513-eea5-4d13-b7ab-41c9e997ef48",
      "metadata": {
        "id": "03dec513-eea5-4d13-b7ab-41c9e997ef48"
      },
      "source": [
        "Now we've got a convolutional layer, let's see what happens when we pass a single image through it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a72a5614-f1cb-4c01-9696-24f07bf2a219",
      "metadata": {
        "id": "a72a5614-f1cb-4c01-9696-24f07bf2a219",
        "outputId": "66f7629d-9066-4e94-cee3-08eaa7d35424",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 768, 16, 16])\n"
          ]
        }
      ],
      "source": [
        "# Pass the image through the convolutional layer\n",
        "image_out_of_conv = conv2d(image.unsqueeze(0)) # add a single batch dimension (height, width, color_channels) -> (batch, height, width, color_channels)\n",
        "print(image_out_of_conv.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af5b58ca-0d73-4c62-b4af-4e8867b764e2",
      "metadata": {
        "id": "af5b58ca-0d73-4c62-b4af-4e8867b764e2",
        "outputId": "4d8fb616-fa7c-4446-b3cd-f3ee29633962",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Showing random convolutional feature maps from indexes: [680, 456, 365, 326, 714]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1200 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Plot random 5 convolutional feature maps\n",
        "import random\n",
        "random_indexes = random.sample(range(0, 758), k=5) # pick 5 numbers between 0 and the embedding size\n",
        "print(f\"Showing random convolutional feature maps from indexes: {random_indexes}\")\n",
        "\n",
        "# Create plot\n",
        "fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(12, 12))\n",
        "\n",
        "# Plot random image feature maps\n",
        "for i, idx in enumerate(random_indexes):\n",
        "    image_conv_feature_map = image_out_of_conv[:, idx, :, :] # index on the output tensor of the convolutional layer\n",
        "    axs[i].imshow(image_conv_feature_map.squeeze().detach().numpy())\n",
        "    axs[i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[]);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b847f2e1-1700-4040-a9aa-df9ab1139cce",
      "metadata": {
        "id": "b847f2e1-1700-4040-a9aa-df9ab1139cce"
      },
      "source": [
        "These feature maps can be considered a **learnable embedding** of our image.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94f6f5b9-a1c7-4aa1-9780-7cd06457b2b3",
      "metadata": {
        "id": "94f6f5b9-a1c7-4aa1-9780-7cd06457b2b3",
        "outputId": "33d9c668-4e6d-45e6-9651-54e19ef0631a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[ 1.4093,  1.4093,  1.4093,  1.4093,  1.4093,  1.4093,  1.4093,\n",
              "            1.4093,  1.4093,  1.4093,  1.4093,  1.4093,  1.4093,  1.4093,\n",
              "            1.4093,  1.4093],\n",
              "          [ 0.2182,  0.1350,  1.3921,  1.0936,  0.9934,  1.3139,  1.4069,\n",
              "            1.3650,  1.3986,  0.4693,  1.5130,  1.3014,  1.4480,  1.3781,\n",
              "            1.4093,  1.4093],\n",
              "          [-0.1016,  0.4778,  0.9850,  0.7579,  0.5773,  1.2884,  0.8851,\n",
              "            0.9378,  0.3922, -0.2659,  0.4152,  1.3772,  1.3490,  1.3276,\n",
              "            1.4093,  1.4093],\n",
              "          [-0.7510, -0.4331, -0.5449, -0.6491, -0.1261,  0.9690,  1.0967,\n",
              "            0.9999, -0.8914, -0.7665, -0.5348,  0.4999,  0.8395,  0.2441,\n",
              "            1.4093,  1.4093],\n",
              "          [-0.1448, -0.5575, -0.3809, -0.4015, -0.5788, -0.0082,  0.7275,\n",
              "            0.5236, -0.9473, -0.8928, -1.0462, -0.7542,  0.6621,  0.6131,\n",
              "            1.4093,  1.4093],\n",
              "          [ 0.4186,  0.5396,  0.1793, -0.5094, -0.0391, -0.8140, -0.6026,\n",
              "           -0.6143, -0.0492, -0.9745, -0.9565, -0.9494,  0.0420,  0.7540,\n",
              "            1.4093,  1.4093],\n",
              "          [ 0.7380,  0.7600,  0.8436,  0.3630,  0.1518, -0.7555, -0.6871,\n",
              "           -0.5212, -0.1070, -0.7384, -0.9249, -0.7976, -0.5327,  0.5486,\n",
              "            1.4093,  1.4093],\n",
              "          [ 0.4508,  1.4874,  1.2350, -0.3153,  0.5232, -0.4210, -0.7846,\n",
              "           -0.3714,  0.1403, -0.5164, -0.8868, -0.9301, -0.7005,  0.3250,\n",
              "            1.4093,  1.4093],\n",
              "          [ 0.5634,  1.1950,  1.0913, -0.3287,  0.1825, -0.5404, -0.7141,\n",
              "           -0.2992,  0.2206, -0.3017, -0.7088, -0.8285, -0.6685,  0.4319,\n",
              "            1.4093,  1.4093],\n",
              "          [ 0.0661,  0.5675,  0.3276, -0.3882, -0.4482, -0.3250, -0.6985,\n",
              "           -0.0622,  0.1337, -0.0965, -0.5040, -0.6660, -0.6591,  0.4038,\n",
              "            1.4093,  1.4093],\n",
              "          [-0.1474,  0.3003, -0.0401, -0.1976, -0.1732, -0.1776, -0.7886,\n",
              "            0.0281,  0.2213,  0.0608, -0.3081, -0.4916, -0.5813,  0.4906,\n",
              "            1.4093,  1.4093],\n",
              "          [-0.1869,  0.3488,  0.0172, -0.1662, -0.0344, -0.2998, -0.5582,\n",
              "            0.0963,  0.2719,  0.1123, -0.0620, -0.2819, -0.5066,  0.4564,\n",
              "            1.4093,  1.4093],\n",
              "          [ 0.2729,  0.2638,  0.3436,  0.0130, -0.1390, -0.3866, -0.3451,\n",
              "            0.2396,  0.3478,  0.0167, -0.0338, -0.3544, -0.2644,  0.2940,\n",
              "            1.4093,  1.4093],\n",
              "          [ 0.6250,  0.7355,  0.6666,  1.0650,  0.1045, -0.2876, -0.5242,\n",
              "            0.3685,  0.5615, -0.0949,  0.0203, -0.1800,  0.2413,  0.3389,\n",
              "            1.4093,  1.4093],\n",
              "          [ 0.6920,  0.8695,  1.0942,  1.5626,  0.8106, -0.1500, -0.2962,\n",
              "            0.6666,  0.8744, -0.3839,  0.2417,  0.3024,  0.3545,  0.0883,\n",
              "            1.4093,  1.4093],\n",
              "          [ 0.5306,  0.6652,  0.6705,  1.2634,  0.6481, -0.1873,  0.2391,\n",
              "            0.5632,  0.2880,  0.0674,  0.5001,  0.6221,  0.5097,  0.1496,\n",
              "            1.4093,  1.4093]]], grad_fn=<SliceBackward0>),\n",
              " True)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "# Get a single feature map in tensor form\n",
        "single_feature_map = image_out_of_conv[:, 0, :, :]\n",
        "single_feature_map, single_feature_map.requires_grad"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc7c08ca-a4ef-4350-b471-088b2f12b80e",
      "metadata": {
        "id": "fc7c08ca-a4ef-4350-b471-088b2f12b80e"
      },
      "source": [
        "The `grad_fn` output of the `single_feature_map` and the `requires_grad=True` attribute means PyTorch is tracking the gradients of this feature map and it will be updated by gradient descent during training."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "572ae1c5-9488-4882-bdc1-409eef95424e",
      "metadata": {
        "id": "572ae1c5-9488-4882-bdc1-409eef95424e"
      },
      "source": [
        "### 4.4 Flattening the patch embedding with `torch.nn.Flatten()`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8219029-6162-4046-8702-0c2cb42f2378",
      "metadata": {
        "id": "c8219029-6162-4046-8702-0c2cb42f2378",
        "outputId": "b69ad8e1-149f-497e-a387-dee5ddc19c0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current tensor shape: torch.Size([1, 768, 16, 16]) -> [batch, embedding_dim, feature_map_height, feature_map_width]\n"
          ]
        }
      ],
      "source": [
        "# Current tensor shape\n",
        "print(f\"Current tensor shape: {image_out_of_conv.shape} -> [batch, embedding_dim, feature_map_height, feature_map_width]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed82899d-7bbc-49f9-a423-8fa6344b8e99",
      "metadata": {
        "id": "ed82899d-7bbc-49f9-a423-8fa6344b8e99"
      },
      "outputs": [],
      "source": [
        "# Create flatten layer\n",
        "flatten = nn.Flatten(start_dim=2, # flatten feature_map_height (dimension 2)\n",
        "                     end_dim=3) # flatten feature_map_width (dimension 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3fa363b-1923-4e27-a0b5-980d885fcda2",
      "metadata": {
        "id": "e3fa363b-1923-4e27-a0b5-980d885fcda2",
        "outputId": "b6bc2423-81ff-4fb1-e6d2-498430e50c9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.4290657..2.3590088].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original image shape: torch.Size([3, 32, 32])\n",
            "Image feature map shape: torch.Size([1, 768, 16, 16])\n",
            "Flattened image feature map shape: torch.Size([1, 768, 256])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# 1. View single image\n",
        "plt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False);\n",
        "print(f\"Original image shape: {image.shape}\")\n",
        "\n",
        "# 2. Turn image into feature maps\n",
        "image_out_of_conv = conv2d(image.unsqueeze(0)) # add batch dimension to avoid shape errors\n",
        "print(f\"Image feature map shape: {image_out_of_conv.shape}\")\n",
        "\n",
        "# 3. Flatten the feature maps\n",
        "image_out_of_conv_flattened = flatten(image_out_of_conv)\n",
        "print(f\"Flattened image feature map shape: {image_out_of_conv_flattened.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47f571a1-2303-4981-85f5-33936b39cf14",
      "metadata": {
        "id": "47f571a1-2303-4981-85f5-33936b39cf14",
        "outputId": "42d588b4-8e84-4d9d-b8ed-0dc755868371",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patch embedding sequence shape: torch.Size([1, 256, 768]) -> [batch_size, num_patches, embedding_size]\n"
          ]
        }
      ],
      "source": [
        "# Get flattened image patch embeddings in right shape\n",
        "image_out_of_conv_flattened_reshaped = image_out_of_conv_flattened.permute(0, 2, 1) # [batch_size, P^2•C, N] -> [batch_size, N, P^2•C]\n",
        "print(f\"Patch embedding sequence shape: {image_out_of_conv_flattened_reshaped.shape} -> [batch_size, num_patches, embedding_size]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4204163-8689-4b9e-8828-4e1e20d9316e",
      "metadata": {
        "id": "e4204163-8689-4b9e-8828-4e1e20d9316e",
        "outputId": "a6bbbaab-585b-4810-e3a5-8c6437559276",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2200x2200 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Get a single flattened feature map\n",
        "single_flattened_feature_map = image_out_of_conv_flattened_reshaped[:, :, 0] # index: (batch_size, number_of_patches, embedding_dimension)\n",
        "\n",
        "# Plot the flattened feature map visually\n",
        "plt.figure(figsize=(22, 22))\n",
        "plt.imshow(single_flattened_feature_map.detach().numpy())\n",
        "plt.title(f\"Flattened feature map shape: {single_flattened_feature_map.shape}\")\n",
        "plt.axis(False);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cdeb08e-948d-4810-ae8d-c607b3b9feec",
      "metadata": {
        "id": "0cdeb08e-948d-4810-ae8d-c607b3b9feec",
        "outputId": "771cde2e-69a8-4ce7-e517-a9f575d5fafc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 1.4093,  1.4093,  1.4093,  1.4093,  1.4093,  1.4093,  1.4093,  1.4093,\n",
              "           1.4093,  1.4093,  1.4093,  1.4093,  1.4093,  1.4093,  1.4093,  1.4093,\n",
              "           0.2182,  0.1350,  1.3921,  1.0936,  0.9934,  1.3139,  1.4069,  1.3650,\n",
              "           1.3986,  0.4693,  1.5130,  1.3014,  1.4480,  1.3781,  1.4093,  1.4093,\n",
              "          -0.1016,  0.4778,  0.9850,  0.7579,  0.5773,  1.2884,  0.8851,  0.9378,\n",
              "           0.3922, -0.2659,  0.4152,  1.3772,  1.3490,  1.3276,  1.4093,  1.4093,\n",
              "          -0.7510, -0.4331, -0.5449, -0.6491, -0.1261,  0.9690,  1.0967,  0.9999,\n",
              "          -0.8914, -0.7665, -0.5348,  0.4999,  0.8395,  0.2441,  1.4093,  1.4093,\n",
              "          -0.1448, -0.5575, -0.3809, -0.4015, -0.5788, -0.0082,  0.7275,  0.5236,\n",
              "          -0.9473, -0.8928, -1.0462, -0.7542,  0.6621,  0.6131,  1.4093,  1.4093,\n",
              "           0.4186,  0.5396,  0.1793, -0.5094, -0.0391, -0.8140, -0.6026, -0.6143,\n",
              "          -0.0492, -0.9745, -0.9565, -0.9494,  0.0420,  0.7540,  1.4093,  1.4093,\n",
              "           0.7380,  0.7600,  0.8436,  0.3630,  0.1518, -0.7555, -0.6871, -0.5212,\n",
              "          -0.1070, -0.7384, -0.9249, -0.7976, -0.5327,  0.5486,  1.4093,  1.4093,\n",
              "           0.4508,  1.4874,  1.2350, -0.3153,  0.5232, -0.4210, -0.7846, -0.3714,\n",
              "           0.1403, -0.5164, -0.8868, -0.9301, -0.7005,  0.3250,  1.4093,  1.4093,\n",
              "           0.5634,  1.1950,  1.0913, -0.3287,  0.1825, -0.5404, -0.7141, -0.2992,\n",
              "           0.2206, -0.3017, -0.7088, -0.8285, -0.6685,  0.4319,  1.4093,  1.4093,\n",
              "           0.0661,  0.5675,  0.3276, -0.3882, -0.4482, -0.3250, -0.6985, -0.0622,\n",
              "           0.1337, -0.0965, -0.5040, -0.6660, -0.6591,  0.4038,  1.4093,  1.4093,\n",
              "          -0.1474,  0.3003, -0.0401, -0.1976, -0.1732, -0.1776, -0.7886,  0.0281,\n",
              "           0.2213,  0.0608, -0.3081, -0.4916, -0.5813,  0.4906,  1.4093,  1.4093,\n",
              "          -0.1869,  0.3488,  0.0172, -0.1662, -0.0344, -0.2998, -0.5582,  0.0963,\n",
              "           0.2719,  0.1123, -0.0620, -0.2819, -0.5066,  0.4564,  1.4093,  1.4093,\n",
              "           0.2729,  0.2638,  0.3436,  0.0130, -0.1390, -0.3866, -0.3451,  0.2396,\n",
              "           0.3478,  0.0167, -0.0338, -0.3544, -0.2644,  0.2940,  1.4093,  1.4093,\n",
              "           0.6250,  0.7355,  0.6666,  1.0650,  0.1045, -0.2876, -0.5242,  0.3685,\n",
              "           0.5615, -0.0949,  0.0203, -0.1800,  0.2413,  0.3389,  1.4093,  1.4093,\n",
              "           0.6920,  0.8695,  1.0942,  1.5626,  0.8106, -0.1500, -0.2962,  0.6666,\n",
              "           0.8744, -0.3839,  0.2417,  0.3024,  0.3545,  0.0883,  1.4093,  1.4093,\n",
              "           0.5306,  0.6652,  0.6705,  1.2634,  0.6481, -0.1873,  0.2391,  0.5632,\n",
              "           0.2880,  0.0674,  0.5001,  0.6221,  0.5097,  0.1496,  1.4093,  1.4093]],\n",
              "        grad_fn=<SelectBackward0>),\n",
              " True,\n",
              " torch.Size([1, 256]))"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "# See the flattened feature map as a tensor\n",
        "single_flattened_feature_map, single_flattened_feature_map.requires_grad, single_flattened_feature_map.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b165987a-8370-471a-a663-711e0c6e60db",
      "metadata": {
        "id": "b165987a-8370-471a-a663-711e0c6e60db"
      },
      "source": [
        "### 4.5 Turning the ViT patch embedding layer into a module\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ef75c7e",
      "metadata": {
        "id": "3ef75c7e"
      },
      "outputs": [],
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    \"\"\"Turns a 2D input image into a 1D sequence learnable embedding vector.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Number of color channels for the input images. Defaults to 3.\n",
        "        patch_size (int): Size of patches to convert input image into. Defaults to 16.\n",
        "        embedding_dim (int): Size of embedding to turn image into. Defaults to 768.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int = 3, patch_size: int = 16, embedding_dim: int = 768):\n",
        "        super().__init__()\n",
        "\n",
        "        # Create a layer to turn an image into patches\n",
        "        self.patch_size = patch_size  # Save the patch size for validation in forward()\n",
        "        self.patcher = nn.Conv2d(in_channels=in_channels,\n",
        "                                 out_channels=embedding_dim,\n",
        "                                 kernel_size=patch_size,\n",
        "                                 stride=patch_size,\n",
        "                                 padding=0)\n",
        "\n",
        "        # Flatten the patch feature maps into a single dimension\n",
        "        self.flatten = nn.Flatten(start_dim=2, end_dim=3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Ensure the input image size is divisible by the patch size\n",
        "        image_resolution = x.shape[-1]\n",
        "        assert image_resolution % self.patch_size == 0, (\n",
        "            f\"Input image size must be divisible by patch size. \"\n",
        "            f\"Image resolution: {image_resolution}, Patch size: {self.patch_size}\"\n",
        "        )\n",
        "\n",
        "        # Apply the patch embedding layer\n",
        "        x_patched = self.patcher(x)  # [batch_size, embedding_dim, num_patches_h, num_patches_w]\n",
        "        x_flattened = self.flatten(x_patched)  # [batch_size, embedding_dim, num_patches]\n",
        "        return x_flattened.permute(0, 2, 1)  # [batch_size, num_patches, embedding_dim]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5599575-44cc-46c9-95a4-e65eb1379a59",
      "metadata": {
        "id": "a5599575-44cc-46c9-95a4-e65eb1379a59",
        "outputId": "3a52b11c-0c61-49b6-d64b-a99ef5c57425",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input image shape: torch.Size([1, 3, 32, 32])\n",
            "Output patch embedding shape: torch.Size([1, 4, 768])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Create an instance of patch embedding layer\n",
        "patchify = PatchEmbedding(in_channels=3,\n",
        "                          patch_size=16,\n",
        "                          embedding_dim=768)\n",
        "\n",
        "# Pass a single image through\n",
        "print(f\"Input image shape: {image.unsqueeze(0).shape}\")\n",
        "patch_embedded_image = patchify(image.unsqueeze(0)) # add an extra batch dimension on the 0th index, otherwise will error\n",
        "print(f\"Output patch embedding shape: {patch_embedded_image.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8576f2f1-a2ad-4874-8f76-08156371f444",
      "metadata": {
        "id": "8576f2f1-a2ad-4874-8f76-08156371f444"
      },
      "source": [
        "### 4.6 Creating the class token embedding\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-equation-1-the-class-token.png\" width=900 alt=\"class token embedding highlight from the vision transformer paper figure 1 and section 3.1\"/>\n",
        "\n",
        "*Left: Figure 1 from the ViT paper with the \"classification token\" or `[class]` embedding token I'm going to recreate highlighted. Right: Equation 1 and section 3.1 of the ViT paper that relate to the learnable class embedding token.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50381789-d73d-4648-9144-4d48da87318f",
      "metadata": {
        "id": "50381789-d73d-4648-9144-4d48da87318f",
        "outputId": "2662673d-3d62-459c-81c9-c8f44ef0b192",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.7323, -0.7078,  0.2371,  ..., -1.7198, -1.1418, -1.7004],\n",
            "         [ 0.5774,  0.3837,  0.8370,  ..., -1.4724, -0.4590, -0.5391],\n",
            "         [ 0.0026, -0.6673,  0.8582,  ..., -0.7801,  0.6293, -0.1357],\n",
            "         [-0.6611, -0.3238,  1.2898,  ..., -0.5000, -0.3766,  0.2627]]],\n",
            "       grad_fn=<PermuteBackward0>)\n",
            "Patch embedding shape: torch.Size([1, 4, 768]) -> [batch_size, number_of_patches, embedding_dimension]\n"
          ]
        }
      ],
      "source": [
        "# View the patch embedding and patch embedding shape\n",
        "print(patch_embedded_image)\n",
        "print(f\"Patch embedding shape: {patch_embedded_image.shape} -> [batch_size, number_of_patches, embedding_dimension]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc0bb859-e62e-41a8-9a47-339e4272a152",
      "metadata": {
        "id": "cc0bb859-e62e-41a8-9a47-339e4272a152",
        "outputId": "cd7e8a0d-ab48-4fc1-ad06-e2d19cca9e6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]], grad_fn=<SliceBackward0>)\n",
            "Class token shape: torch.Size([1, 1, 768]) -> [batch_size, number_of_tokens, embedding_dimension]\n"
          ]
        }
      ],
      "source": [
        "# Get the batch size and embedding dimension\n",
        "batch_size = patch_embedded_image.shape[0]\n",
        "embedding_dimension = patch_embedded_image.shape[-1]\n",
        "\n",
        "# Create the class token embedding as a learnable parameter that shares the same size as the embedding dimension (D)\n",
        "class_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension), # [batch_size, number_of_tokens, embedding_dimension]\n",
        "                           requires_grad=True) # make sure the embedding is learnable\n",
        "\n",
        "# Show the first 10 examples of the class_token\n",
        "print(class_token[:, :, :10])\n",
        "\n",
        "# Print the class_token shape\n",
        "print(f\"Class token shape: {class_token.shape} -> [batch_size, number_of_tokens, embedding_dimension]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7287b01-76cb-4371-ab07-981f7bbf2be5",
      "metadata": {
        "id": "a7287b01-76cb-4371-ab07-981f7bbf2be5",
        "outputId": "f860502a-17e7-4ed2-c587-754399b4405e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
            "         [ 0.7323, -0.7078,  0.2371,  ..., -1.7198, -1.1418, -1.7004],\n",
            "         [ 0.5774,  0.3837,  0.8370,  ..., -1.4724, -0.4590, -0.5391],\n",
            "         [ 0.0026, -0.6673,  0.8582,  ..., -0.7801,  0.6293, -0.1357],\n",
            "         [-0.6611, -0.3238,  1.2898,  ..., -0.5000, -0.3766,  0.2627]]],\n",
            "       grad_fn=<CatBackward0>)\n",
            "Sequence of patch embeddings with class token prepended shape: torch.Size([1, 5, 768]) -> [batch_size, number_of_patches, embedding_dimension]\n"
          ]
        }
      ],
      "source": [
        "# Add the class token embedding to the front of the patch embedding\n",
        "patch_embedded_image_with_class_embedding = torch.cat((class_token, patch_embedded_image),\n",
        "                                                      dim=1) # concat on first dimension\n",
        "\n",
        "# Print the sequence of patch embeddings with the prepended class token embedding\n",
        "print(patch_embedded_image_with_class_embedding)\n",
        "print(f\"Sequence of patch embeddings with class token prepended shape: {patch_embedded_image_with_class_embedding.shape} -> [batch_size, number_of_patches, embedding_dimension]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48502c61-16b0-4659-b95f-0e830ae93077",
      "metadata": {
        "id": "48502c61-16b0-4659-b95f-0e830ae93077"
      },
      "source": [
        "### 4.7 Creating the position embedding\n",
        "\n",
        "\n",
        ".\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-equation-1-the-position-embeddings.png\" width=900 alt=\"extracting the position embeddings from the vision transformer architecture and comparing them to other sections of the vision transformer paper\"/>\n",
        "\n",
        "*Left: Figure 1 from the ViT paper with the position embedding I'm going to recreate highlighted. Right: Equation 1 and section 3.1 of the ViT paper that relate to the position embedding.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33e5e5bf-e744-4249-ac9b-08986be5ef81",
      "metadata": {
        "id": "33e5e5bf-e744-4249-ac9b-08986be5ef81",
        "outputId": "1be4a86f-ec97-41a6-f9ac-a684f97229ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
              "          [-0.1436, -0.1450,  0.4573,  ..., -0.4802, -0.6611,  0.1512],\n",
              "          [-0.0604, -0.2222,  0.7091,  ..., -0.1936,  0.4300, -0.2363],\n",
              "          [-0.6420, -0.1064,  0.1353,  ..., -0.4079, -0.1621, -0.0689],\n",
              "          [-0.7531,  0.1716,  0.8412,  ..., -0.6355, -0.3636, -0.2324]]],\n",
              "        grad_fn=<CatBackward0>),\n",
              " torch.Size([1, 5, 768]))"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "# View the sequence of patch embeddings with the prepended class embedding\n",
        "patch_embedded_image_with_class_embedding, patch_embedded_image_with_class_embedding.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bb7f6d1-0824-47eb-a059-6854da5c7433",
      "metadata": {
        "id": "5bb7f6d1-0824-47eb-a059-6854da5c7433",
        "outputId": "286e1468-21a3-42b4-e760-bc93eddd74e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]], grad_fn=<SliceBackward0>)\n",
            "Position embedding shape: torch.Size([1, 65, 768]) -> [batch_size, number_of_patches, embedding_dimension]\n"
          ]
        }
      ],
      "source": [
        "# Calculate N (number of patches)\n",
        "number_of_patches = int((height * width) / patch_size**2)\n",
        "\n",
        "# Get embedding dimension\n",
        "embedding_dimension = patch_embedded_image_with_class_embedding.shape[2]\n",
        "\n",
        "# Create the learnable 1D position embedding\n",
        "position_embedding = nn.Parameter(torch.ones(1,\n",
        "                                             number_of_patches+1,\n",
        "                                             embedding_dimension),\n",
        "                                  requires_grad=True) # make sure it's learnable\n",
        "\n",
        "# Show the first 10 sequences and 10 position embedding values and check the shape of the position embedding\n",
        "print(position_embedding[:, :10, :10])\n",
        "print(f\"Position embedding shape: {position_embedding.shape} -> [batch_size, number_of_patches, embedding_dimension]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6654c7ed-eb94-408b-b435-9b352c84328b",
      "metadata": {
        "id": "6654c7ed-eb94-408b-b435-9b352c84328b"
      },
      "source": [
        "### 4.8 Putting it all together: from image to embedding\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8de90548-e6b0-4123-90ca-a23b0fab52a9",
      "metadata": {
        "id": "8de90548-e6b0-4123-90ca-a23b0fab52a9",
        "outputId": "48ecb47d-2103-4771-f2a6-3c9560e42942",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image tensor shape: torch.Size([3, 32, 32])\n",
            "Input image with batch dimension shape: torch.Size([1, 3, 32, 32])\n",
            "Patching embedding shape: torch.Size([1, 64, 768])\n",
            "Class token embedding shape: torch.Size([1, 1, 768])\n",
            "Patch embedding with class token shape: torch.Size([1, 65, 768])\n",
            "Patch and position embedding shape: torch.Size([1, 65, 768])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# 1. Set patch size\n",
        "patch_size = 4\n",
        "\n",
        "# 2. Print shape of original image tensor and get the image dimensions\n",
        "print(f\"Image tensor shape: {image.shape}\")\n",
        "height, width = image.shape[1], image.shape[2]\n",
        "\n",
        "# 3. Get image tensor and add batch dimension\n",
        "x = image.unsqueeze(0)\n",
        "print(f\"Input image with batch dimension shape: {x.shape}\")\n",
        "\n",
        "# 4. Create patch embedding layer\n",
        "patch_embedding_layer = PatchEmbedding(in_channels=3,\n",
        "                                       patch_size=patch_size,\n",
        "                                       embedding_dim=768)\n",
        "\n",
        "# 5. Pass image through patch embedding layer\n",
        "patch_embedding = patch_embedding_layer(x)\n",
        "print(f\"Patching embedding shape: {patch_embedding.shape}\")\n",
        "\n",
        "# 6. Create class token embedding\n",
        "batch_size = patch_embedding.shape[0]\n",
        "embedding_dimension = patch_embedding.shape[-1]\n",
        "class_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension),\n",
        "                           requires_grad=True) # make sure it's learnable\n",
        "print(f\"Class token embedding shape: {class_token.shape}\")\n",
        "\n",
        "# 7. Prepend class token embedding to patch embedding\n",
        "patch_embedding_class_token = torch.cat((class_token, patch_embedding), dim=1)\n",
        "print(f\"Patch embedding with class token shape: {patch_embedding_class_token.shape}\")\n",
        "\n",
        "# 8. Create position embedding\n",
        "number_of_patches = int((height * width) / patch_size**2)\n",
        "position_embedding = nn.Parameter(torch.ones(1, number_of_patches+1, embedding_dimension),\n",
        "                                  requires_grad=True) # make sure it's learnable\n",
        "\n",
        "# 9. Add position embedding to patch embedding with class token\n",
        "patch_and_position_embedding = patch_embedding_class_token + position_embedding\n",
        "print(f\"Patch and position embedding shape: {patch_and_position_embedding.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02f725de-64d1-41d2-a9d6-374cf6d4f589",
      "metadata": {
        "id": "02f725de-64d1-41d2-a9d6-374cf6d4f589"
      },
      "source": [
        "## 5. Equation 2: Multi-Head Attention (MSA)\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-equation-2-msa-block-mapped-to-equation.png\" alt=\"mapping equation 2 from the ViT paper to the ViT architecture diagram in figure 1\" width=900/>\n",
        "\n",
        "***Left:** Figure 1 from the ViT paper with Multi-Head Attention and Norm layers as well as the residual connection (+) highlighted within the Transformer Encoder block. **Right:** Mapping the Multi-Head Self Attention (MSA) layer, Norm layer and residual connection to their respective parts of equation 2 in the ViT paper.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b76ae98c",
      "metadata": {
        "id": "b76ae98c"
      },
      "outputs": [],
      "source": [
        "# 1. Create a class that inherits from nn.Module\n",
        "class MultiheadSelfAttentionBlock(nn.Module):\n",
        "    \"\"\"Creates a multi-head self-attention block (\"MSA block\" for short).\n",
        "    \"\"\"\n",
        "    # 2. Initialize the class with hyperparameters from Table 1\n",
        "    def __init__(self,\n",
        "                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n",
        "                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n",
        "                 attn_dropout:float=0): # doesn't look like the paper uses any dropout in MSABlocks\n",
        "        super().__init__()\n",
        "\n",
        "        # 3. Create the Norm layer (LN)\n",
        "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
        "\n",
        "        # 4. Create the Multi-Head Attention (MSA) layer\n",
        "        self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
        "                                                    num_heads=num_heads,\n",
        "                                                    dropout=attn_dropout,\n",
        "                                                    batch_first=True) # does our batch dimension come first?\n",
        "\n",
        "    # 5. Create a forward() method to pass the data through the layers\n",
        "    def forward(self, x):\n",
        "        x = self.layer_norm(x)\n",
        "        attn_output, _ = self.multihead_attn(query=x, # query embeddings\n",
        "                                             key=x, # key embeddings\n",
        "                                             value=x, # value embeddings\n",
        "                                             need_weights=False) # do we need the weights or just the layer outputs?\n",
        "        return attn_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ceb1dfc0-40ad-4cee-bc54-e9a5cec56895",
      "metadata": {
        "id": "ceb1dfc0-40ad-4cee-bc54-e9a5cec56895",
        "outputId": "cace8caf-977e-4939-c677-42ab8f8f0067",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape of MSA block: torch.Size([1, 65, 768])\n",
            "Output shape MSA block: torch.Size([1, 65, 768])\n"
          ]
        }
      ],
      "source": [
        "# Create an instance of MSABlock\n",
        "multihead_self_attention_block = MultiheadSelfAttentionBlock(embedding_dim=768, # from Table 1\n",
        "                                                             num_heads=12) # from Table 1\n",
        "\n",
        "# Pass patch and position image embedding through MSABlock\n",
        "patched_image_through_msa_block = multihead_self_attention_block(patch_and_position_embedding)\n",
        "print(f\"Input shape of MSA block: {patch_and_position_embedding.shape}\")\n",
        "print(f\"Output shape MSA block: {patched_image_through_msa_block.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "236848a7-a1e0-403d-8c78-d1c61d1e45fd",
      "metadata": {
        "id": "236848a7-a1e0-403d-8c78-d1c61d1e45fd",
        "tags": []
      },
      "source": [
        "## 6. Equation 3: Multilayer Perceptron (MLP)\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/images/08-vit-paper-equation-3-annotated.png\" alt=\"mapping equation 3 from the ViT paper to the ViT architecture diagram in figure 1\" width=900/>\n",
        "\n",
        "***Left:** Figure 1 from the ViT paper with MLP and Norm layers as well as the residual connection (+) highlighted within the Transformer Encoder block. **Right:** Mapping the multilayer perceptron (MLP) layer, Norm layer (LN) and residual connection to their respective parts of equation 3 in the ViT paper.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68d9dbfe",
      "metadata": {
        "id": "68d9dbfe"
      },
      "outputs": [],
      "source": [
        "# 1. Create a class that inherits from nn.Module\n",
        "class MLPBlock(nn.Module):\n",
        "    \"\"\"Creates a layer normalized multilayer perceptron block (\"MLP block\" for short).\"\"\"\n",
        "    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n",
        "    def __init__(self,\n",
        "                 embedding_dim:int=768, # Hidden Size D from Table 1 for ViT-Base\n",
        "                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n",
        "                 dropout:float=0.1): # Dropout from Table 3 for ViT-Base\n",
        "        super().__init__()\n",
        "\n",
        "        # 3. Create the Norm layer (LN)\n",
        "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
        "\n",
        "        # 4. Create the Multilayer perceptron (MLP) layer(s)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(in_features=embedding_dim,\n",
        "                      out_features=mlp_size),\n",
        "            nn.GELU(), # \"The MLP contains two layers with a GELU non-linearity (section 3.1).\"\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(in_features=mlp_size, # needs to take same in_features as out_features of layer above\n",
        "                      out_features=embedding_dim), # take back to embedding_dim\n",
        "            nn.Dropout(p=dropout) # \"Dropout, when used, is applied after every dense layer..\"\n",
        "        )\n",
        "\n",
        "    # 5. Create a forward() method to pass the data through the layers\n",
        "    def forward(self, x):\n",
        "        x = self.layer_norm(x)\n",
        "        x = self.mlp(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "442fb987",
      "metadata": {
        "id": "442fb987",
        "outputId": "d4975ef2-787f-44bb-dbb1-94f56849c3cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape of MLP block: torch.Size([1, 65, 768])\n",
            "Output shape MLP block: torch.Size([1, 65, 768])\n"
          ]
        }
      ],
      "source": [
        "# Create an instance of MLPBlock\n",
        "mlp_block = MLPBlock(embedding_dim=768, # from Table 1\n",
        "                     mlp_size=3072, # from Table 1\n",
        "                     dropout=0.1) # from Table 3\n",
        "\n",
        "# Pass output of MSABlock through MLPBlock\n",
        "patched_image_through_mlp_block = mlp_block(patched_image_through_msa_block)\n",
        "print(f\"Input shape of MLP block: {patched_image_through_msa_block.shape}\")\n",
        "print(f\"Output shape MLP block: {patched_image_through_mlp_block.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6259bdc6-525a-4bd6-89e8-c2dcdab09d0d",
      "metadata": {
        "id": "6259bdc6-525a-4bd6-89e8-c2dcdab09d0d"
      },
      "source": [
        "## 7. Create the Transformer Encoder\n",
        "\n",
        "\n",
        "\n",
        "`x_input -> MSA_block -> [MSA_block_output + x_input] -> MLP_block -> [MLP_block_output + MSA_block_output + x_input] -> ...`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c43855c",
      "metadata": {
        "id": "2c43855c"
      },
      "outputs": [],
      "source": [
        "# 1. Create a class that inherits from nn.Module\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    \"\"\"Creates a Transformer Encoder block.\"\"\"\n",
        "    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n",
        "    def __init__(self,\n",
        "                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n",
        "                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n",
        "                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n",
        "                 mlp_dropout:float=0.1, # Amount of dropout for dense layers from Table 3 for ViT-Base\n",
        "                 attn_dropout:float=0): # Amount of dropout for attention layers\n",
        "        super().__init__()\n",
        "\n",
        "        # 3. Create MSA block (equation 2)\n",
        "        self.msa_block = MultiheadSelfAttentionBlock(embedding_dim=embedding_dim,\n",
        "                                                     num_heads=num_heads,\n",
        "                                                     attn_dropout=attn_dropout)\n",
        "\n",
        "        # 4. Create MLP block (equation 3)\n",
        "        self.mlp_block =  MLPBlock(embedding_dim=embedding_dim,\n",
        "                                   mlp_size=mlp_size,\n",
        "                                   dropout=mlp_dropout)\n",
        "\n",
        "    # 5. Create a forward() method\n",
        "    def forward(self, x):\n",
        "\n",
        "        # 6. Create residual connection for MSA block (add the input to the output)\n",
        "        x =  self.msa_block(x) + x\n",
        "\n",
        "        # 7. Create residual connection for MLP block (add the input to the output)\n",
        "        x = self.mlp_block(x) + x\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a63be4de-ffff-4fa1-97b8-103012797d36",
      "metadata": {
        "id": "a63be4de-ffff-4fa1-97b8-103012797d36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ee08f4f-f75e-4b2f-e42b-63200a6846db"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==================================================================================================================================\n",
              "Layer (type (var_name))                            Input Shape          Output Shape         Param #              Trainable\n",
              "==================================================================================================================================\n",
              "TransformerEncoderBlock (TransformerEncoderBlock)  [1, 197, 768]        [1, 197, 768]        --                   True\n",
              "├─MultiheadSelfAttentionBlock (msa_block)          [1, 197, 768]        [1, 197, 768]        --                   True\n",
              "│    └─LayerNorm (layer_norm)                      [1, 197, 768]        [1, 197, 768]        1,536                True\n",
              "│    └─MultiheadAttention (multihead_attn)         --                   [1, 197, 768]        2,362,368            True\n",
              "├─MLPBlock (mlp_block)                             [1, 197, 768]        [1, 197, 768]        --                   True\n",
              "│    └─LayerNorm (layer_norm)                      [1, 197, 768]        [1, 197, 768]        1,536                True\n",
              "│    └─Sequential (mlp)                            [1, 197, 768]        [1, 197, 768]        --                   True\n",
              "│    │    └─Linear (0)                             [1, 197, 768]        [1, 197, 3072]       2,362,368            True\n",
              "│    │    └─GELU (1)                               [1, 197, 3072]       [1, 197, 3072]       --                   --\n",
              "│    │    └─Dropout (2)                            [1, 197, 3072]       [1, 197, 3072]       --                   --\n",
              "│    │    └─Linear (3)                             [1, 197, 3072]       [1, 197, 768]        2,360,064            True\n",
              "│    │    └─Dropout (4)                            [1, 197, 768]        [1, 197, 768]        --                   --\n",
              "==================================================================================================================================\n",
              "Total params: 7,087,872\n",
              "Trainable params: 7,087,872\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 4.73\n",
              "==================================================================================================================================\n",
              "Input size (MB): 0.61\n",
              "Forward/backward pass size (MB): 8.47\n",
              "Params size (MB): 18.90\n",
              "Estimated Total Size (MB): 27.98\n",
              "=================================================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "# Create an instance of TransformerEncoderBlock\n",
        "transformer_encoder_block = TransformerEncoderBlock()\n",
        "\n",
        " # Print an input and output summary of our Transformer Encoder (uncomment for full output)\n",
        "summary(model=transformer_encoder_block,\n",
        "         input_size=(1, 197, 768), # (batch_size, num_patches, embedding_dimension)\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "       row_settings=[\"var_names\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8. Big ol' ViT"
      ],
      "metadata": {
        "id": "BwgNL8RSpEWd"
      },
      "id": "BwgNL8RSpEWd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2df890d5",
      "metadata": {
        "id": "2df890d5"
      },
      "outputs": [],
      "source": [
        "# 1. Create a ViT class that inherits from nn.Module\n",
        "class ViT(nn.Module):\n",
        "    \"\"\"Creates a Vision Transformer architecture with ViT-Base hyperparameters by default.\"\"\"\n",
        "    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n",
        "    def __init__(self,\n",
        "                 img_size:int=32, # Training resolution from Table 3 in ViT paper\n",
        "                 in_channels:int=3, # Number of channels in input image\n",
        "                 patch_size:int=2, # Patch size\n",
        "                 num_transformer_layers:int=6, # Layers from Table 1 for ViT-Base\n",
        "                 embedding_dim:int=256, # Hidden size D from Table 1 for ViT-Base\n",
        "                 mlp_size:int=1024, # MLP size from Table 1 for ViT-Base\n",
        "                 num_heads:int=4, # Heads from Table 1 for ViT-Base\n",
        "                 attn_dropout:float=0, # Dropout for attention projection\n",
        "                 mlp_dropout:float=0.1, # Dropout for dense/MLP layers\n",
        "                 embedding_dropout:float=0.1, # Dropout for patch and position embeddings\n",
        "                 num_classes:int=10): # Default for ImageNet but can customize this\n",
        "        super().__init__() # don't forget the super().__init__()!\n",
        "\n",
        "        # 3. Make the image size is divisible by the patch size\n",
        "        assert img_size % patch_size == 0, f\"Image size must be divisible by patch size, image size: {img_size}, patch size: {patch_size}.\"\n",
        "\n",
        "        # 4. Calculate number of patches (height * width/patch^2)\n",
        "        self.num_patches = (img_size * img_size) // patch_size**2\n",
        "\n",
        "        # 5. Create learnable class embedding (needs to go at front of sequence of patch embeddings)\n",
        "        self.class_embedding = nn.Parameter(data=torch.randn(1, 1, embedding_dim),\n",
        "                                            requires_grad=True)\n",
        "\n",
        "        # 6. Create learnable position embedding\n",
        "        self.position_embedding = nn.Parameter(data=torch.randn(1, self.num_patches+1, embedding_dim),\n",
        "                                               requires_grad=True)\n",
        "\n",
        "        # 7. Create embedding dropout value\n",
        "        self.embedding_dropout = nn.Dropout(p=embedding_dropout)\n",
        "\n",
        "        # 8. Create patch embedding layer\n",
        "        self.patch_embedding = PatchEmbedding(in_channels=in_channels,\n",
        "                                              patch_size=patch_size,\n",
        "                                              embedding_dim=embedding_dim)\n",
        "\n",
        "        # 9. Create Transformer Encoder blocks (we can stack Transformer Encoder blocks using nn.Sequential())\n",
        "        # Note: The \"*\" means \"all\"\n",
        "        self.transformer_encoder = nn.Sequential(*[TransformerEncoderBlock(embedding_dim=embedding_dim,\n",
        "                                                                            num_heads=num_heads,\n",
        "                                                                            mlp_size=mlp_size,\n",
        "                                                                            mlp_dropout=mlp_dropout) for _ in range(num_transformer_layers)])\n",
        "\n",
        "        # 10. Create classifier head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(normalized_shape=embedding_dim),\n",
        "            nn.Linear(in_features=embedding_dim,\n",
        "                      out_features=num_classes)\n",
        "        )\n",
        "\n",
        "    # 11. Create a forward() method\n",
        "    def forward(self, x):\n",
        "\n",
        "        # 12. Get batch size\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        # 13. Create class token embedding and expand it to match the batch size (equation 1)\n",
        "        class_token = self.class_embedding.expand(batch_size, -1, -1) # \"-1\" means to infer the dimension (try this line on its own)\n",
        "\n",
        "        # 14. Create patch embedding (equation 1)\n",
        "        x = self.patch_embedding(x)\n",
        "\n",
        "        # 15. Concat class embedding and patch embedding (equation 1)\n",
        "        x = torch.cat((class_token, x), dim=1)\n",
        "\n",
        "        # 16. Add position embedding to patch embedding (equation 1)\n",
        "        x = self.position_embedding + x\n",
        "\n",
        "        # 17. Run embedding dropout (Appendix B.1)\n",
        "        x = self.embedding_dropout(x)\n",
        "\n",
        "        # 18. Pass patch, position and class embedding through transformer encoder layers (equations 2 & 3)\n",
        "        x = self.transformer_encoder(x)\n",
        "\n",
        "        # 19. Put 0 index logit through classifier (equation 4)\n",
        "        x = self.classifier(x[:, 0]) # run on each sample in a batch at 0 index\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f86190a3-ed1f-4ab3-881c-4eecea996912",
      "metadata": {
        "id": "f86190a3-ed1f-4ab3-881c-4eecea996912",
        "outputId": "c8ccd1b1-471e-4254-e524-3554d502256a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.5572,  1.1784,  0.3913,  1.1599,  0.2731,  0.2038,  0.4134,  0.6121,\n",
              "          0.0054, -0.9568]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "##Testing the ViT\n",
        "\n",
        "# Create a random tensor with same shape as a single image\n",
        "random_image_tensor = torch.randn(1, 3, 32, 32) # (batch_size, color_channels, height, width)\n",
        "\n",
        "# Create an instance of ViT with the number of classes we're working with (pizza, steak, sushi)\n",
        "vit = ViT(num_classes=len(class_names))\n",
        "\n",
        "# Pass the random image tensor to our ViT instance\n",
        "vit(random_image_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c0a0c9c-6d98-47fd-a152-8a06be99b5fa",
      "metadata": {
        "id": "2c0a0c9c-6d98-47fd-a152-8a06be99b5fa"
      },
      "source": [
        "### 8.1 Getting a visual summary of our ViT model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "494bde26-ed1e-45dc-b615-78ac268ca20e",
      "metadata": {
        "id": "494bde26-ed1e-45dc-b615-78ac268ca20e"
      },
      "outputs": [],
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "# # Print a summary of our custom ViT model using torchinfo (uncomment for actual output)\n",
        "# summary(model=vit,\n",
        "#         input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)\n",
        "#         # col_names=[\"input_size\"], # uncomment for smaller output\n",
        "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "#         col_width=20,\n",
        "#         row_settings=[\"var_names\"]\n",
        "# )\n",
        "\n",
        "##I have included the image directly cause it was takin a while to do this fro whaetevr reason"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0279251-5cc1-42a5-bebc-8391ef911343",
      "metadata": {
        "id": "d0279251-5cc1-42a5-bebc-8391ef911343"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-summary-output-custom-vit-model.png\" alt=\"input and output summary of our custom made ViT model\" width=900/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8088ca7-48b4-4a96-8f58-f84980073e0b",
      "metadata": {
        "id": "e8088ca7-48b4-4a96-8f58-f84980073e0b"
      },
      "source": [
        "## 9. Training Setup\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9107b068-f253-4026-ad21-83be41404043",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9107b068-f253-4026-ad21-83be41404043",
        "outputId": "a1c2835e-99d7-4eac-fcf7-45c75932310b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30 | Train Loss: 1.7735 | Test Loss: 1.5777 | Test Accuracy: 0.4216\n",
            "Epoch 2/30 | Train Loss: 1.4553 | Test Loss: 1.3616 | Test Accuracy: 0.4992\n",
            "Epoch 3/30 | Train Loss: 1.3049 | Test Loss: 1.2230 | Test Accuracy: 0.5544\n",
            "Epoch 4/30 | Train Loss: 1.2104 | Test Loss: 1.1711 | Test Accuracy: 0.5750\n",
            "Epoch 5/30 | Train Loss: 1.1366 | Test Loss: 1.0923 | Test Accuracy: 0.6058\n",
            "Epoch 6/30 | Train Loss: 1.0801 | Test Loss: 1.0218 | Test Accuracy: 0.6320\n",
            "Epoch 7/30 | Train Loss: 1.0297 | Test Loss: 0.9830 | Test Accuracy: 0.6450\n",
            "Epoch 8/30 | Train Loss: 0.9856 | Test Loss: 1.0009 | Test Accuracy: 0.6446\n",
            "Epoch 9/30 | Train Loss: 0.9490 | Test Loss: 0.9297 | Test Accuracy: 0.6726\n",
            "Epoch 10/30 | Train Loss: 0.9166 | Test Loss: 0.9142 | Test Accuracy: 0.6715\n",
            "Epoch 11/30 | Train Loss: 0.8851 | Test Loss: 0.9234 | Test Accuracy: 0.6710\n",
            "Epoch 12/30 | Train Loss: 0.8566 | Test Loss: 0.8702 | Test Accuracy: 0.6905\n",
            "Epoch 13/30 | Train Loss: 0.8284 | Test Loss: 0.8710 | Test Accuracy: 0.6895\n",
            "Epoch 14/30 | Train Loss: 0.8034 | Test Loss: 0.8373 | Test Accuracy: 0.7048\n",
            "Epoch 15/30 | Train Loss: 0.7766 | Test Loss: 0.8655 | Test Accuracy: 0.6994\n",
            "Epoch 16/30 | Train Loss: 0.7508 | Test Loss: 0.8300 | Test Accuracy: 0.7034\n",
            "Epoch 17/30 | Train Loss: 0.7287 | Test Loss: 0.7747 | Test Accuracy: 0.7281\n",
            "Epoch 18/30 | Train Loss: 0.7030 | Test Loss: 0.7539 | Test Accuracy: 0.7340\n",
            "Epoch 19/30 | Train Loss: 0.6824 | Test Loss: 0.7992 | Test Accuracy: 0.7184\n",
            "Epoch 20/30 | Train Loss: 0.6621 | Test Loss: 0.7612 | Test Accuracy: 0.7362\n",
            "Epoch 21/30 | Train Loss: 0.6393 | Test Loss: 0.7627 | Test Accuracy: 0.7376\n",
            "Epoch 22/30 | Train Loss: 0.6173 | Test Loss: 0.7555 | Test Accuracy: 0.7451\n",
            "Epoch 23/30 | Train Loss: 0.5981 | Test Loss: 0.7350 | Test Accuracy: 0.7472\n",
            "Epoch 24/30 | Train Loss: 0.5802 | Test Loss: 0.7353 | Test Accuracy: 0.7447\n",
            "Epoch 25/30 | Train Loss: 0.5638 | Test Loss: 0.7162 | Test Accuracy: 0.7510\n",
            "Epoch 26/30 | Train Loss: 0.5407 | Test Loss: 0.7241 | Test Accuracy: 0.7490\n",
            "Epoch 27/30 | Train Loss: 0.5236 | Test Loss: 0.7377 | Test Accuracy: 0.7479\n",
            "Epoch 28/30 | Train Loss: 0.5081 | Test Loss: 0.7441 | Test Accuracy: 0.7504\n",
            "Epoch 29/30 | Train Loss: 0.4926 | Test Loss: 0.6957 | Test Accuracy: 0.7686\n",
            "Epoch 30/30 | Train Loss: 0.4726 | Test Loss: 0.7102 | Test Accuracy: 0.7594\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "\n",
        "\n",
        "\n",
        "vit.classifier[-1] = nn.Linear(vit.classifier[-1].in_features, 10)  # Update for CIFAR-10\n",
        "\n",
        "\n",
        "# Setup optimizer with ViT paper hyperparameters\n",
        "optimizer = optim.AdamW(\n",
        "    vit.parameters(),\n",
        "    lr=0.0003,  # Learning rate from ViT paper\n",
        "    betas=(0.9, 0.999),  # ViT default values\n",
        "    weight_decay=0.01  # Regularization strength\n",
        ")\n",
        "\n",
        "# Setup loss function for multi-class classification\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Compile the model for better performance (PyTorch 2.0+)\n",
        "compiled_model = torch.compile(vit)\n",
        "\n",
        "# Efficient training loop\n",
        "def train(model, train_dataloader, test_dataloader, optimizer, loss_fn, epochs, device):\n",
        "    model = model.to(device)\n",
        "    results = {\"train_loss\": [], \"test_loss\": [], \"test_accuracy\": []}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for X, y in train_dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(X)\n",
        "            loss = loss_fn(y_pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_loss /= len(train_dataloader)\n",
        "        results[\"train_loss\"].append(train_loss)\n",
        "\n",
        "        # Evaluation phase\n",
        "        model.eval()\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "        with torch.no_grad():\n",
        "            for X, y in test_dataloader:\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                y_pred = model(X)\n",
        "                test_loss += loss_fn(y_pred, y).item()\n",
        "                correct += (y_pred.argmax(1) == y).sum().item()\n",
        "\n",
        "        test_loss /= len(test_dataloader)\n",
        "        test_accuracy =  correct / len(test_dataloader.dataset)\n",
        "        results[\"test_loss\"].append(test_loss)\n",
        "        results[\"test_accuracy\"].append(test_accuracy)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | \"\n",
        "              f\"Train Loss: {train_loss:.4f} | \"\n",
        "              f\"Test Loss: {test_loss:.4f} | \"\n",
        "              f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Train the compiled model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "results = train(\n",
        "    model=compiled_model,\n",
        "    train_dataloader=train_dataloader,\n",
        "    test_dataloader=test_dataloader,\n",
        "    optimizer=optimizer,\n",
        "    loss_fn=loss_fn,\n",
        "    epochs=30,\n",
        "    device=device\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32cb3ad2-6eea-4b69-9ce8-322270643919",
      "metadata": {
        "id": "32cb3ad2-6eea-4b69-9ce8-322270643919"
      },
      "source": [
        "### 9.5 Plot the loss curves of our ViT model\n",
        "\n",
        "We've trained our ViT model and seen the results as numbers on a page.\n",
        "\n",
        "But let's now follow the data explorer's motto of *visualize, visualize, visualize!*\n",
        "\n",
        "And one of the best things to visualize for a model is its loss curves.\n",
        "\n",
        "To check out our ViT model's loss curves, we can use the `plot_loss_curves` function from `helper_functions.py` we created in [04. PyTorch Custom Datasets section 7.8](https://www.learnpytorch.io/04_pytorch_custom_datasets/#78-plot-the-loss-curves-of-model-0)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcca1148-6475-4012-bfc9-cbad2706c22d",
      "metadata": {
        "id": "fcca1148-6475-4012-bfc9-cbad2706c22d",
        "outputId": "70969476-f2fa-4969-f1fa-e4a16db2a376"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2oAAAG5CAYAAAD/HsejAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAACluUlEQVR4nOzdd3zV5d3/8deVPcg4IWFlkAiIA2SKA3BXRa2jzrbe1rbW2ltbu2t7d9m7vW/buz9rrau21Q5t3auK27gHEAQBBQWSkDBDcrLIzrl+f3zPCQECZJxzvme8n49HHic584NCks+5ruv9MdZaREREREREJHIkuF2AiIiIiIiI7EmNmoiIiIiISIRRoyYiIiIiIhJh1KiJiIiIiIhEGDVqIiIiIiIiEUaNmoiIiIiISIRRoyYiIiIiIhJh1KiJjIAxpsoYc5rbdYiIiISaMeZVY4zXGJPqdi0i8UCNmoiIiIgckDGmFFgIWODcML5uUrheSyTSqFETCTJjTKox5hZjzBb/xy2Bdx+NMfnGmKeNMY3GmAZjzBvGmAT/bT8wxmw2xrQYY9YZY051908iIiLS5wrgXeCvwBcCVxpjio0xjxlj6owx9caY2/rd9hVjzEf+n2sfGmNm+6+3xpjJ/e73V2PML/2fn2SMqfX/TNwG3GuM8fh/dtb5V/SeNsYU9Xt8njHmXv/PXK8x5gn/9auNMZ/ud79kY8xOY8zMEP03EgkqNWoiwfdfwLHATGAGMA/4sf+27wC1QAEwFvgRYI0xU4HrgKOttVnAGUBVWKsWERHZvyuA+/0fZxhjxhpjEoGngWqgFCgEHgAwxlwM/Nz/uGycVbj6Qb7WOCAPmAhcjfP76r3+r0uAduC2fvf/B5ABHAmMAX7nv/7vwOX97ncWsNVau2KQdYi4SsvJIsH3eeDr1todAMaYG4E/Aj8BuoHxwERr7XrgDf99eoFU4AhjTJ21tsqNwkVERPZmjFmA0yQ9ZK3daYzZAHwOZ4VtAvA9a22P/+5v+i+vAn5jrV3q/3r9EF7SB/zMWtvp/7odeLRfPb8Cyv2fjwcWAaOttV7/XV7zX94H/MQYk22tbQb+A6epE4kKWlETCb4JOO8uBlT7rwP4P5wfVi8YYzYaY24A8Ddt38R593GHMeYBY8wERERE3PcF4AVr7U7/1//0X1cMVPdr0vorBjYM8/XqrLUdgS+MMRnGmD8aY6qNMc3A60Cuf0WvGGjo16T1sdZuAd4CLjTG5OI0dPcPsyaRsFOjJhJ8W3DeeQwo8V+HtbbFWvsda+0hwKeBbwfOollr/2mtDbxraYFfh7dsERGRPRlj0oFLgBONMdv858a+hbO1fztQsp/Ajxpg0n6etg1nq2LAuL1ut3t9/R1gKnCMtTYbOCFQnv918vyN2ED+hrP98WLgHWvt5v3cTyTiqFETGblkY0xa4AP4F/BjY0yBMSYf+CnO9guMMecYYyYbYwzQDPQCvcaYqcaYU/yhIx042zx63fnjiIiI9Dkf5+fREThnr2cCh+Ns3T8f2ArcZIzJ9P8cnO9/3J+B7xpj5hjHZGNM4E3MFcDnjDGJxpgzgRMPUkMWzs/FRmNMHvCzwA3W2q3As8Ad/tCRZGPMCf0e+wQwG7ge58yaSNRQoyYycotxfoAEPtKAZcAHwCpgOfBL/32nAC8BrcA7wB3W2ldxzqfdBOwEtuEchv5R2P4EIiIiA/sCcK+1dpO1dlvgAyfM47M4u0MmA5twwrIuBbDWPgz8CmebZAtOw5Tnf87r/Y9rxDnX/cRBargFSMf5Gfku8Nxet/8HzhnwtcAOnKME+OsInG8rAx4b/B9bxH3G2r1Xl0VEREREYoMx5qfAodbayw96Z5EIotRHEREREYlJ/q2SX8ZZdROJKtr6KCIiIiIxxxjzFZywkWetta+7XY/IUGnro4iIiIiISITRipqIiIiIiEiEce2MWn5+vi0tLXXr5UVEJIwqKip2WmsL3K4jWuhnpIhIfDjQz0fXGrXS0lKWLVvm1suLiEgYGWOq3a4hmuhnpIhIfDjQz0dtfRQREREREYkwg2rUjDFVxphVxpgVxph93uLzT5y/1Riz3hjzgTFmdvBLFRERERERiQ9D2fp4srV2535uWwRM8X8cA9zpvxQREREREZEhCtYZtfOAv1sn6/9dY0yuMWa8tXZrkJ5fRCRkuru7qa2tpaOjw+1Sol5aWhpFRUUkJye7XYqIiEhUG2yjZoEXjDEW+KO19u69bi/EGSgYUOu/bo9GzRhzNXA1QElJybAKFhEJttraWrKysigtLcUY43Y5UctaS319PbW1tZSVlbldTlAZY84Efg8kAn+21t601+0nAU8Clf6rHrPW/sJ/WxXQAvQCPdbaueGpWkREotlgG7X51totxpgxwIvGmLV7TXgf6DebfSZp+xu8uwHmzp2rSdsiEhE6OjrUpAWBMYbRo0dTV1fndilBZYxJBG4HPoXzRuRSY8xT1toP97rrG9bac/bzNAc6PiAiIrKPQYWJWGu3+C93AI8D8/a6Sy1Q3O/rImBLMAoUEQkHNWnBEaP/HecB6621G621XcADOFv+RUREQuagjZoxJtMYkxX4HDgdWL3X3Z4CrvCnPx4LNOl8moiIxIj9be/f23HGmJXGmGeNMUf2uz5wfKDCfwRgQMaYq40xy4wxy2JtVVJERIZuMFsfxwKP+98lTQL+aa19zhhzDYC19i5gMXAWsB5oA74YmnJFRETCbjDb+5cDE621rcaYs4AncJKQ4eDHB5wn1PEAERHp56Arav6tHjP8H0daa3/lv/4uf5OGdVxrrZ1krZ1urd1n1pqIiAyssbGRO+64Y8iPO+uss2hsbBzy46688koeeeSRIT8ujh10e7+1ttla2+r/fDGQbIzJ9399sOMDIiIi+xjUGTUREQmd/TVqvb29B3zc4sWLyc3NDVFV0s9SYIoxpswYkwJchrPlv48xZpzxbz0xxszD+flaP8jjAyIiIvsI1hw1EZGYcOO/1/DhluagPucRE7L52aeP3O/tN9xwAxs2bGDmzJkkJyczatQoxo8fz4oVK/jwww85//zzqampoaOjg+uvv56rr3aOOZWWlrJs2TJaW1tZtGgRCxYs4O2336awsJAnn3yS9PT0g9b28ssv893vfpeenh6OPvpo7rzzTlJTU7nhhht46qmnSEpK4vTTT+e3v/0tDz/8MDfeeCOJiYnk5OTw+uv77N6LSdbaHmPMdcDzOPH891hr1+x1BOAi4GvGmB6gHbjMWmuNMQMeH3DlDyIiIlFFjZqIiMtuuukmVq9ezYoVK3j11Vc5++yzWb16dd8ssnvuuYe8vDza29s5+uijufDCCxk9evQez/HJJ5/wr3/9iz/96U9ccsklPProo1x++eUHfN2Ojg6uvPJKXn75ZQ499FCuuOIK7rzzTq644goef/xx1q5dizGmb3vlL37xC55//nkKCwuHteUymvm3My7e67q7+n1+G3DbAI/bCMwIeYEiIhJz1KiJiPRzoJWvcJk3b94eA6NvvfVWHn/8cQBqamr45JNP9mnUysrKmDlzJgBz5syhqqrqoK+zbt06ysrKOPTQQwH4whe+wO233851111HWloaV111FWeffTbnnOOMBps/fz5XXnkll1xyCZ/5zGeC8CcVERGR/YnaM2od3b18uKWZtq4et0sREQmqzMzMvs9fffVVXnrpJd555x1WrlzJrFmz6Ojo2OcxqampfZ8nJibS03Pw743WDhwsmJSUxJIlS7jwwgt54oknOPPMMwG46667+OUvf0lNTQ0zZ86kvr5+qH80CZfmLVD3sdtViIiMXFuD2xW4JmobtfcqGzjr1jdYvTm4Z0lERMItKyuLlpaWAW9ramrC4/GQkZHB2rVreffdd4P2uocddhhVVVWsX78egH/84x+ceOKJtLa20tTUxFlnncUtt9zCihUrANiwYQPHHHMMv/jFL8jPz6empuYAzy6ueuI/4Ylr3K5CRGRkaivg/ybBjo/crsQVUbv1sdjjHJKvaWhjXlmey9WIiAzf6NGjmT9/PtOmTSM9PZ2xY8f23XbmmWdy1113cdRRRzF16lSOPfbYoL1uWloa9957LxdffHFfmMg111xDQ0MD5513Hh0dHVhr+d3vfgfA9773PT755BOstZx66qnMmKGjVxEruxA2rHW7ChGRkdm6AqwPtq2CMYe7XU3YRW2jVuhJxxjY1NDmdikiIiP2z3/+c8DrU1NTefbZZwe8LXAOLT8/n9Wrdye+f/e73z3ga/31r3/t+/zUU0/l/fff3+P28ePHs2TJkn0e99hjjx3weSWC5BRCyzbo7YbEZLerEREZHm+lc9lQ6W4dLonarY+pSYmMzUqjxqtGTUREZA/ZEwDrNGsiItEq0KB547NRi9oVNYCSvAxqG9rdLkNEJCJde+21vPXWW3tcd/311/PFL37RpYokbLKLnMvmzZBb7G4tIiLD5a1yLuN0RS2qG7WivHTe2aDUMRGRgdx+++1ulyBuyZ7gXDZvdrcOEZHhsnZ3oxa4jDNRu/URoNiTwbbmDjp7et0uRUREJHLkFDqXTWrURCRK7doJXa0waiy0boOu+DvuFN2NWl4G1sKWxn1nComIiMSt1GxIGeXMUxMRiUaBc2mHnOz/usq1UtwS3Y1av4h+ERER8TPG2f7YXOt2JSIiwxM4lzYp0KjF3zm16G7U8jIARfSLiIjsI7tQK2oiEr28lYCBshOcr+MwUCSqG7Wx2WkkJxpF9ItIVGtsbOSOO+4Y1mNvueUW2toO/D2wtLSUnTt3Duv5JYqpURORaOatcnYGZI2H1BxtfYw2iQmGIo8i+kUkuoW6UZM41X/otYhItGmoBE+Zs5U7rzQutz5GdTw/QJEnXStqIhI8z94A21YF9znHTYdFN+335htuuIENGzYwc+ZMPvWpTzFmzBgeeughOjs7ueCCC7jxxhvZtWsXl1xyCbW1tfT29vKTn/yE7du3s2XLFk4++WTy8/MpLy8/aCk333wz99xzDwBXXXUV3/zmNwd87ksvvZQbbriBp556iqSkJE4//XR++9vfBu0/iYRB/6HXmqUmItHGWwlTPuV87ikL/s/mKBD1jVpxXgarV211uwwRkWG76aabWL16NStWrOCFF17gkUceYcmSJVhrOffcc3n99depq6tjwoQJPPPMMwA0NTWRk5PDzTffTHl5Ofn5+Qd9nYqKCu69917ee+89rLUcc8wxnHjiiWzcuHGf525oaODxxx9n7dq1GGNobGwM5X8CCQUNvRaRaNW1C1q3Ow0aQF4ZrH0GfL2QkOhubWEU/Y2aJwNvWzetnT2MSo36P46IuO0AK1/h8MILL/DCCy8wa9YsAFpbW/nkk09YuHAh3/3ud/nBD37AOeecw8KFC4f83G+++SYXXHABmZmZAHzmM5/hjTfe4Mwzz9znuXt6ekhLS+Oqq67i7LPP5pxzzgnqn1PCQEOvRSRaeaudS0/p7ktft/+NpxK3qgq7qD6jBlCcp4h+EYkd1lp++MMfsmLFClasWMH69ev58pe/zKGHHkpFRQXTp0/nhz/8Ib/4xS+G9dwDGei5k5KSWLJkCRdeeCFPPPEEZ5555kj/aBJuGnotItEqcB4tz7+iFlhZi7Pkx+hv1DyK6BeR6JaVlUVLSwsAZ5xxBvfccw+tra0AbN68mR07drBlyxYyMjK4/PLL+e53v8vy5cv3eezBnHDCCTzxxBO0tbWxa9cuHn/8cRYuXDjgc7e2ttLU1MRZZ53FLbfcwooVK0LyZ5cQ0tBrEYlWgYas/9ZHiLtAkajfKxiYpaYVNRGJVqNHj2b+/PlMmzaNRYsW8bnPfY7jjjsOgFGjRnHfffexfv16vve975GQkEBycjJ33nknAFdffTWLFi1i/PjxBw0TmT17NldeeSXz5s0DnDCRWbNm8fzzz+/z3C0tLZx33nl0dHRgreV3v/tdaP8jSPBp6LWIRCtvJaTlQEae83V2ISQkx92KmtnfVphQmzt3rl22bNmIn8day/Sfv8BFc4r4+blHBqEyEYk3H330EYcffrjbZcSMgf57GmMqrLVzXSop6gTrZyR/Px86m+Err4z8uUREwuW+C2FXHXz19d3X3TrbSVG+5G/u1RUCB/r5GPVbH40xTkS/VtRERET2pKHXIhKNAjPU+ssr09bHaFScl0F1/S63yxARcdUxxxxDZ2fnHtf94x//YPr06S5VJK7rP/Q6MdntakREDs7XC42b4Ihz97zeUwY1S8FaZ2t3HIiNRs2TwZuf7MRai4mT/3EiElyx8P3jvffec7uE/SZLiks09FpEok1TrRPFP9CKWmcTtHt3n12LcVG/9RGciP727l7qd3W5XYqIRKG0tDTq6+vVZIyQtZb6+nrS0tLcLkUC+g+9FhGJBt4q5zIwQy0g8HUcbX+MmRU1cCL680elulyNiESboqIiamtrqaurc7uUqJeWlkZRUZHbZUiAhl6LSLTZe4ZaQP9ZaoVzwluTS2KjUesX0T+7xONyNSISbZKTkykrKzv4HUWiTaBR09BrEYkWDZVOFH924Z7Xx+GKWsxsfQSo9ba7XImIiEgEScvR0GsRiS7eSvBMhITEPa9PyYBR46ChypWy3BATjVpGShL5o1IU0S8iItJf39BrraiJSJTwVu17Pi3AU7r7DFsciIlGDaDIk0GNV42aiIjIHrIL1aiJSHSw1lkx2zvxMSDOZqnFTKNWnJdBTYO2PoqIiOxBQ69FJFq0e50I/r2DRAI8Zc73s+6O8Nblkthp1DzpbGlsp6fX53YpIiIikaP/0GsRkUjW4F8tO9CKGhYaq8NWkptip1HLy6DHZ9naFB8dtoiIyKD0H3otIhLJAtsaD3RGDeLmnNqgGzVjTKIx5n1jzNMD3HaSMabJGLPC//HT4JZ5cIFZajqnJiIi0o+GXotItDhoo9ZvllocGMocteuBj4Ds/dz+hrX2nJGXNDwl/llqtQ3tMMmtKkRERCKMhl6LSLRoqHIi+FMyBr49M98ZORIngSKDWlEzxhQBZwN/Dm05wzc+N40EoxU1ERGRPWjotYhEC2/l/oNEwBk54imLmxW1wW59vAX4PnCgpI7jjDErjTHPGmOOHHFlQ5ScmMD4nHTNUhMREelPQ69FJFo0VO4/SCQgr1QragHGmHOAHdbaigPcbTkw0Vo7A/gD8MR+nutqY8wyY8yyurq64dR7QMV56dR4FdEvIiLSR0OvRSQadHdAy5b9n08L8JSCtxp8sZ/0PpgVtfnAucaYKuAB4BRjzH3972CtbbbWtvo/XwwkG2Py934ia+3d1tq51tq5BQUFI69+L8WeDK2oiYiI7E1Dr0Uk0gUi9w+09RGcFbfeTmjZGvqaXHbQRs1a+0NrbZG1thS4DHjFWnt5//sYY8YZY4z/83n+560PQb0HVJyXwY6WTjq6e8P90iIiIpFLQ69FJNIdbIZaQKCRi4Ptj8Oeo2aMucYYc43/y4uA1caYlcCtwGXWWhuMAoeiOC8dgFoFioiIiOymodciEukCjddgVtQgLgJFhtSoWWtfDUTwW2vvstbe5f/8NmvtkdbaGdbaY621b4ei2IMJRPTXNOicmoiIBI8x5kxjzDpjzHpjzA0D3L7feaIHe2xYaOi1iEQ6b5UTfJQx+sD3yykCkxgXQ6+HMkct4mnotYiIBJsxJhG4HfgUUAssNcY8Za39cK+77jNPdAiPDa3+Q69zi8P60iIigxJIfHROU+1fYrLzfUxbH6NLQVYqqUkJChQREZFgmgest9ZutNZ24QRrnReGxwaPhl6LSKTzVjrR+4MRJ7PUYqpRM8ZQ5EnX1kcREQmmQqCm39e1/uv2NtA80cE+NrT6GjUFiohIBPL5nMj9gwWJBOSVaUUtGhXnZWjro4iIBNNA+3D2Dsza3zzRwTzWuWMoZ40Ghl43aUVNRCJQy1Yncv9gM9QCPKXQ7oX2xhAW5b7Ya9Q8GWzS1kcREQmeWqD/wa4iYI+lqQPMEz3oY/s9R+hmjWrotYhEssEmPgYEVt5iPFAk9hq1vHRaOnpoalMEsYiIBMVSYIoxpswYk4IzU/Sp/nc4wDzRgz42bDT0WkQi1WBnqAXEySy1mEp9hH4R/d42cjJyXK5GRESinbW2xxhzHfA8kAjcY61dE5gl6h9VcxHwNWNMD9DO7nmiAz7WlT9IdiFsWOvKS4uIHJC30onczxlkKm1gi2SMB4rEXKNWFIjob2hjWqEaNRERGTn/dsbFe113V7/PbwNuG+xjXdF/6HVistvViIjs5q1yIvcTB9mapGZBRr62Pkab4jzNUhMREdmHhl6LSKQKzFAbijhIfoy5Ri0nPZnstCRF9IuIiPSX7Z8KoHNqIhJpvJWDDxIJ8JRBQ1VIyokUMdeogbOqpuRHERGRftSoiUgkam90ovaHs6LWXAs9XSEpKxLEZqPm0Sw1ERGRPWjotYhEosD2xcHOUAvwlIL1QVNNsCuKGLHZqOWlU+ttx+cbcKaoiIhI/NHQaxGJRIFAkOFsfYSYTn6MyUatJC+Drh4fda2dbpciIiISGTT0WkQiUcMwV9TiYJZaTDZqRXm7I/pFRETET0OvRSTSeCshs8CJ3B+KUWMhKV0ratGm2KOIfhERkX1kF+qMmohElobKoa+mgbNLwFOqFbVoU+RJB1BEv4iISH/9h16LiEQCb/XQEx8D8spieuh1TDZqacmJjMlKVUS/iIhIfxp6LSKRpKfLidgfapBIgMffqNnYDBCMyUYNnFlqOqMmIiLSj2apiUgkadzkROyPZEWtuw1atwe3rggRu42ax4noFxERET81aiISSQLny0ayogYxGygSs41aSV4GW5va6e71uV2KiIhIZNDQaxGJJIHzZcMJE+n/uBg9pxazjVpRXgY+C1sataomIiICaOi1iESWhkpIznCi9ocjtwRMQswmP8Zso9YX0a/kRxEREYeGXotIJPH6o/mNGd7jk1Igu0hbH6NNcZ4/ol+z1ERERHbT0GsRiRQNlcMPEgnIK9WKWrQZn5NOUoJRRL+IiEh/GnotIpHAWuds2XDPpwV4SnVGLdokJhgm5KYrol9ERKS/7Akaei0i7mvdDj3tw098DPCUwa466GwJTl0RJGYbNXC2P9Yool9ERGS3nEI09FpEXBc4VzbirY/+x8fgqlpMN2oleRnUakVNRERkt75Zatr+KCIuGukMtYAYnqUW041akSeD+l1d7OrscbsUERGRyNDXqNW6W4eIxDdvlROtn1M8sueJ4VlqMd2oFec5Ef212v4oIiLi0NBrEYkEDZVOtH5SysieJz0X0j0xmfwY242axx/Rr+2PIiIiDg29FpFI4K10ovWDwVOmrY/RJrCipoh+ERERPw29FpFIEIwZagF5ZVpRizajM1NIT07U0GsREZH+NPRaRNzU2QJtO0c+Qy3AUwqNNdAbW7kUMd2oGWMoycugpkFn1ERERPpo6LWIuCkQ/DHSxMcATxnYXmiqCc7zRYiYbtTAmaVWqxU1ERGR3TT0WkTcFKwZagF9s9Ria/tjzDdqRZ4MahrasNa6XYqIiEhk0NBrEXFTsGaoBcToLLWYb9SK8zLY1dWLt03vGoqIiAAaei0i7mqodCL103KC83xZ4yExNeZmqQ26UTPGJBpj3jfGPD3AbcYYc6sxZr0x5gNjzOzgljl8iugXERHZi4Zei4ibvFXB2/YIkJAAnolxvfXxeuCj/dy2CJji/7gauHOEdQWNIvpFRET2oqHXIuImb2Xwtj0GeMqgoSq4z+myQTVqxpgi4Gzgz/u5y3nA363jXSDXGDM+SDWOSKBRU0S/iIiIn4Zei4hberudKP1grqjB7llqMZRLMdgVtVuA7wO+/dxeCPTPw6z1X7cHY8zVxphlxphldXV1Q6lz2EalJpGXmaKIfhERkQANvRYRtzTVOFH6wZqhFuApg65W2LUzuM/rooM2asaYc4Ad1tqKA91tgOv2aWettXdba+daa+cWFBQMocyRKfYool9ERGQPGnotIm4I9gy1gEDjF0OBIoNZUZsPnGuMqQIeAE4xxty3131qgeJ+XxcBEbPxvSgvQ2EiIiIi/WnotYi4Idgz1AJicJbaQRs1a+0PrbVF1tpS4DLgFWvt5Xvd7SngCn/647FAk7V2a/DLHZ5iTwabG9vp9cXOnlUREZER0dBrEXGDt9KJ0s8KcpxF7kTAxNQstWHPUTPGXGOMucb/5WJgI7Ae+BPwn0GoLWiK89Lp7rVsb+5wuxQREZHIoKHXIuKGhkpnm2JCkMc5J6c5b0DF0Ipa0lDubK19FXjV//ld/a63wLXBLCyYij27I/on5Ka7XI2IiEgE6D/0Orf4wPcVEQkWb3Xwg0QCPKVxd0Yt6vVF9OucmoiIiENDr0Uk3KwNzQy1AE+Ztj5Gm8LcdIyBGq8i+kVERAANvRaR8Nu104nQD3aQSEBeKbRug67YWJyJi0YtJSmB8dlp1GpFTURExKGh1yISboHzY6FcUYOY2f4YF40a+CP6NUtNRETEoaHXIhJugQYqZGfU1KhFpWJPBjUN2vooIiJDZ4w50xizzhiz3hhzwwHud7QxptcYc1G/66qMMauMMSuMMcvCU/EgqVETkXBqqASMP0o/BGJsllr8NGp56Wxv6aCju9ftUkREJIoYYxKB24FFwBHAZ40xR+znfr8Gnh/gaU621s601s4NabFDlV2kM2oiEj7eSucNouS00Dx/ugdSc2ImUCR+GjVPBtbC5katqomIyJDMA9Zbazdaa7uAB4DzBrjf14FHgR3hLG5ENPRaRMKpoTJ0QSLgbOnOK9WKWrRRRL+IiAxTIVDT7+ta/3V9jDGFwAXAXezLAi8YYyqMMVfv70WMMVcbY5YZY5bV1dUFoexB0NBrEQknb1XozqcFxNAstbhp1EoCjZoi+kVEZGjMANfZvb6+BfiBtXag/fXzrbWzcbZOXmuMOWGgF7HW3m2tnWutnVtQUDCigget/9BrEZFQ6mpzovPzSkP7Op4yZ6i2L/qPO8VNozYmK5WUpARF9IuIyFDVAsX9vi4C9u5s5gIPGGOqgIuAO4wx5wNYa7f4L3cAj+NspYwMGnotIuHSl/gYwq2P4ASK+LpjIigpbhq1hARDUW66IvpFRGSolgJTjDFlxpgU4DLgqf53sNaWWWtLrbWlwCPAf1prnzDGZBpjsgCMMZnA6cDq8JZ/ABp6LSLhEuoZagGBRjAGAkWS3C4gnIryFNEvIiJDY63tMcZch5PmmAjcY61dY4y5xn/7QOfSAsYCjxtjwPmZ+09r7XOhrnnQNPRaRMIl0DiFekUtcAbOWwWcGNrXCrG4atSKPel8UNvodhkiIhJlrLWLgcV7XTdgg2atvbLf5xuBGSEtbiQ09FpEwsVb5UTnp3tC+zo5RZCQHBPJj3Gz9RGc5MfGtm6aOxRDLCIiAqhRE5Hw8FY6QSJmoHymIEpIhNySmNj6GF+NmkcR/SIiInvQ0GsRCYdQz1DrL69MK2rRpi+iX+fUREREHBp6LSKh5uuFxk2hn6EWECOz1OKqUSvOSwegVsmPIiIiDg29FpFQa97sROaHOvExwFMGHU3Q1hCe1wuRuGrUctKTyUpN0tZHERGRAA29FpFQC1fiY0CgIYzy7Y9x1agZY5yIfq+2PoqIiAAaei0ioReuGWoBMTJLLa4aNXAi+rWiJiIi4qeh1yISag2VTmR+4I2hUOubpaZGLaoU52VQ423DWut2KSIiIu5Ly4HkTA29FpHQ8VY5kfkJieF5vZQMGDU26gNF4q9R86TT0e2jrrXT7VJERETcZ4wTKKJZaiISKt7K8G17DPCUQUNVeF8zyOKuUSsZrYh+ERGRPWRP0NZHEQkNa52GKVxBIgExMEst7hq1wNBrRfSLiIj4ZRdpRU1EQqPdC51N7qyoNW+B7o7wvm4QxV2jVuQJrKipURMREQE09FpEQiewqhWuYdcBnlLAOoO2o1TcNWrpKYnkj0rV1kcREZEADb0WkVAJ9wy1gBiYpRZ3jRpAcV46Ndr6KCIi4tDQaxEJFddW1KJ/llp8NmqeDDZp66OIiIhDQ69FJFQaqmDUOCcyP5wy8yFllFbUok1xXjpbmzro6fW5XYqIiIj7NPRaRELFWxX+1TRwRo94SqN6llpcNmoleRn0+ixbm6I3BUZERCRoNPRaRELFjRlqAZ5SbX2MNsVKfhQREdlNQ69F4tora7fz1vqdwX/i7g5npT7cQSIBeWXOipovOnfRxWejludv1BQoIiIi4tDQa5G409rZw3ceWsmX/rqMHzz6QfBfoLEasC6uqJVBbye0bHXn9UcoLhu18TlpJCYYRfSLiIgEaOi1SFxZUdPI2be+wePv1zK9MIdabzveXV3BfZHA+TA3zqj1f90oPacWl41aUmIC43PStKImEgqdrdDudbsKERkqDb0WiQu9Psvt5eu56M636em1PHD1cdyw6DAAVm9pCu6LuTVDLSDKZ6kluV2AWxTRLxIiT14LDRvhmjfcrkREhqL/0OvcYrerEZEQ2NLYzrceXMF7lQ2cc9R4fnXBdHLSk2lqc96gWbW5iYVTCoL3gt5KJyI/Mz94zzkUOcVgEqM2UCR+G7W8dF5ZW+d2GSKxpbcb1r8EXa3QugNGjXG7IhEZrP5Dr9WoicScxau2csOjH9Drs/z24hlcOLsQYwwAORnJlORlsHpzCFbUPGVOYJEbEpOd72dRuqJ20K2Pxpg0Y8wSY8xKY8waY8yNA9znJGNMkzFmhf/jp6EpN3hK8jLY2dpJe1ev26WIxI7apU6TBlD9lru1iMjQaOi1SEza1dnD9x9ZyX/ev5yy/Eye+cZCLppT1NekBUwvzGFVsBs1byV4Jgb3OYcqimepDeaMWidwirV2BjATONMYc+wA93vDWjvT//GLYBYZCoHkx1qdUxMJng3lYBIgOQOq3nS7GhEZCg29Fok5H9Q2cs4f3uThilquPXkSj3zteErzMwe877TCHGoa2mlsC1KgiM8H3mr3Eh8DPGVRu/XxoI2adfjfIifZ/2FDWlUYFHkU0S8SdBvLYcJsKDkOqrSiJhJVAkOv1aiJRD2fz3Lnqxv4zB1v09Hdy7++cizfO+MwkhP3/6v/9MIcAFZvbg5OES1bnWh8t4JEAvLKoL0BOoK8WhgGg0p9NMYkGmNWADuAF6217w1wt+P82yOfNcYcuZ/nudoYs8wYs6yuzt3zYcV56QCK6BcJlvZG2FwBk06G0vlQ9xHsCsHwTBEJjcDQ6yZtfRSJZtuaOrj8L+/x6+fWcvqRY3n2+oUce8jogz4u0Kh9sLkxOIUEzoVFwooaROWq2qAaNWttr7V2JlAEzDPGTNvrLsuBif7tkX8AntjP89xtrZ1rrZ1bUBDERJlhKBiVSlpyAjVKfhQJjqo3wPrgkJOhdKFznc6piUQXDb0WiWrPrd7Gmb9/nRU1jfzmwqO4/XOzyc1IGdRjgx4o0hfNXxqc5xuuvoj+KlfLGI4hzVGz1jYCrwJn7nV9c2B7pLV2MZBsjHEph3NwjDEUKaJfJHg2lDvbpoqOhgmzdE5NJBpp6LVIVGrr6uGHj33ANfdVUJKXwTPfWMglRxfvExhyMEENFPFWOdH4OS6nyPYNvY7BFTVjTIExJtf/eTpwGrB2r/uMM/6/CcaYef7nrQ96tUFW7EmnxqutjyJBsbEcShdAUooTh1t8jM6piUQbDb0WiTqrNzdxzh/e5IGlNVxz4iQeueZ4yvYTGHIwQQ0U8VY60fiJySN/rpFIzYKM/Jjd+jgeKDfGfAAsxTmj9rQx5hpjzDX++1wErDbGrARuBS6z1kZ84EhJXga1DW1EQakikc1b7Qy5nnTy7utK58OONdDW4F5dIjI0/Ydei0hE8/ksd7++gQvueIu2zl7uv+oYblh0GClJQ9owt4egBooEZqhFgryyqFxRO+jAa2vtB8CsAa6/q9/ntwG3Bbe00CvOy6Cls4em9u5B798VkQFsLHcuD+nfqPU7p3b4p8Nfk4gMnYZei0SF7c0dfOehlby5fidnHDmWmz5zFJ7Mkf8uO60wG4BVm5tYMGWEp5i8lXDkBSOuKSg8ZbDpXberGLLht9wxoC+iX8mPIiOzoRyyxkPB1N3XTZgNSena/igSTfpmqSn5USRSvfjhds685XUqqr3872emc9flc4LSpAHkZqRQnJc+8kCR9kZo97ofJBLgKXW+r/UEaUZcmMR1o9YX0a9ZaiLD5+uFytec1bT+h5aTUqB4ngJFRKJJ/xU1EYko7V29/Nfjq/jK35cxITedf399AZ+dVzLkwJCDCUqgSCBhMZK2PlofNNW4XcmQxHmjFlhRU6MmMmxbVzrvmvU/nxZQugC2r9Y5NZFooaHXIhHpwy3NfPq2N7n/vU1cfcIhPPafxzN5zKiQvNa0whw2NbTR1DaCUKFImaEWEKWz1OK6UctOSyYnPVkR/SIj0Xc+7aR9bytdAFjY9E44KxKR4dLQa5GI4vNZ/vzGRs6//S2a27v5x5fn8aOzDic1KTFkr9kXKLJlBKtqkTJDLaBvlpoatahSnKeIfpER2VAOY6fDqDH73lY4B5LSdE5NJJpo6LVIRNjR0sGVf13KL5/5iBMOLeC5b57AwikFIX/daROcRm1E2x+9VU4kfmpWcIoaqVFjnXPzUTb0+qCpj7GuJC+DtVtb3C5DJDp17XJSlI69ZuDbk1KdAdhVb4S3LhEZvuwi2PCy21WIxLVX1m7new9/wK6uHn55/jQ+f0zwz6LtjyczhSJP+ggbtcrI2fYIzm4BT6m2PkabYk8Gtd52fD7NUhMZsuq3wde9Zyz/3koXwLZVTgKUiEQ+Db0WcU1Hdy8/fXI1X/rrMsZkp/Hv6xZw+bETw9akBUwvzBlZ8mNDVeQEiQRE4Sy1uG/UivIy6Or1saOl0+1SRKLPhnJITIWJx+//PjqnJhJdNPRaxBVrtzVz7m1v8vd3qvnygjKeuPZ4pox1Z+vg9KIcquuHGSjS0+VE4UfSiho4jaO3Cmz0LM7EfaNW7FFEv8iwbSyHkmMhOX3/9ymc6zRziukXiQ6K6BcJK2st975Vybm3vUXDrm7+9qV5/OScI0IaGHIwIwoUaapxovAjJUgkwFMK3W3QuiMoT9fR3RuU5zkQNWqK6BcZnpZtsOPDgWP5+0tO859TU6Mm0csYc6YxZp0xZr0x5oYD3O9oY0yvMeaioT42YmjotUjY1LV08sW/LuXGf3/Igsn5PPfNhZx4aOgDQw5mRIEifYmPEbaiFuTkx4vueptvPbgiKM+1P3HfqBXmOisBiugXGaKNrzqXBzqfFlA6H7Z9AB0jHKAp4gJjTCJwO7AIOAL4rDHmiP3c79fA80N9bETRippIWJSv28Gi37/O2xvq+cV5R/KXL8wlf1Sq22UBIwwUibQZagFBnKW2q7OHD7c0U+Q5wI6iIIj7Ri0tOZGx2anUNCiiX2RINpRDxmgYd9TB71u6wNkGsend0NclEnzzgPXW2o3W2i7gAeC8Ae73deBRYMcwHhs5NPRaJKQ6unv5+VNr+OK9Sxmdmcq/r1vAFceVhj0w5GCGHSjSUOlE4Y8aG/yiRiK3BDBBWVFbWdOIz8LsiZ6R13UAcd+ogRPRrzNqIkNgrbOiVnYiJAzi20jR0ZCYou2PEq0KgZp+X9f6r+tjjCkELgDuGupj+z3H1caYZcaYZXV1dSMuetg09FokZD7e3sL5t7/FX9+u4srjS3nyuvlMHRchs8b2Mq3QHyjSPsRAEW+lcx4swhpPklIgpygos9Qqqr0AzC5RoxZyxZ4MarX1UWTwdnwErdsOfj4tIDndCRVRoybRaaDfNvaODbsF+IG1du/T5YN5rHOltXdba+daa+cWFLh8RkVDr0WCbktjO+fd9hY7Wzu598qj+fm5R5KW7F5gyMEEAkXWDHVVzVsVedseA4I0S61ik5dDx44iJz155DUdgBo1nIj+rc0ddPX43C5FJDpsLHcuB3M+LaB0PmxdCR3NoalJJHRqgeJ+XxcBe3cxc4EHjDFVwEXAHcaY8wf52MiTXQTNm92uQiSmPLi0ho6eXh655nhOPmyM2+UcVKBRG9I5NWudRi3SgkQCgjBLzeezLK/2MifE2x5BjRrgRPRb67zTISKDsKEcRk+G3OKD3zdg4nywvVDzXujqEgmNpcAUY0yZMSYFuAx4qv8drLVl1tpSa20p8Ajwn9baJwbz2IikodciQdXrszy8rIYFk/Mpzc90u5xB8WSmUJg7xECR1u1OBH7ErqiVwa466GwZ9lOsr2uluaMn5NseQY0a0C+iX+fURA6upxOq3xraahpA8TxISNb2R4k61toe4DqcNMePgIestWuMMdcYY64ZzmNDXfOIZU9AQ69FgueNT+rY0tTBZ+eVuF3KkAw5UKQvmr80JPWMWKAub/WwnyJwPi0cK2pJIX+FKBBo1BTRLzIINUucd8sGez4tICUTCmerUZOoZK1dDCze67q9g0MC1195sMdGvJwi57J5y9BWzkVkQA8urSEvM4XTDo+wJMSDmF6Uw3NrttHc0U122iDOYwWCOiJ56yM42x/HTRvWU1RUe8nLTKEsDCujWlEDxmWnkZxoFNEvMhgby8EkOpH7Q1W6ALa8D52twa9LRIKnb+i1zqmJjNTO1k5e/HA7F84uJCUpun71nuY/pzboVTVvJZgEfxR+BArCLLXl1V5ml3jCMk4huv62hEhigqEwN11bH0UGY0M5FM11Zi0NVd85Nc1TE4lofUOv1aiJjNRjy2vp8VkuPTr6VqenD7VRa6h0woiSUkJY1Qik50K6Z9iBIvWtnWzcuSss2x5BjVqf4jxF9IscVFuDsyI21PNpAcXHQEISVL0V3LpEJLg09FokKKy1PLC0hrkTPUweE5nz0g4kry9QZJCJzd5KyCsNaU0j5ikb9iy15ZsagfCcTwM1an2KPBnUeLX1UeSAKl8H7NDPpwWkjoIJs5wwEhGJXBp6LRIUy6q9bKzbFZWraQHTC3NYVds4uDt7qyI3SCRgBLPUKqq9JCcajioaxq6iYVCj5lecl07Dri52dfa4XYpI5NpYDilZUDhn+M9RugA2V0DXruDVJSLBp6HXIiP2wJIaRqUmcfZR490uZdimF+VQVd9Gc8dBxnV0tjjR95EaJBKQVwZNNdA79N/5l1d7OXJCTtgGlatR8yv2KKJf5KA2lEPZQkgcRPLT/kxcAL4eJz1SRCKXhl6LjEhzRzfPrNrCuTMnkJESvUHrgw4UCWwnjNQZagGeMuf3kKaaIT2sq8fHytrGsG17BDVqffoi+uvVqIkMqGEjNFYP/3xaQMkxTmqkYvpFIpuGXouMyJMrttDR7eOyKN72CEMIFOmboRbhjVr/iP4hWLOlic4enxo1NxR70gF0Tk1kfzaUO5fDPZ8WkJoFE2bqnJpIpNPQa5EReXDpJg4fn93X6ESrQQeK9M1QKw11SSPTN/S6akgPC+eg6wA1an55mSlkpiRSo+RHkYFtLHe2Qo2ePPLnKl0AtcugS//eRCJW/6HXIjIkqzc3sXpzM5+dVxyWeVuhNq0wexBbHyud6Pv03LDUNGxZEyAxdciBIss3eSnMTWdsdlqICtuXGjU/Y4wT0a8zaiL78vU6iY+TTnLS4EZq4gLwdUPt0pE/l4iEhoZeiwzbg0trSE1K4LwZhW6XEhTTC3Oo3LnrwIEiDZWRv+0RICEBPBOHtPXRWktFtZe5peFbTQM1anso8mRQ06CtjyL72PI+dDSN/HxaQMmxYBJ0Tk0kkmnotciwtHf18sSKzZw1fTw5GSMI34oggUCRNQfa/uitjPwgkQBPGTRUDfrumxvb2d7cGdZtj6BGbQ/FeenUeNuw1rpdikhkCZxPO+Sk4DxfWjaMn6FzaiKRTEOvRYbl2dVbaenoierZaXs7aKBIbw801kT++bQAT6lzRm2Qv/MHzqfNLlGj5ppiTwZtXb007OpyuxSRyLKxHMYdBZn5wXvO0gXO1sdurWKLRCQNvRYZlgeW1lA6OoNjyvLcLiVoRo9KZUJOGqv216g11YDtjY6tj+Cs/HW1QFv9oO5eUe0lIyWRw8ZlhbiwPalR66cvol+BIiK7dbY6M89Gmva4t4kLoLfLCRURkcikodciQ7KxrpUllQ1cenRJTISI9DetMGf/K2qB817RtPURBh0oUlHtZWZxLkmJ4W2d1Kj1U5yniH6RfVS/5QR/BOt8WoDOqYlEPg29FhmSB5fVkJhguHBObISI9De9MIeNO3fRMlCgSLTMUAsYwiy1XZ09fLS1mblhPp8GatT2UOxxVtQU0S/Sz4ZySEqDkuOC+7zpuTBuus6piUQyDb0WGbSuHh+PVtRy6mFjGJMVvgj3cJle5A8U2TJAoIi3yom8zxof3qKGK7fEuRzELLWVNY34LMxWo+auzNQkRmemKKJfpL+N5TDxeEgOwQ+d0oXOtsrujuA/t4iMXGDodet2tysRiXivrN3OztYuLpsXOyEi/QUCRVbVDrD90VvpRN4nRElrkZzuzFMbxNbHZf4gkVlhDhIBNWr7KMpTRL9In+YtULc2+NseAybOh95O2FwRmucXkZEJDL1u0vZHkYN5YGkN47LTOGFKgdulhMQBA0UaqqJn22NAXtmgtj5WVHs5dOwoctLDP2rhoI2aMSbNGLPEGLPSGLPGGHPjAPcxxphbjTHrjTEfGGNmh6bc0Cv2OBH9IgJsfNW5DHaQSMDE4wCjc2oikUpDr0UGZUtjO699XMclc4vCHjgRTgMGilgbXTPUAjxlB11R8/ksyzd5wz4/LWAwf5M6gVOstTOAmcCZxphj97rPImCK/+Nq4M5gFhlOxXkZbPa20+vTLDURNpRDZgGMOTI0z5/ugXHToFqNmkhE0tBrkUF5eJkzxuLiubG57TFgwECRXTuhqzV6ZqgFeEqhdRt07X+BZn1dKy0dPcyZ6M6ohYM2atbR6v8y2f+xdxdzHvB3/33fBXKNMVFymnBPxZ4MenyWrU3a/ihxzudzzqcdclJo95xPXOCcU+vpDN1riMjwaOi1yEH1+iwPLathweT8vlFPsWraQIEigUCOaNz6CNBYvd+7BAZdR/KKGsaYRGPMCmAH8KK19r297lII1PT7utZ/3d7Pc7UxZpkxZlldXd0wSw6tvoh+nVOTeLdjDeyqC935tIDSBdDTAZuXh/Z1RGToNPRa5KDeWr+TzY3tXHp0bK+mwe5AkT22P0bbDLWAQcxSW1blJS8zhdLR7jTgg2rUrLW91tqZQBEwzxgzba+7DDTRb5+9g9bau621c621cwsKIvOgZYn/nRCdU5O4t6HcuQzV+bSAicc7lzqnJhKZNPRa5IAeXFqDJyOZTx0x1u1SQi5/VCrj9w4UCTQ6uRPdKWq4BjFLbfkmL7NLPK4NLx/SfiZrbSPwKnDmXjfVAv3fRigCovK7+oTcdBIM1GqWmsS7jeWQP3V3mECoZOTBWJ1TE4lY2YU6oyayH/Wtnbzw4TY+M7uI1KREt8sJi2mFOXs2at5KJ+o+FGN8QindA6k5+52lVt/aSeXOXcwtdWfbIwwu9bHAGJPr/zwdOA1Yu9fdngKu8Kc/Hgs0WWu3BrvYcEhOTGB8Tjo1Xm19lDjW3QHVb4d+NS1g4nzY9B70dIXn9URk8LILNfRaZD8ef38z3b02LrY9BkwvzKFy5y5aO3ucK7xV0bftEZyt3Z6J+936uHxTI+De+TQY3IraeKDcGPMBsBTnjNrTxphrjDHX+O+zGNgIrAf+BPxnSKoNkyJPOjVaUZN4VvOuc24s1OfTAkoXQE87bHk/PK8nIoOnodciA7LW8q8lm5hdksuhY7PcLidsphfmYC2sCayqNVRGX5BIwAFmqVVUe0lONH3n8tyQdLA7WGs/AGYNcP1d/T63wLXBLc09xXkZvP5xZIadiITFhnJISILS+eF5vYn+16l+E0qOCc9risjg9B96HfhcRKio9rKhbhe/ufAot0sJq2n+xmXV5iaOKUp3Iu7zSt0targ8ZbB2Mfh6IWHPrasV1Q0cOSGHtGT3trTG7kS+ESj2ZLCjpZOO7l63SxFxx8ZyKJoHqWF6hzBzNIw5QoEiIpFIQ69FBvTA0hoyUxI5+6ionEg1bAVZqYzLTnOSH6M1mj8grwx83fsEJnX1+FhZ2+TqtkdQozagktFORH+tzqlJPNpVD1s/CN/5tIDAOTWdgxGJLBp6LbKP5o5unvlgK+fOLCQz9aAb1GLO9CJ/oEi0N2qBId17bX9cs6WJrh4fc9WoRZ5ijyL6JY5VvgrY8J1PCyhdAN27YMuK8L6uiByYhl6L7OPfK7fQ3t3LZXEUItLf9MIcNu7cRWfdBueKaAwTgf3OUgsMup6tRi3yBKbKK6Jf4tKGcieudsI+R1NDq/85NRGJHBp6LbKPB5fWcNi4LI4qci9owk2BQBFv7Trnd4Z0dxuaYcspgoTkfVbUlm/yUuRJZ2y2uyMH1KgNoGBUKilJCYrol/hjLWx8FcoWQmKYt3KMKoCCw3ROTSQSaei1SJ81W5r4oLaJy44udm0QstsCgSLdOzc4QSLR+t8hIRFyS/ZYUbPWsqzK6/r5NFCjNqCEBKOIfolP9RugqSb859MCJs6HTe9Cb487ry8iA9PQa5E+Dy2tISUpgfNnFbpdimsCgSKpLTW7z3lFK0/pHkOva73t7GjpVKMWyYo9GWxSoybxZmO5cxnu82kBpQugqxW2rnTn9UVkYBp6LQJAR3cvj7+/mUXTxpGbkeJ2Oa6aPmEUnq6t0RskErDXLLXlm5zzaWrUIlhxnlbUJA5tKHe2AOQd4s7r65yaSGTS0GsRAJ5dvZXmjh4ujdMQkf6Oy+8kmR46sie6XcrIeMqgownaGgAnSCQzJZGpETDEXI3afpTkZdDc0UNTu949lDjR2wNVbziraW7tNc8aC/mH6pyaSKTpP/RaJI49sKSGiaMzOLZstNuluG5WltPYVPaOcbmSEQokVvpX1SqqvcwsySUp0f02yf0KIlRfRL9W1SRebK6Azmb3zqcF6JyaSOTR0GsRNta18l5lA5fMLSYhIUrDM4JoUtJOAD7YletuISPVN0utitbOHj7a2sycEve3PYIatf3qi+jXLDWJFxvLAQNlJ7pbR+kCp2Hc9oG7dYjIbhp6LcJDy2pJTDBcPKfI7VIiQnZ7Ld0k8V59utuljEygUWuoZGVNIz7r/vy0ADVq+7F7RU0R/RInNpTDhJmQkeduHaULnMvqt9ytQ0R209BriXPdvT4eqajllMPGMMbl2VoRo6GS+qSxfLCl1e1KRiYlE0aNBW8lFdVejIFZWlGLbDkZyWSlJVGjFTWJBx3NULvUvbTH/rLGwejJOqcmEkmMcbY/aui1xKlX1u5gZ2snlylEZDdvJR1ZJWyoa2VXZ5QfV/CUQUMVFdVeDh2TRU56stsVAWrUDkgR/RI3qt4E2+v++bSAifOh+h3w9bpdiYgE5BRqRU3i1oNLaxibncqJhxa4XUpksBYaqkgaXYa18OHWZrcrGhlPKdZbxfJN3ojZ9ghq1A5IEf0SNzaWQ3IGFB/jdiWO0gXQ2QTbVrldiQgAxpgzjTHrjDHrjTE3DHD7ecaYD4wxK4wxy4wxC/rdVmWMWRW4LbyVB1G2GjWJT1ub2nl13Q4unlMcEUmAEaHdC51N5BROBWBVbZPLBY1QXhk0b6azoz0i5qcF6G/bAZTkZVDrbcda63YpIqG1oRwmHg9JqW5X4uibp6ZzauI+Y0wicDuwCDgC+Kwx5oi97vYyMMNaOxP4EvDnvW4/2Vo701o7N9T1hkx2IbRuUyKrxJ1HltXis3DJXG177OOPss8aP4Wx2ams3hzljZqnDIOl2OxQoxYtivMy6OzxUdfS6XYpIqHTVAv1n0TG+bSAnEJnv7jOqUlkmAest9ZutNZ2AQ8A5/W/g7W21e5+Vy8TiL13+LIngPU5zZpInPD5LA8uq2H+5NGUjM5wu5zI0eA0anjKmF6YwwfR3qj5Z6lNS2+gNIL+P6tRO4C+5EcFikgs21DuXEbK+bSA0gVQ/Tb4fG5XIlII1PT7utZ/3R6MMRcYY9YCz+CsqgVY4AVjTIUx5ur9vYgx5mr/tslldXV1QSo9iDT0WuLQWxt2Uutt59KjS9wuJbJ4A43aRKYV5kR/oIjHadSO9bRgTOTMyFOjdgDFec5cCEX0S0zbWO7E0o7ZeyeXy0oXQEcjbF/tdiUiA/3U3mfFzFr7uLX2MOB84L/73TTfWjsbZ+vktcaYEwZ6EWvt3dbaudbauQUFERhYoKHXEoceWFpDbkYypx8x1u1SIou3yvndISWT6YU5UR8oUm+z2GVTOTK93u1S9qBG7QCK+mapaUVNYpTPBxtfdbY9RtA7SIDOqUkkqQX6H04pAvabqmGtfR2YZIzJ93+9xX+5A3gcZytl9NHQa4kzDbu6eGHNNi6YVUhacqLb5USWhqq+VajphTlAdAeKVGxqZJMdSwnb3S5lD2rUDiAtOZGCrFRF9Evs2r4K2uojb9sjQG4x5E7UOTWJBEuBKcaYMmNMCnAZ8FT/OxhjJhv/fhljzGwgBag3xmQaY7L812cCpwPRuUysodcSZx5bXkt3r+UybXvcl7ey71zXmOw0xmRFd6BIxSYvNYwluyOyZkUmuV1ApCv2pOuMmsSuwPm0Q05ytYz9Kl0I655xVv4S9L6SuMNa22OMuQ54HkgE7rHWrjHGXOO//S7gQuAKY0w30A5caq21xpixwOP+Hi4J+Ke19jlX/iAjpaHXEkestTy4tIZZJblMHZfldjmRpbvDecPGv6IGzqraqihu1JZXezl8VAkJjZH1O4catYMoyctgaZXX7TJEQmNjuXM2LWuc25UMrHQ+rLgPdnwI46a5XY3EMWvtYmDxXtfd1e/zXwO/HuBxG4EZIS8wXDT0WuLE8k2NfLKjlV9fON3tUiJP4ybAgqe076pphTmUr9tBW1cPGSnR1V509fhYWdvEFw85BDZ1OMm2gTO5LouMdjGCFedlsLWpne5eJc9JjOluh+p3IiuWf286pyYSWTT0WuLEg0s3kZmSyDlHRcYv7BElkPiYt+eKms/Ch1uiL1BkzZYmunp8FJQ4w7v7Rg9EADVqB1HsycBnYWtjh9uliATXpnegtzMyz6cFeCZCTonOqYlECg29ljjQ0tHNv1du5dMzJpCZGl2rQ2HRb4ZawPQif6BIFG5/rKh2ds6VHepfPfWqUYsaRYGIfp1Tk1izoRwSU2Di8W5XcmClC5wVNRt784NFoo6GXkscePqDrbR393Lp0cUHv3M88lZCyijIzO+7amx2GgVZqVHbqBV50skvnAQm0Rk9ECHUqB1EsSL6JVZtLIfiYyAl0+1KDqx0vpNMWbfW7UpEREOvJQ48sGQTU8dmMbM41+1SIpO3yjmfttdYn+mFOVGX/GitZVm1l7kTPZCY7HyP09bH6DE+J43EBKOIfoktrTtg26rITXvsr3SBc6ntjyLu09BriXEfbmlmZW0Tlx5djIm0+aKRoqFyjyCRgGmFOazf0UpbV/Rsja71tlPX0smciR7nirwybX2MJkmJCUzITaPG2+52KSLBs/E15zKSz6cF5E6E7CI1aiKRQI2axLiHltWQkpjABbMK3S4lMvl8zopavyCRgKP8gSIfbY2eQJHlm5zzabMDjZqnTCtq0aYkL0NbHyW2bCyHtFwYP9PtSg7OGJ1TE4kUabkaei0xq6O7l8eW13LGtHF4MlPcLicytWx1gsg8+zZqgUCRD2qjZ/vjsiovmSmJTB3rn5WXVwbtDdARGX8GNWqDUOzJoFZhIhIrrHWCRA45ERIS3a5mcErnw6462Pmx25WIxLfA0GutqEkMen7NNpo7evisQkT2LxC0McDWx2gMFKmo9jKzJJekRH9LFPhzRUigiBq1QSjOy2Bna1dU7bkV2a+dH0PLlsien7a3vnNqb7hbh4g4Q68VJiIx6IElNZTkZXDsIaPdLiVyDTBDrb9oChRp7exh7bZm5kzM231lYKUwQrY/qlEbhCKPE9Ffq3NqEgs2lDuX0XA+LcBTBlkToEqDr0Vcp6HXEoOqdu7inY31XHp0MQkJChHZr4ZKJ8I+Z+BVx2gKFFlZ04jPsjtIBHY3oBESKKJGbRCK8xTRLzFkY7nT+AywbSFiBc6pVb2pc2oibtPQa4lBDy2rIcHARXOK3C4lsnkrIbfYibIfwPQoChSpqPZiDHuOYUjNgox8rahFk8AsNUX0S9Tr7XaanWhaTQsonQ+7dkD9ercrEYlvGnotMaan18fDFbWcctgYxmanuV1OZAvMUNuP6YVOoMiqKAgUWVbt5dAxWeSk79V0ekp1Ri2a5I9KIT05kZoGbX2UKFe7FLpao+t8WkDpQudS59RE3KWh1xJjytfVUdfSyaVHl7hdSuRrqBww8TFgbHYq+aNSWbU5slfUfD7L+9Xe3bH8/UXQLLWDNmrGmGJjTLkx5iNjzBpjzPUD3OckY0yTMWaF/+OnoSnXHcYYivPSqVHyo0S7DeVgEqDsBLcrGbq8Q2DUOJ1TE3GbZqlJjHlgySbGZKVy8tQCt0uJbB1NTnT9foJEwPmdeXphdsQHinyyo5WWzh7mDtSoecqgqRZ6usJf2F6SBnGfHuA71trlxpgsoMIY86K19sO97veGtfac4JcYGYo9mqUmMWBjOUyYDem5blcydMY42x8D59SMDnuLuEKNmsSQbU0dlK/bwTUnTtod0S4DC5zbOsCKGjjbH1/7uI72rl7SUyJzDFBFtTPoes7+VtSsD5pqYPSkMFe2p4P+jbTWbrXWLvd/3gJ8BMTduPbivAxqve1YBRlItGpvhM0V0Xk+LaB0gXMupmGj25WIxC8NvZYY8khFDT4Ll8zV7LSDCmwHPEgY2TR/oMiHERwoUlHtZXRmChNHZ+x7Y98sNfe3Pw7prQNjTCkwC3hvgJuPM8asNMY8a4w5cj+Pv9oYs8wYs6yurm7o1bqoyJNOa2cPjW3dbpciMjxVbzjvEEXj+bSAiZqnJuI6Db2WGOHzWR5cVsNxh4ymND/T7XIiXyBg4wBbHwGmFzmBIpG8/bGiuoHZEz2YgXbnRNAstUE3asaYUcCjwDettXu3yMuBidbaGcAfgCcGeg5r7d3W2rnW2rkFBdG1D7gvol/n1CRabSh33gUvOtrtSoYvfwpkjtE5NRG3aei1xIB3NtZT09DOZfO0mjYoDZVOdH1q1gHvNi47jfxRKayK0EZtZ2snVfVtA297BMgaB0npEZH8OKhGzRiTjNOk3W+tfWzv2621zdbaVv/ni4FkY0x+UCt1mSL6JeptLHe2DialuF3J8O19Tk1E3KGh1xIDHlhaQ056MmccOc7tUqKDt/Kgq2kQCBTJidgVteX+82kDBomA87uGpzQ6VtSMsyb4F+Aja+3N+7nPOP/9MMbM8z9vfTALdVtxXjqAIvolOnmrnXNd0Xw+LaB0AbRsiYi94yJxS0OvJcp5d3Xx/OptXDCrkLTkyAy8iDgNVQc9nxYwvTCHj7e30N7VG9KShqNik5fkRMM0/8y3AeWVRc2K2nzgP4BT+sXvn2WMucYYc43/PhcBq40xK4FbgctsjKVuZKUl48lI1tZHiU4by53LaD6fFtB3Tu1Nd+sQiWcaei1R7vH3N9PV6+PSo7XtcVB6uqC59qCJjwGRHCiyvNrLtMKcAzfogaHXLrczB43nt9a+CRwwB9taextwW7CKilTFeYrolyi1oRyyxkPBVLcrGbmCqc4e+aq3YPYVblcjEp/6D70OfC4SJay1PLi0hhnFuRw+PtvtcqJDU43z5swgtj7CnoEi+z0L5oLOnl5W1jZxxbETD3xHTxl074LWHZA1NjzFDUADI4ag2ONE9ItEFV8vVL7mrKbFwuwxnVMTcZ9mqUkUe7+mkXXbW7hMq2mDN8gZagGRGiiyZkszXT0+5pYepHkMNKQuH7NQozYERXnpbPa24/Ppl0OJIltXQrs3Ns6nBZQudLZgNFa7XYlIfFKjJlHswSU1ZKQk8ukZE9wuJXoEGpZBrqgZ45wBi7RAkUCQyOySgzRqgYbU5XNqatSGoNiTQVevj+0tHW6XIjJ4fefTTnK1jKCaON+5VEy/iDs09FqiVGtnD//+YAvnHDWeUakHPQEkAd4qJ7J+1OC3AU4vzOGTHa10dEdOoEhFtZfivHTGZKcd+I65xYBxPflRjdoQBGapbarXOTWJIhvKYex0GDXG7UqCp+AwyBitQBERt2jotUSpp1duoa2rl0uPLnG7lOjSUOkEbAzhCMW0whx6fTZiAkWstSyr9jLnYKtpAEmpzvlbbX2MHsUef0S/zqlJtOhqg5r3YNJJblcSXAkJMPF4qFajJuIaDb2WKPTA0hqmjBnF7JJct0uJLoOcodbf9MLdgSKRoNbbTl1L5+DDTSJglpoatSEo9KRjDEp+lOhR/Tb0dsVGLP/eShdC4ybnQ0TCT0OvJcqs3dbMippGLptXgomFcK1wsdbZ+jjIIJGA8TlpjM5MYVVtZDRqFf7zaXMm5g3uAREwS02N2hCkJiUyLjtNs9Qkemwsh8RUZ/Up1uicmoi7NPRaosyDS2tISUzgglmFbpcSXVp3QHfboIddBwQCRSIl+bGi2ktmSiJTx2UN7gGeUti1AzpbQ1rXgahRG6JiTwa1Ddr6KFFiQzmUHAvJ6W5XEnxjjoB0j86pibhFQ68linR09/L4+5s5/cix5GWmuF1OdBli4mN/kRQoUlHtZVaJh8SEQa6mRkDyoxq1ISrKS9eKmkSHlu2wY01sxfL3l5DgrKrpnJqIO7L9qxI6pyZR4IUPt9PY1s1lChEZuiHOUOsvECjykcuBIq2dPazd1szsoQzfjoBZamrUhqjYk8G25g46e9x/Z0DkgDa+6lzG4vm0gNIFzjtdTbVuVyISf3L8jZqSHyUKPLh0E0WedI6fNNrtUqKPtxIwkDv0JveoosgIFFmxqRGfZfBBIrC7MXUxUESN2hAV52VgLWxW8qNEuo3lToT9uKPcriR0dE5NxD19Q68VKCKRrbp+F2+tr+fSucUkDHbbm+zmrXKi6pOGvmW0L1DE5UatotqLMTBrKGmf6bnOzEhtfYweiuiXqGCtcz6t7ERni2CsGjvN+SZa9YbblYjEn76h11pRk8j20LIaEgxcNLfI7VKiU2CG2jAEAkU+cDn5sWKTl6ljs8hOSx7aA/PKtPUxmpSMdoZeK6JfIlrdWueAf6yeTwvom6emFTWRsNPQa4kCPb0+Hl5Wy0lTxzA+JwaDtcJhGDPU+nM7UMTns7xf7R3a+bQAT5m2PkaTsVlppCQmKFBEItuGcucyls+nBZQugIaN2n4l4gYNvZYI9+q6Ona0dHLp0cVulxKdOltgV92wgkQC3A4U+WRHKy2dPcwpGUajllcGTTWujSFRozZECQmGQk+6Ivolsm0sh9GTITcOfjDpnJqIezT0WiLcA0tryB+VyimHjXG7lOgUOJ81zK2PANNdDhRZVt0ADDFIJMBTCr4eaHYntEyN2jAUeRTRLxGsp9OZLRYPq2kA46ZDao7OqYm4QUOvJYJtb+6gfN0OLp5bRHKifuUdlkCjNoKtjxNy0shzMVCkotpL/qgUJvqPLw2Jy8mP+ls7DMV5GTqjJpGrZgl0t8X++bSAhESYeJzOqYm4QUOvJYI9UlFLr89yydw42F0SKiOYoRYQCBRZtdmdrY/Lq73MLvFgzDASP12epaZGbRiKPRl427pp6eh2uxSRfW0sB5PonN2KFxPnQ/16aNEviyJhpaHXEqF8PstDy2o49pA8yvIz3S4nenkrId3jRNWPwPTCbD7Z3hL2QJGdrZ1U1bcNb9sjQNYESEzVilo0Kc7zR/TrnJpEog3lUDQX0nLcriR8Ak1p1Zvu1iESbzT0WiLUu5X1VNe3cdnRQx/SLP2MIJq/v+mFOfT4LGu3tYy8piFYXu0Fhnk+DZx0ac9E12apqVEbhpI8f0S/zqlJpGlrgC3vx8/5tIBxR0FKlho1kXDT0GuJUA8sqSE7LYkzp41zu5To5q0a0bbHgGmFzpvH4T6nVlHtJSUxoe/1h8VTqq2P0aTYo1lqEqEqXwds/JxPC0hM0jk1CSljzJnGmHXGmPXGmBsGuP08Y8wHxpgVxphlxpgFg31sVNPQa4lA3l1dPLd6GxfMKiQtOdHtcqJXb48TTT+CIJGAwtx0PBnJrA7z4OuKai/TCrNH9vfAUwYNVWBt0OoarKSwv2IMyM1IZlRqErVebX2UCLOx3FlZKpzjdiXhN3E+fPICtGyHrLFuVyMxxBiTCNwOfAqoBZYaY56y1n7Y724vA09Za60x5ijgIeCwQT42emnoddzaUNfKzS9+zI7mDrdL2UdjWzddvT4u1bbHkWmqcaLpg7CitjtQJHyNWmdPLx9sbuILx00c2RPllUFXC7TVQ2Z+cIobJDVqw2CMcSL6taImkWZDOZQthMRktysJv9KFzmX1WzDtM+7WIrFmHrDeWrsRwBjzAHAe0NdsWWtb+90/E7CDfWzU09DruNLR3cvt5ev542sbSU1OYPpItpSFSEFWKgunFHDEhGy3S4luge1+QTijBs45tbtf30hHd29YVjrXbGmmq8c3/PNpAYFG1VulRi1aFOdlUF2/y+0yRHZr2AiN1XDcdW5X4o7xMyBllHNOTY2aBFchUNPv61rgmL3vZIy5APhfYAxw9lAe63/81cDVACUlUbQSkF3ovEkkMa983Q5+9uQaNjW0ccGsQn501uEUZKW6XZaEShBmqPV3VJETKLJuWwszinOD8pwHEggSmV0y0kat1LlsqHTC2sJIZ9SGqdiTQU1DO9aF/aoiAwr8ohRv59MCEpOg5FidU5NQGGj4zj7f/K21j1trDwPOB/57KI/1P/5ua+1ca+3cgoKC4dYaftkTNPQ6xm1taudr91XwxXuXkpRo+OdVx/C7S2eqSYt1DZVONH3WhKA8XSDQ44MwbX9cVuWlOC+dMdlpI3sij3/rpAuBIlpRG6bivHTau3vZ2dqlb1QSGTaWQ3YRjJ7sdiXumTgfXr4RWutgVBT9oiuRrhboPzG3CNhvzKG19nVjzCRjTP5QHxuVsgt3D73OKXK7Ggminl4ff327it+9+DE9Pst3Tz+Ur5xwCKlJCuiIC95Kp0lJCM66TjgDRay1VGzysmByELYqJqc7zaoLs9TUqA1T/4h+NWriOl+vk/h4+Kedw/3xqv85tSPPd7UUiSlLgSnGmDJgM3AZ8Ln+dzDGTAY2+MNEZgMpQD3QeLDHRr3A0OvmLWrUYkhFtZcfP7Gaj7Y2c/LUAm48dxolozPcLkvCqaEqKEEiAeEMFKn1tlPX0snskZ5PC8grc2WWmrY+DlNxniL6JYJseR86muJvftreJsx0osK1/VGCyFrbA1wHPA98BDxkrV1jjLnGGHON/24XAquNMStwUh4vtY4BHxv2P0QoBYZeN9W6W4cERWNbFz987AMuvPNtGtu6uOvy2dxz5dFq0uKNtf4ZaqVBfdrphTl8vL2Fju7eoD7v3ioCg65Hej4twKVZalpRG6YiTzqAIvolMgTOpx1ykqtluC4xGUqO0eBrCTpr7WJg8V7X3dXv818Dvx7sY2OKhl7HBGstjy7fzP8s/oim9m6+srCMb552KJmp+lUxLrXVO5H0QQoSCZheGJ5AkYpqL5kpiUwdlxWcJ/SUQctW6G53tkKGiVbUhikjJYn8USlaUZPIsLEcxh0V9tjYiDRxPuz4EHbVu12JSHzQ0Ouo9/H2Fi7947t89+GVlOVn8vTXF/BfZx+hJi2eBc5jBXHrI+wOFAn19sdl1V5mlXhITAjScZC8fhH9YaRGbQSKPBnUeNWoics6W6FmSfymPe6t/zk1EQk9Db2OWm1dPfzvsx9x1u/f4OMdLfz6wuk8/NXjOHy85o/FvcA2vyCvqBV50snNSGZ1CBu1lo5u1m1rHvn8tP487jRqeqtkBIrzMlhR43W7DIl31W+Br1vn0wImzIKkdOe/yxHnul2NSHzQ0Ouo8+KH2/n5U2vY3NjOxXOK+OFZh5OXmeJ2WRIpAg1JbnBnOhpjmB7iQJGVNU34LEFu1EqdyzAnP2pFbQSKPelsaeygp9fndikSzzaUQ1IalBzndiWRISlF59Qk7rV0dNPVE8afTdmFOqMWJWq9bVz1t2V85e/LyExN5OFrjuP/Lp6hJk321FDpRNKH4DzWNH+gSGdPaAJFKqq9GAMzS3KD96QZeZCaHfZAETVqI1CSl0Gvz7K1qcPtUiSebSyHicdD8ggHOsaSiQtg+xpoa3C7EpGws9bynYdWctFdb1O5c1d4XlRDryNed6+PO1/dwKdufp231u/kh4sO45lvLOTo0jy3S5NI5K0M+rbHgOmFOXT3OoEioVCxycvUsVlkpyUH70mNcVbVtKIWPYr7zVITcUXzFqhbq22PeytdAFioftvtSkTCzhjDZ2YXUl3fxtm3vsFDS2uw1ob2RfsPvZaI897Ges76/Rv8+rm1LJySz0vfOZGvnjiJ5ET9Gij70VAZ9CCRgOkhDBTp9Vner/YGb35af3llWlGLJsUep1GrbVBEv7hk46vOpYJE9lQ429kOqkARiVNnThvPc99cyIyiXL7/6Adc+8/lNLV1h+4F+w+9lohR39rJdx9eyaV3v0tbVy9/+cJc7r5iLoW54YsXlyjU1ea86RLkGWoBRZ50ctJDEyjyyY4WWjp7mBuKRs1TCo2bwBfaGXD9HbRRM8YUG2PKjTEfGWPWGGOuH+A+xhhzqzFmvTHmA2PM7NCUG1nG56aRYLSiJi7aUA6ZBTDmSLcriSxJqVB0NFS94XYlIq4Zn5POfVcdww/OPIwX1mxn0e9f592NIRpboaHXEcXns/xrySZO+X+v8cT7m/nPkybx0rdP5NTDx7pdmkSDxmrnMkRbH40xHFUUmkCRvkHXIWnUyqC3K6xvSA1mRa0H+I619nDgWOBaY8wRe91nETDF/3E1cGdQq4xQyYkJjM9J1yw1cYe1zoraISdBghbH91G6ELathnYls0r8SkwwfO2kSTz6teNJSUrgs396l/97fi3dwQ7B0tDriPHhlmYuuuttfvjYKg4bl8Wz1y/k+2ceRnpKotulSbQI0Qy1/qYV5rBuW/ADRSqqveSPSqHEfzwpqPpmqYVv++NBf7uz1m611i73f94CfAQU7nW384C/W8e7QK4xZnzQq41AxXnpbFKjJm7YvgZ27dD5tP0pnY9zTu0dtysRcd2M4lye+cZCLp5TxO3lG7jorneorg9i0IiGXruutbOH/376Qz5925tU17fx/y6ewQNXH8uUsVlulybRJkQz1PoLVaDI8movs0s8GBOkQdf9BRrXMAaKDOlteGNMKTALeG+vmwqBmn5f17JvM4cx5mpjzDJjzLK6urohlhqZij0Z1Hh1Rk1csLHcudT5tIEVzoXEVJ1TE/HLTE3iNxfN4PbPzaayrpWzfv8Gj1TUBidoREOvXWOtZfGqrZz2/17jnrcqufToYl7+zolcOKcoNL+sSuxrqHSi6NNDsH3QLxSBInUtnVTVt4Vm2yM4Z3ETksI69HrQA6+NMaOAR4FvWmub9755gIfs853fWns3cDfA3LlzQxxBFR4leRnUtXTS0d1LWrK2FUgYbSiH/Km7txzJnpLTdE5NZABnHzWemSW5fOvBFXz34ZW8um4Hv7pgOjnpI4yy1tDrsKuu38VPn1zDax/XccT4bO68fDazSkL3y7XECW+VE5wRwkY/FIEiyzc5Rx3mlobo30BikjMAPJK2PgIYY5JxmrT7rbWPDXCXWqC439dFQFxsVA9E9NcqUETCafNyJ3peq2kHVroAtq2C9ka3KxGJKIW56fzrK8fyvTOm8uzqbZz1+zdYWjXCuYMaeh02nT29/OHlTzj9d69TUe3lp+ccwVPXzVeTJsERwhlqAcYYphcGN1BkebWXlMQEjpyQE7Tn3IenLLK2Phpn3fwvwEfW2pv3c7engCv86Y/HAk3W2q1BrDNiFec5Ebc1iuiXcNj5CTz0BfjTyZCcDrMud7uiyFY635nttOldtysRiTiJCYZrT57MI9ccR2KC4dI/vsPNL6yjZ7hBIxp6HRZvr9/Jot+/wf978WNOO2IsL337RL60oIwkzUSTYPD1grc6pEEiAcEOFKmo9jKtMDu0O9zCPEttMP+q5wP/AZxijFnh/zjLGHONMeYa/30WAxuB9cCfgP8MTbmRJzBLTRH9ElJNm+Gpr8Ptx8D6l+DEG+D6lTBuutuVRbaioyExBarfdLsSkYg1q8TD4usXcsGsIm59ZT2X/PGd4aUZa+h1SO1o6eD6B97nc39+j16f5W9fmsftn5vNuJw0t0uTWNK8GXzdIZuh1l8gUOTjba0jfq7Onl4+2NwUuvNpAZ4y6GgKW6L0Qc+oWWvfZOAzaP3vY4Frg1VUNCnISiU1KUER/RIabQ3w5s3w3t2AhXlXw8LvwKgCtyuLDsnpTqhIlRo1kQMZlZrE/7tkBidOLeC/Hl/Fot+/wX+ffyQXzCoa/JP0H3qdM4THyQH19Pr415JN/Ob5dXR2+/jGqVP4z5Mm6Vy8hEYgKCPEWx9hz0CR6UUj2664enMzXT2+MDRqpc5lQyUUhn6r8aDDRGRgxhiKPIrolyDr2gXv3gFv3QqdLTDjs3DSDeCZ6HZl0ad0AbzxW+hohrRst6sRiWjnzpjAbH/QyLceXMlr6+r4xfnTyE4bRNBI/6HXxfNCW2gc2N7cwQNLanhg6Sa2NnWwYHI+vzjvSA4pGOV2aRLLwjBDLaA4zwkUCcY5teX+QdezQ92o9Z+lVjg7tK+FGrWgKM7L0Bk1CY6eLlj+N3jtN86MtKlnwyk/hrF7z5iXQSudD6//xjmndujpblcjEvGKPBn86yvHcserG/j9y5+wrNrL7y+byZyJeQd+oIZej5jPZ3l7Qz33vVvNix9tp9dnWTgln1+cN43TDh+juH0JPW8lJCSHZVXcGMO0wuygJD9WVHspyctgTFaItwL3X1ELAzVqQVCSl0FFdXj2qkqM8vlg9SPwyi+hsRomzofL7te70sFQNM/5oVP9pho1kUFKSkzgG6dOYf7kfL754Ptc8sd3+fopk7nu5Mn7D63Q0Oth8+7q4pGKWv65ZBOVO3fhyUjmywvK+Ny8EkrzM90uT+JJQ6UTQZ8Qnq210wpzuPfNKrp6fKQkDS8Qx1pLxSYvCybnB7m6AaRkwqixYZulpkYtCIo9GbR09NDU1k1Oxgjn0Eh8sRY+eQFe/gVsX+2Eg3z+UZh8akjnl8SVlAwonANVGnwtMlRzJnpY/I2F/PTJNdzy0ie8+clOfnfpzL7RNHvQ0Oshsdbyfk0j971bzdMfbO07X/ONUyezaNp4nUETdwRmqIXJ9MIcunp9fLy9hWmFwzunVuttp66lM/TbHgM8pWrUoklfRL+3jZyMEM5ukNhS/Q68fCNsesfZC37hX+DIz0CCIpaDrnQBvPk757xfapbb1YhElay0ZH536UxOPLSAHz+xmrN+/wa/vGAa580s3PfO2RM09PogdnX28MSKzdz/7iY+3NpMZkoil8wt4vPHTOTw8TpHKy7zVkLR3LC93FGFuYATKDLcRm1ZtTMDck645gh6ysIWUqZGLQiKAhH9DW3D/ksmcWTbanjlv+Hj55zl87NvhtlXQKJWY0OmdL4TKFLzHkw+ze1qRKLS+bMKmTPRw/UPvM/1D6zgtY/r+MV50xiV2u9XiZwi2FDuXpERbO22Zu5/dxOPv7+Z1s4eDhuXxS/Pn8b5swr3/G8o4pa2Bid6PgxBIgH9A0U+O8znqKj2Mio1ianjwvRGbF4ZfPAg9HRCUmpIX0rfGYIgsAVEs9TkgBoqofx/YNXDTvrgqT+DY65xtuZJaBUfAwlJzjtgatREhq04L4OHvnoct76yntte+YRlVU7QyKzAO9n9h14n6leMju5enlu9jfverWZZtZeUpATOmT6ezx9bwuwSj8JBJLIEBjmHIZo/IBAosqp2+IEiFdWNzCrJJTEhTP+ePGWAhcZNkD8lpC+l76JBkJOeTHZakiL6ZWAt2+H1/4OKvzrNwoJvwvzrIT1MS/TiHP6dMFvn1ESCICkxgW9/6lAWTsnnmw+s4KK73uFbp03haydNJrH/0Os4nqVWXb+Lf763iYcramnY1cXE0Rn86KzDuGhOMXmZKW6XJzKwwLmrMJ5Rg5EFirR0dLNuWzOnnxLahmkP/ZMf1ahFB0X0yz46mpw5aO/e4SyPz/kCnPB9yB7vdmXxqXQBvH2rM6MuRSlqIiN1dGkei69fyI+fWM1vX/iY1z/ZyR3z8smHuBx63dPr45W1O7jvvU28/nEdiQmG0w4fw+XHTmT+pHwSwvVuv8hw9c1QKw3ry44kUGRlTRM+S+gHXffXf5ZaiKlRC5KSvAzWbW9xuwyJBN3tsORP8ObN0O6FaRfCyf8Foye5XVl8KzvB+X+y9C8w/xtuVyMSE3LSk7n1spmcdGgBP31yNV99chuPQlwNvd57MPXY7FS+edoULju6hHE5IZ7pJBJM3krn7HyY38yc7m/OhhMosqy6AWNgVkluCCrbj8wCZxRJGGapqVELkuK8DF5euwOfz+pds3jV2wMr7odXb4KWLc5ZqFN/CuNnuF2ZABxyEhx2Drz0cyfRauLxblckEhOMMVw4p4i5pR5+9M83oB7+/cYyTplyLpkxGpKxv8HUP/v0kZx2+Jj9z5oTiWQNVWENEgkoycsgOy1pWIEiFdVepo7NIistjIFsxjiramGI6I/N76AuKPak09Xjo661k7HZegctrlgLHz7pDKuu/wSKjobP3A1lC92uTPozBs6/A+4+GR6+Er76OmSNc7sqkZgxcXQmf/3ap+j6n3R2bN7I2be+we8vm8WM4ly3SwuaxjZnMPX972kwtcQgb5VzTCDMnECRHFZvHlqgSK/PsmJTI+fOnBCiyg7AUwr160P+MmrUgqQob3dEvxq1OLKh3JmFtuV9KDgMLvsnTD1Lw6ojVVoOXHof/PlUePiL8IWnNBZBJIiSkxLBU8T5o+Av23xceOfbfPv0Q/nqCZPCl8gWZBpMLXGhp9MZVh/GxMf+phfmcO9bQwsU+WRHCy2dPeE9nxbgKYX1L4HPF9L5t2rUgqTYszuif25pnsvVSMhtroCXboTK1yCnGM6/E466FBL0AzvijT0CPv17eOwrzjbIM37ldkUisSV7AqO76nj2+hP40eOr+M1z63j94zp+d+lMxueku13doA00mPriOUVcfqwGU0sM8lYD1pWtj+AkPw41UKSi2guEOUgkIK8MejqchNvs0K3oqVELkiKP88NnU72SH2Na3cfOsOqPnoKM0XDmTTD3SyEfeChBdtQlULME3rnNOa925AVuVyQSO/xDr3Mykrntc7M4saKAnz+1hjNveYObPjOdRdMjO/l23bYW7nu3WoOpJb64MEOtv0CgyOohBIpUVHnJH5VKSZ4L82gDDW1DpRq1aJCWnMjY7FQNvY5VTbVOSMiK+yE5A076IRx3LaRmuV2ZDNcZ/wNbV8CT18GYI6BgqtsVicSGfkOvTWISl8wt5ujSPK5/4H2+dv9yLphVyOHjI+97Z4/PUr52B0urNJha4pBL0fwBE0dnkOUPFLlskI+p2ORlzsRcd/59Bv47eaugdH7IXkaNWhAVezKo0dDroetuh6e/7RzKTEhytg+aBP9l4r7XJST5rx/Kdf7L/p8PdF1C0l6vnQjVbztx+1g45hpY+B3IzHf7v5qMVFIKXPw3+OMJ8OB/wFdegdRRblclEv0GGHpdlp/JI9ccz+9e+pi7X9/I4+9bl4scmAZTD1FXGySn61x2LPBWOZHzmQWuvLwxhmkTBh8oUtfSSXV9G58/piTEle1Hbonze2KIZ6mpUQui4rwMllQ2uF1GdOntgUe+DOsWOymJ1oKvF2yX/7IXfD3OYU3bu//rfD3+zwe4bqRMAsz4LJx0g/MPU2JHTiFcdA/843x46uvO5/qFQ2Rksgudy72GXqckJfCDMw/j+lOn0OuLzEYtIyVRq2eD1bQZ7poPMz+vs77RzueDmndh9CGu/gw8qiiHe9+uorvXR/JBRlws3+Ti+TRwgshyikI+S02NWhAVe9J5ckX7oP6CCU5Ttvi7sO4ZWPR/cMzVoXmdvuatp1+j17vn5/u9rgcy8iG3ODS1ifsOORFO+YmT3lk8D479mtsViUS3HH+jtp+h10pJjBHP3QDtXnj3Dph+MUyY6XZFMlwr/+mkV597m6tlTCvMoavHCRQ5csKBz6ktr/aSkphw0PuF1Bn/A6PGhPQl1KgFUVFeBj4LWxrbmTha81QO6vXfQsW9sOBboWvSwB+bmqAYdtm/Bd+C2mXwwo9hwiwoOdbtikSiV+BgffMWd+uQ0PnkRSdU6/hvwMoH4OlvwVUvKfk4GrU1wAs/geJjndVRFwUCRVbVNh20AVtW7WV6UY67b/wcfk7IX0LLPkHUF9HfoOTHg1r+Dyj/pbOl8NSfuV2NxDtj4II7na2tD30BWra7XZFI9ErLdUKXmje7XYmEQne7sxtm9BQ45cfOtscty6Hir25XJsPx0s+hownO/n8hnQc2GP0DRQ6ks6eXVbVN7m17DCM1akFUnOeP6FegyIF9/Dz8+3qYdAqc+wedCZLIkJYDl/zD+YH1yJec85MiMnTGOOfU1KjFpjd/5wRPnP1bZzTN9IuhdKGzfby1zu3qZChqlsDyvzlb/sdNc7uaQQeKrN7cTFevj9klatRkCMbnpJOUYBTRfyC1FfDwlTBuOlzyd21HlMgybhp8+haofhNe/rnb1YhEr+wJ2voYi+o3OI3atIvgkJOc64xxVmO62uDFn7pangxBb4+TuJ01wQlLixDTi3L4aFsL3b2+/d5nuX/Q9eyJuWGqyj1q1IIoMcFQ6ElXRP/+7FwP/7zYOXj5+Yc1g0wi04zLYO6X4e0/wIdPul2NSHTKKXJSASV2WAvPfAeS0vZNeSyYCsd/3QmlqHrLnfpkaJb+CbavgkU3RdTvY/0DRfanotpLSV4GY7LSwliZO9SoBVmxJ4Mar86o7aN1B9z3Gefzyx8LeUqOyIic+b9QOAeeuBZ2fuJ2NSLRp9/Qa4kRax6HjeXOubSscfvefsL3IKcEnvk29HaHvz4ZvOat8MqvYPJpcPi5blezh0CgyP62P1prWVbtZW4cnE8DNWpBV5yXTq1W1PbU2QL3XwS76uBzD8PoSW5XJHJgSanO1tykFGcYdtcutysSlxljzjTGrDPGrDfG7LNPyBjzeWPMB/6Pt40xM/rdVmWMWWWMWWGMWRbeyl3Sf+i1RL+OZnjuhzDuKDj6qoHvk5IBZ/0G6tbCO7eHtz4Zmud/BL1dsOg3EZcTMDEvg6zU/QeK1DS0s7O1k9lq1GQ4ijwZ1O/qYlen3kUEoKcLHroCtq2Gi/8GRXPcrkhkcHKK4MK/wM518NQ3nG0/EpeMMYnA7cAi4Ajgs8aYI/a6WyVworX2KOC/gbv3uv1ka+1Ma+3ckBccCfoPvZbo9+r/Qut2OOeWA0fwT10EU8+C134NjTVhK0+GYMMrsOYxWPidiHzjPCHBcGRhNqs2Nw94e8WmBsDFQddhpkYtyIrznIj+Wm1/dH6xferrzjeFc2+FQ093uyKRoZl0Mpz8X7D6EViy9+/dEkfmAeuttRuttV3AA8B5/e9grX3bWuv1f/kuUBTmGiNL/6HXEt22fgDv3QVzvzi4N1sX/dr5+f9c5ARUiF93h3POMO8QmH+929Xs1/TCHD7a2jxgoEhFtZdRqUkcOjZyztWFkhq1ICv2KKK/z8s3wgcPwMk/hlmXu12NyPAs+DYcusjZKrLpPberEXcUAv2XB2r91+3Pl4Fn+31tgReMMRXGmKv39yBjzNXGmGXGmGV1dVEec66h17HB53N+sU/Pg1MHmeiYWwInfh/WPu2M45HI8dbvoWEjnPVbSI7cII5AoMgn21v3ua2iupFZJbkkJkTWls1QUaMWZCV5gaHXcd6ovXe3E+E790twwnfdrkZk+BIS4IK7nK2QD3/BCcaReDPQbwQD7oU1xpyM06j9oN/V8621s3G2Tl5rjDlhoMdaa++21s611s4tKCgYac3u0tDr2PD+P6B2CZz+35A+hK1mx10H+VOdwdhdcf77UKRo2Ahv/D848gKYfKrb1RzQ/gJFWjq6WbetOW62PYIataDLy0whN6WXT7Y14vPF6ZmWNU/As9+HqWc779pE2EFVkSFLz3WGYbd7NQw7PtUCxf2+LgL2WSoyxhwF/Bk4z1pbH7jeWrvFf7kDeBxnK2Vs09Dr6LerHl76GZQcDzM+O7THJqU4s9UaNznNgbjLWlj8PUhMgTP+1+1qDqp0dOaAgSIrahrx2fg5nwZq1ILOtHtZnPwDvvzB5/j8//yFHzzyAc+t3kZrvISLVL0Fj10NxfPgor8c+NCxSDQZfxSc8zuoegNe+e+gPrXPZ7EKK4lkS4EpxpgyY0wKcBnwVP87GGNKgMeA/7DWftzv+kxjTFbgc+B0YHXYKneThl5Ht5d+6qQ2n3Pz8N5wLVsIR13qbLfTmBN3ffQUrH8JTv4RZI93u5qD2h0osmejVlHtxRiYWZzrTmEuSHK7gJji64XHvsJ4W0dnejZ/6/ohN62+gmuWnUxKYiLHHJLHKYeN4ZTDxjBxdKbb1Qbf9g/hX58Fz0T47AOQnO52RSLBNfNzULME3roFio6Gw88Z9lN19vTy5ic7WbxqGy99tJ0EA8dNGs1xk/I5ftJoDsnPxGg1OiJYa3uMMdcBzwOJwD3W2jXGmGv8t98F/BQYDdzh///W4094HAs87r8uCfintfY5F/4Y4ZdTBBvK3a5ChmPTu/D+fU7gxJjDh/88p/8S1j3nzFa74intsHFDZws8ewOMnQ7z9ntENuJML8zhb+9U093rIznRWVeqqPYydWwWWWnJLlcXPmrUgunVm2D9S5hzfkfaYZ+Gx7/KTzf8mWunbuavo7/Ns+vbufHfH3Ljvz9kUkGmv2kby9xST99fwqjVVOvMSktOh8sfhYw8tysSCY1Fv4atK+GJrzm/wAwh3riju5dX19Xx7OqtvPzRDlo7e8hKS+JTh48lIcHw9nqncQMYm53K8f6m7fjJ+RTm6o0PN1lrFwOL97rurn6fXwXsM2DKWrsRmLH39XGh/9DrRP26ETV6u+Hpb0F2EZzw/ZE916gxcOpPnLNqqx+F6RcFp0YZvFdvgpYtcMnfourfYf9AkSMmZNPrs6zY1Mi5Mye4XVpYRc//sUi37ll4/Tcw83KY80XnXaPPPwLv/IHRL/+C7zSt4TuX3Et1+lxeWbuDV9bu4G9vV/OnNyrJSkvihEMLOPWwMZw0dQx5mSlu/2mGpt0L913kvGvzxcVO4pNIrAoMw/7jCfDg5XDVS5Cy/xXyXZ09lK/bwbOrtlG+bgdtXb3kZiRz9vTxnDl9HPMn5ZOS5LxRY62lur6NtzfU8/aGnbz+cR2Pv++c8Zk4OoPj/Stuxx0ymoKs1LD8cUWGrf/Q65z4nlYQVd77I+z4EC69D1JHjfz55n4JVtzvJOdO+RSk5Yz8OWVwtq+Bd++E2Vc4R1KiSP9AkSMmZPPx9hZaOnuYWxo/59NAjVpw1G9wzmWNnwln9wvPSEhwtg2UHA+PfgnuOYOJp/6ULx73db44v4xdnT28uX4nr3y0g1fW7eCZD7ZiDMwqzu1bbTt8fFZkb3/q7oAHPg/1652VtHHT3a5IJPRyi+HCP8N9F8K/vwmfuXuPLT3NHd288tEOFq/aymsf19HZ4yN/VAoXzCpk0bTxHHNI3oCr6MYYSvMzKc3P5HPHlGCt5ePtrby9YSdvra/n6ZVb+dcSJyX+0LGj+lbcjjlkNDnp8bMVRKJE/6HXatSiQ9NmZ7j1lDPgsOFv7d5DQiKcfTP86RR45Vdw1m+C87xyYD4fPP1tpzE+7Ua3qxmy0tGZjPIHilxydDEV1c6Yyjkl8bVj66CNmjHmHuAcYIe1dtoAt58EPAlU+q96zFr7iyDWGNm6djnvqickwaX/GPhcVvHR8NU3nOHPL/4UKt+AC+4iMzOfM44cxxlHjsPns6zZ0szLa7dTvnYHv33hY377wseMz0nj5MPGcMrUMcyfnE96SgSFc/jP5FH9Flz4FzjkRLcrEgmfyac6w7DLfwnF82ic9gVe+HA7z63expuf7KSr18fY7FQ+O6+ERdPGMbc0b8hzX4wxTB2XxdRxWXxxfhk9vT7WbGnuW3F7YOkm/vp2FQnG2SZy3KTRHD8pn6NLPWSk6H04cVn/oddR9m5+3Hr+h+DrcbZ4B/NN4sLZcPSXYemfnLO+E2YG77llYCv/CTXvwrm3ReVxlIQEw5ETdgeKLK/2kj8qleK8+DoGMJif5H8FbgP+foD7vGGtDdJbL1HEWqf5qlvrrCYdaMtfeq6zXWrZX+C5H8Gd8+HCP0GZM04nIcEwvSiH6UU5fPO0Q9nR0sGr6+p45aMdPPn+Zv753iZSkxI4btJoTj1sDCcfNoYiT0Z4/pwDsRaeu8FJEjrjf7TvXOLSztnX0bn6dcYuvoGrnmxjWe9kCnPTueK4iSyaPp5ZxbkkBHEoZ1JiAjOKc5lRnMvXTppEZ08vK2uaeHvDTt5eX889b1byx9c2kpxomFmc2xdMMqskl9SkCHqTR+KDhl5Hl09egg+fhFN+DHllwX/+U37iPP8z34Yvv6hU6FBqa4AXfgLFx8DMz7tdzbBNL8zhH+9W09Pro2KTlzkTcyN7l1kIHLRRs9a+bowpDUMt0efdO53Dsaf+FCadcvD7GwNHX+X8w3n4i/C3c+HE7zuHdfc64DkmK41L5hZzydxiOnt6WVrp5eW123ll7Q5+8uQaeHINU8dmccrhTorkrOJcksIZSPLWLbDkbmeo5XHXhu91RVy2vbmD59dsY/GqrSypbGCUvZzn0tdxT8YfqLnkeY6YfEjYfpCkJiUyryyPeWV5fPM0aOvqYVmVl7c31PPOhp3c9son3PryJ6QlJ3B0aV7fitu0Cdnh/X4h8UlDr6NHdzss/g6MngzHfyM0r5GeC6f/Ch6/Gpb/zTm7JqHx0s+ho8nZcpoQvd/rpxfl0Nnj452N9VTXt/H5Y+IvAyFYe2OOM8asxBkA+l1r7ZogPW/kqnoTXvixs4d7wbeH9thx0+GrrznDB1/7tfNcn/nT7m0ie0lNSmTBlHwWTMnnp+ccwcaduyhfu4OXP9rBn17fyJ2vbiA3I5kTDy3glMPGcOKhBeRmhDCQZOUDzjeBaRfBp4I7T0okEm1ubOe51dt4dtVWKjZ5sRYmjxnFdSdP5sxp4xnPVMw9Z3Dk29+CSY+Dceed4owUJ5johEMLAGhq72ZJZUPfittvnlsHrCMrNYljDsnrW3GbOjYrqCt/IoCGXkeTN28BbxVc8aQTmBQqR10C7/8DXroRDvs0jCoI3WvFq5qlTiN83HUwbp8TS1Flmj9Q5G9vVwMwZ2L0beEcKTOYIav+FbWn93NGLRvwWWtbjTFnAb+31k7Zz/NcDVwNUFJSMqe6unoktbuneYuT+JaWC195BdKyh/9cKx90YnCTUuH8O2HqmUMrpaObNz7eyctrt/PqujoadnWRYGDuxDxOPmwMpx4+hiljRgXvHf71L8E/L4WJxzuplqH8hi7iok31bTy7eiuLV29jZU0jAIePz2bRtHEsmjaOKWOz9nzA+/fBk9c6b9yc9rPwFzwIdS2dvLuxvm/Fraq+DYDRmSkcO2m0MwpgUj6lozOCvipojKnwzxWTQZg7d65dtmyZ22WM3N/Ohe42Jx1VIlP9BrjjWDj8XLjoL6F/vbp1cOfxMP0SuODO0L9ePOntgbtPgrZ6uG4JpGYd9CGRzOezHHXjC+zq6iE5IYFVN54ek9v4D/TzccQratba5n6fLzbG3GGMybfW7hzgvncDd4PzQ2ikr+2Kni546AroaoMvPD2yJg1gxqVQOAceuRL+dSkcey2c9nNIGtyKWHZaMmcfNZ6zjxpPr8+ysraxb7Xt18+t5dfPraXIk943aPvYQ0aTljzMv+Rb3ocHr4CCw+HS+9WkSczZUNfKc6udbY1rtjjf2o4qyuH7Z05l0bTxlOUfYFD9rMudYdhv3gxFc+Gws8NU9eAVZKXy6RkT+PQM5+zQ5sZ23tlQz9vrd/L2hnqe+WArAONz0vq2SR4/aTQTNMNNhktDryObtc6Ms6Q0OONX4XnNgqlw/Nfhzd853zdL54fndePB0j/B9lVw8d+ivkkDJ7/hiAnZLKlsYHpRTkw2aQcz4kbNGDMO2G6ttcaYeUACUD/iyiLV8z+E2qVw8V9hzGHBec78yfDll+DFn8C7t8Omt+GieyDvkCE9TWKCYXaJh9klHr5z+lS2NrVTvraOV9Zu56FlNfz9nWoyUhL51BFjOW/mBBZOKRj8oO2GSrj/YsgYDZ9/eOQNqkgECMTfL161ledWb2Pd9hYAZpfk8uOzD+eMI8dRnDeE0J5Fv3GGYT9+DVz96pCGYbuhMDedi+YUcdGcIqy1VO7c5V9tq+fVdXU8ttzZsvbyd05kUkEQ5ilJ/NHQ68i25nHY8IrzvStrXPhe94Tvw6pH4ZnvwDVvQKLGi4xY81Zn/MHk0+CI89yuJmimF+awpLKBORPja35awGDi+f8FnATkG2NqgZ8ByQDW2ruAi4CvGWN6gHbgMjuY/ZTRaMU/YemfnYO2R14Q3OdOToOz/s9JgXzyWrjrBDj39zDtwmE/5ficdD53TAmfO6aEju5e3tlYzwtrtrF41TaeXLGF3IxkFk0bz3kzJzCvNG//Z1R27YT7PuNE9l7+KGSPH3ZNIm6z1hmF8ezqrTy7ahsbd+5ycn5K8/j5p4/gjGnjGJ8zzBWk5DRnTMcfT3BW3r/8IqS4mM46BMYYDikYxSEFo7j82In4fJZ121tYWtXAIQdaSRQ5EA29jlwdzfDcD2HcUTD3y+F97ZQMZwTAA5+Fd+9wZs7KyDz/I+jtcpruGEpGPKrIOac2uyQ+G7VBnVELhajbf791JfzldCg6Gv7jidC+M9i4CR69Cmreg9lfgDNvCuove109Pl7/uI6nVm7hxQ+3097dy7jsND49YzznzihkWmH27vMpXbvgr+fAjg/hiqeg5Jig1SESTtubO7jnrUoWr9pKTUM7iQmG4w4ZzZnTxnH6kWMZk5UWvBf75CW4/yI46lK44K6Y+qE5XDqjNjRR9zNyfz5+Af55sfOmhWapRZbnfuQ0SVe95GzXdsO/PgsbX4XrlqqRH4kNr8A/LoCTfggn3eB2NUHV0d3L39+p4srjy0hJit4EywMJ6Rm1uNDW4Ay1zhgNF90b+u0buSVw5TNQ/j/OHu6aJXDxvTDm8KA8fUpSAqcdMZbTjhhLW1cPL364nX+v3MJf367iT29UUpafyadnTODcaQVMfuVq2LrCOZOmJk2iUHNHN398bQN/ebOSnl7Lgin5XHfyZD51xDjyMkOUjjrlNOeH5av/6wy8P/qq0LyOSKQLzFLT0OvIsm0VvHcXzLnSvSYNnDeibz8Gnv0BXHa/e3VEs+4OeOa7znGZ+d90u5qgS0tO5OoTIvsYQSipUTsYXy88+mVo2QZffC58UbKJyU5yXNlCeOxquPtkWHSTs8IWxHfnM1KSOG9mIefNLKSxrYtnV2/jqRVb+MMrH1P42veYnPQqb0z9EZPGncyEoL2qSOh19fi4/71q/vDKehp2dXHujAl89/SplIwO01bEE74Ptcvg2Rtg/CwomhOe1xWJJIGxMxp6HTl8Pnj625DucebAuskzEU78Hrz8C/j4eTj0DHfriUZv3woNG+Dyx5zt9xJTYnMNMZjK/8dZUj7r/9z5RWvSKXDNW85q1r+vh0e+5OwrD4HcjBQ+O6+Ef119LKtOqODSpFd5MP0y/mPlNI6/6RUuuesd7nu3moZdXSF5fZFgsNby75VbOO3m17jx3x9y2Lgs/n3dAm797KzwNWngDBn9zN3Omc6HroBdsZuxJLJffUOv1ahFjPf/AbVL4PT/howImEt13Nch/1BntmxXm9vVRJeGjfD6b53chMmnul2NhIAatQNZ+wy88VuY9R/O9gC3ZI2Fyx+HU34CHz4Jf1wIm5eH7vWW3cOo926GWf/Bpd+/i1e/exLf/tShNLR18eMnVjPvVy9x5b1LePz9Wlo7e0JXh8gQvbOhnvNvf4uv/+t9MlIS+esXj+b+q45huv8wcthl5MEl/4BddfDol5wVepF40jf0utbtSgScN4xe+hmUHA8zPut2NY6kFDj7/0FjtTPeRAbHWqe5TUyGM/7H7WokRNSo7c/O9U7E9oRZcNZv3a7GeXf+hO/CFxc7Mcd/OR3eud35hxpMHz3txOVOOQPOuQWMoTQ/k2+cOoUXv3UCi7+xkC8vLOOT7a1868GVzP3li1x7/3KeX7ONzh79EiruWLethS/eu4TP/ulddrR08tuLZ/DMNxZy0tQxQR/cPGQTZsLZv3UOzJfrh6nEoewJWlGLFC/9FDpbnMbI7e+N/ZWd4IQvvXkL7PzE7Wqiw0f/hvUvwcn/tfssqMQcnVEbSGcrPPh5SEiCS/4eWXt+S451Zo489XUnirXydTjvDsgcPfLn3vSecx5vwiwnvGSv0BRjnMGDR0zI5gdnHMbyTV6eXLGFxau28syqrWSlJXHmkeM4b2Yhx00aTeL+4v5FgmRrUzs3v/Axjy6vJTM1iRsWHcaVx5cOf6h7qMy+wgkFeuO3zsH9qYvcrkgkfDT0OjJsehfev88ZMTT2CLer2den/hvWPee8WXzFk5HVSEaazlZ47gYYOw3mXe12NRJCatT2Zi08dR3s/Ng5mJlb4nZF+8rIg0vvgyV/ghf+C+5aABf+GUrnD/856z6Gf13qbFH53EOQcuC5SQkJhrmlecwtzeNnnz6CtzbU8+SKzTy7ehsPV9SSPyqVc44az7kzJzCrONf9VQ2JKc0d3dz1qpPkaC18aX4Z1548GU+oUhyD4azfwrYP4LGvwldfHfJAe5GopaHX7uvtcQJEsovgxB+4Xc3AssbCqT+Bxd+F1Y/C9IvcrihyvXYTNG8OTxK5uEr/d/f2zu2w5nE47ecw6WS3q9k/Y+CYq52QkYe/CH87B068wdkemTDE1YTmrc5A64RkZ6B1Zv6QHp6UmMCJhxZw4qEFdHT3Ur52B0+u2MI/l2zir29XUZyXzqePmsB5MwuZOi5raLWJ9NPZ08v9727iD698gretm/NnTuA7p0+lOC8Khkonpzkr9H88ER68Ar78QtQMwxYZEQ29dt97d8GONc6bvKmj3K5m/+Z+yVn1e/5HMOVTkObS+eJItn0NvHOHs1NDY5Ninhq1/irfgBd/Cod/OnpmUYyfAV99zdkq8Or/QNUb8Jk/OUlzg9HR5Azmbfc6s9vyykZUTlpyIoumj2fR9PE0d3TzwprtPLliM398fSN3vLqBqWOzOHfmBM6dMSE6frmWiODzWZ5etZX/e34tNQ3tLJiczw2LDmNaYZT9EPeUOv8+/3mJ82/2/Du0vUdiX3a/iH41auHXtNmZ6TjldDjsHLerObCERDjnd/CnU5wzvYt+7XZFkSUwWiEtB0670e1qJAzUqAU0bYaHr4TRk5wzX9H0y1NqFlzwRyg70dkycNcC5+sppx34cT2d8MDnoW4tfP5hJ/QgiLLTkrloThEXzSliZ2sni1dt5ckVW/i/59fxf8+vY1ZJLufNmMDZR02gICs1qK8tsePt9Tv532fXsmpzE4ePz+bvX5rOCYeGaZ5hKBx6Opz4fXjt184A4LlfdLsikdDS0Gt3Pf9D8PXAot9Ex+82hbPh6C/DkrudZMog/24S1Vb+C2rehXNvi4zRChJyatTAaVgeugJ6OpxtAWnZblc0dMbArM87QQUPfxHuv9A5MHzqT53o1r35fPDE15wVuAv+6MxrC6H8UalccVwpVxxXSk1DG09/sJUnV2zm5//+kF88/SHHT8rnkIJMEowhwRgSE5xzcAnGkGiM/3P6Pk/0f+3c17lfQoJz38QEJ/gk0X+bMZCY0P95/M/f91r9L+l7Huc5YVRqEoW56SQlKiQ1nNZua+amZ9fy6ro6CnPTufmSGZw/s5CEWAipOfEH/mHY34fxR0GhhmFLDNPQa/d88pIz1ufkH494x0xYBcYRPfNt+PJLTvJ1vGtrgBd/AsXHwMzPu12NhIkaNXCSczYvc86PFEx1u5qRKZgKX3kZnv8vZ1p99dtw0V+cLVf9vfgT57DuaT+HGZeFtcTivAy+dtIkvnbSJD7Z3sJTK53kyDVbmuj1WayFXmvp9Vl81uKz0OsL8hiCIUpJTKA0P4NJBaOYPGYUkwqcj0MKMslM1T+jYNrS2M7NLzpJjlmpSfzorMO44rgITHIciYREJwDojyfCQ1+Aq18LTnKrSCTS0Gt3dHc4u2xGT4b533C7mqFJz4XTfwWPXw3L/6adBwAv3wjtjXD2zWpc44h+w3z/Plh2D8y/Ho44z+1qgiM5Hc652ZlL8tQ34K4T4Nxb4cjzndvfvg3euQ3mfdX1s3hTxmbxndOn8p3TD94g+/yNW6+1+HxOM+ezFp/Paep6rb/J8+3b5Pn6N34He6z/+sBjm9u72bCzlQ07drF2WwvPr9lG/75xQk4akwLN25hRTCrIZHLBKAqyUpV2OQRN7d3c+eoG7n3LSXL8ysJD+M+TJpGbEcFJjiORkQeX/A3uOQMeuwo+/8jQg4BEooGGXrvjzd+BtxL+4wlIisLjBUddAsv/Di/93DlbNyqKt7yPVM1SqPgrHHcdjJvmdjUSRvHdqG153zmUWXYCnPJTt6sJviPPd/Z2P/JlePgLUPklKJzrRPofcR6c+b/RsV/dLyHBkIBx/S9tZ08vm+rbWL+jlQ11rWyo28WGulYeXlbDrq7dQ7+z0pL6Vt4mjXGat0ljRlGSl0GytlH26ezp5R/vVHNb+Xqa2ru5YGYh3z79UIo8cRA2Uzgbzvo/+Pf1cO8iZ0U8a4Jzpie70AkFyp7grEhE0b9VkX1o6HV41W9wGrVpF0Z2gvWBGOMM5r5rPrz0Myd8KR719sAz33J+Npx0wx43dXd3U1tbS0dHh0vFyVCkpaVRVFREcvIAR5L2w+3fed3T1uBEZGcWxPYcCk8pfOk5ePkXzlbIZffAxPlwwd16936YUpMSmTI2iylj9xw1YK1lW3MHG3bsYv2Olr4G7s31dTy6fPc7ycmJhomjM5lUkLnHVspDCjLJShv8P95o5/NZnlq5hd++sI5abzsLpzhJjkdOiLIkx5Ga/QVorYO1T8PHz0PrDmCvrb5J6f7mrd9H1l5fZxbo37RELg29Dh9rnS2PiSlwxv+4Xc3IjDkMjv+603TOuhwmHu92ReG39E+wbRVc/DcnPK6f2tpasrKyKC0t1Q6eCGetpb6+ntraWsrKBn9eNEa7k4Pw9cIjX3JmunzpuSHPDYs6iclw+n87qZAfPuF8npzmdlUxxxjD+Jx0xueks2DKnn+nmju62Vi3iw07Wllf1+pc7mjl5Y920NNvH+W47DQmjcnc5yzc2OzY2kb55ic7+d9nP2LNlmaOGJ/NP748nYVT4nRbizFw4vecD4CeLud7U/NWZ6Bpy1ZnJaJ5s3Nd9TvOdb7uPZ8nIQlGjdu3oevf1GWNh6QY3UoqkU1Dr8Pnwydgwytw5q8ha5zb1YzcCd+DVY86O6CueWPggLRY1bwVXvkVTDp1wOM5HR0datKihDGG0aNHU1dXN6THxed3y1d+CRvL4dO3xlfa2pTTDh7ZLyGRnZbMzOJcZhbn7nF9d6+P6vo2/xbKVv92yl08vnwzLZ09ffcblZrUtwLXdx6uIJPxuelkpiRGzTfpD7c0c9Nza3n9YyfJ8ZZLZ3LujAmxkeQYLEkpkFvifOyPzwdtO3c3b82bnWauxf/59jXwyQvQ3bbvYzMLBl6R639dJA/EleiUPUFDr8Ohoxme+yGMOwqOvsrtaoIjJdOZp/bAZ+HdO5xMgXjxwn9Bb5ezRX4/P+ej5ee/DO//Vfw1ah89DW/e7Gw3mvMFt6uROJecmMDkMc7qWX/WWupaOvc4B7d+RyvvbKznsfc373HftOQECrJSKRiVSv6oVOdz/0ff1/5Lt5ITNze28/9eWMfj728mOy2ZH599OJcfOzG2khzDKSEBRo1xPibMGvg+1joD7Vv6NXL9V+maapx5PO3efR+bmrP7fFzWBDjtZ85riQxXtr8509Dr0Hr1JmjZ5owaiqWVy8POgkMXOX++/9/encdHVZ97HP/8SIAkLGEJIhIVWkSQJUEiq2iVCxJFXEBwwRa8SAW1XL2o4IbU2lovxeWyeNGLlQgVaiv0VlRQQG2RJQGsLC0IwbIJITSQgAlZnvvHDDFIQjJhkjOTfN+v17wyc+acM8/8sjx55vyWzkNrx8/QzhW+2bl/NNm3xq/USjXot7gCDu+Ad++DC/wD+EVClHOO8xpHcV7jKPq0O70bZU5eAekZx9l1OIeDx3LJyM4jIzuPwzkn+TrzBKlf/4sjx0+Wet5G9SN9BVyj0wu4Fg3rE9eoHi0aRtGiUX2aN6wXlAlPjp7IZ9aqr3hj9W4Axl71A8Zf3Y7YmFrUdcUrzvmmuI5uAud1LHu//G9LXI3bf+ZVukPbYMDPqytqqalOLXp9bN/Z95PK++ZLWPuq70Po+CSvowm+5F/DzJ6+JZVGvOV1NFWrIA/emwjNfuD57NzirdpTqOVlw9t3+boVjUgJz6lqRfB1g+wSH0uX+LIn3cgvLOLI8ZPFRVxGTt5p9w9n57HtwDE+zc4jO7eg1HM0jan73dW5712tK3m/aUw9Ir7XdTE3/7uZHI/l5nNrt3geHtie1k2ig9oWEgR1o32f1uoTW6lKpxa9PqpCrUoUFfnGcEU3gf5TvI6majS92DeW9+Ofw/Zl0H6g1xFVnb++DEd2wsg/hvScAllZWSxYsIDx48cHdNz111/PggULaNKkSdUEVoPUjkLNDJbcD5k7fOuJ1IZL5lKr1Y2oQ8vGUbRsXP4f+Nz8Qg6XKOQO5/gLvJxcDmefJCMnjw3/zCIjO49v8wvPOD6ijqNZg3rFV+fiGtZnza5M9mV9y9XtWzApuQMdWzWuircpIuFCi15XrU1vwd51cNMs3xqNNVXvB+GLt32zWrZd6/ugqaY5sgs+nQaX3Qzt+lf4sKn/t4Wt+48FNZTLLmjMlBs7lfl8VlYWs2bNOqNQKywsJCKi7KENS5cuDVqMVaG8+KtT7SjUVv83bF3i677zg6u9jkYkpETVjSC+aUyF1i07nldw2lW5jNMKPN/XHQezOT82iheGdaVvuxo+o6qIVIwWva46xzNh+dNwUR9IvNPraKpWZD3f2mpv3gif/QaufdLriILLDJY+6pvZctCvvI6mXJMmTWLnzp0kJiZSt25dGjZsSKtWrdi0aRNbt27l5ptvZs+ePeTm5jJhwgTGjh0LQJs2bUhNTSUnJ4fk5GSuvPJKVq9eTevWrVmyZAnR0aUX4K+99hpz5szh5MmTtGvXjpSUFGJiYjh48CD33Xcfu3btAmD27Nn06dOHefPmMW3aNJxzdO3alZSUFEaNGsXgwYMZNmwYAA0bNiQnJ4dVq1YxderUCsX/wQcf8Pjjj1NYWEhcXBzLly/n0ksvZfXq1bRo0YKioiLat2/PmjVriIs7x/+DzMyTW/fu3a1a7Fxl9kwTs4V3mxUVVc9riojIaYBU8yjfhOOt2nJkdfrtjWav9fc6ippn8f1mU5uZfbPF60iqzztjzH4eZ5ax3etIgmvLErMpjc1Wz6jQ7lu3bq3igM4uPT3dOnXqZGZmK1eutJiYGNu1a1fx85mZmWZmduLECevUqZMdPnzYzMwuvvhiy8jIsPT0dIuIiLCNGzeamdltt91mKSkpZb7eqePNzJ544gl75ZVXzMxs+PDh9uKLL5qZWUFBgWVlZdnmzZutffv2lpGRcVosP/nJT+z3v/998XkaNGgQUPyHDh2y+Pj44v1O7fPMM88Ux/Dhhx/arbfeWup7KO17drb8eO6zBYSyo3vhndHQ/BK4aWaZU5uKiIhIFYuN1xi1YPvnWtiYAr3GQ8vLvI6m+gz8BURG+7pAmpW/fzjIy/FNlNKyM/T4qdfRVEqPHj1OW8z5lVdeISEhgV69erFnzx527NhxxjFt27YlMTERgO7du7N79+4yz79582b69etHly5dmD9/Plu2bAFgxYoVjBs3DoCIiAhiY2NZsWIFw4YNK76i1axZ+V2CKxL/mjVruOqqq4r3O3Xee+65h3nz5gEwd+5cRo8eXe7rVUTNLdQK8mDRj32Lx45464zV3EVERKQalVz0Ws5dYQG897CvS+nVj3kdTfVq1BL6PwW7VvmmsK8JPnneNyvqDdPDdmmFBg0aFN9ftWoVH330EZ9//jlffPEF3bp1Izc394xj6tf/bnK/iIgICgrK/vswatQoZsyYwZdffsmUKVNKPd8pZlbqumWRkZEUFRUV73Py5HezZFck/rLOe+GFF9KyZUtWrFjB2rVrSU5OLjO2QNTcQu39R2FfGtwyG1q09zoaERGR2q3kotdy7tb9DxzcDIOer52L1CfdA60S4cPHfWtGhrODW+HzWdDtbriop9fRVFijRo3Izs4u9bmjR4/StGlTYmJi+Pvf/86aNWvO+fWys7Np1aoV+fn5zJ8/v3h7//79mT17NuCbCOTYsWP079+fRYsWkZmZCcCRI0cA3/i4tLQ0AJYsWUJ+fn5A8ffu3ZtPPvmE9PT0084LMGbMGEaOHMnw4cODNhlJzSzUNsyDtN/ClQ9Bxxu9jkZERERKLnot5+boPlj5S7hkYO39P6dOBAyeDjmHfG0RroqKfFdGo2LDbs3K5s2b07dvXzp37swjjzxy2nODBg2ioKCArl278tRTT9GrV69zfr1nn32Wnj17MmDAADp06FC8/eWXX2blypV06dKF7t27s2XLFjp16sQTTzzB1VdfTUJCAg8//DAA9957L5988gk9evRg7dq1p11Fq0j8LVq0YM6cOdx6660kJCQwYsSI4mOGDBlCTk5O0Lo9AjjzqG9vUlKSpaamBv/E+zbA3EFwcW/f+hN1QmN6TRGR2sw5l2ZmNXAV3qpRZTnSS99shlf7wm2/hU63eB1NeFv0E9j+AYxfA83alr9/TfbnhyHtDRi7CloleB1N4DbOhyXjYch/w+U/DujQbdu20bFjxyoKTAKVmprKQw89xGeffVbmPqV9z86WH2vWFbXjmb5xaQ3Pg6FzVaSJiIiECi16HRxffQRbF0O/iSrSwDdWLaa5r2Dzjz0KGyeOwPKn4MKekDjS62jkHDz//PMMHTqUX/0quMsq1JxCrbDAN8NjziEYPg8aNPc6IhERETlFi16fu/xceG8iNPsh9P2Z19GEhuimvlkg96XChje9jiYwH0+Fb7N8E4jUqTn/kp+r+++/n8TExNNub7zxhtdhndWkSZP4+uuvufLKK4N63vCcVqY0K56F9E9gyAxofbnX0YiISA3inBsEvAxEAK+b2fPfe/4u4NTUeznAODP7oiLH1hpa9Prc/fUl+Fc63L0YIuuXt3ft0XUEbEiBj57xjdlrcI6LDFeHPesh7U3f0grnd/Y6mpAyc+ZMr0MIGTWjfN/6J98fr+6j4PK7vY5GRERqEOdcBDATSAYuA+5wzn1/0ap04Goz6wo8C8wJ4Njao/EFuqJWWZk74bPp0Hko/PAar6MJLc7BDb+Bkzmw/GmvoylfYQG89xA0Oh+umex1NBLCwr9Qy9gOi8dB6+6Q/ILX0YiISM3TA/jKzHaZ2UngbeCmkjuY2Woz+5f/4RogvqLH1ipa9LpyzGDpIxBRDwY+53U0oem8DtD7Adg0H75e7XU0Z7f+dfjmSxj0K63zK2cV3oVaXjYsvAsio3zj0tQNQEREgq81sKfE473+bWX5d+D9QI91zo11zqU651IzMjLOIdwQpkWvK2frYtj5MVz7JDRu5XU0oevqRyH2QnjvP6Gw9PWxPHfsAKz4BfywP1x2s9fRSIgL3zFqZrB4PGR+BT9e4vuUTkREJPhcKdtKXdvGOXcNvkLt1IjyCh9rZnPwd5lMSkryZu2cqnZq0et/LIWYZl5HEx6sCD6YDOd3gSvGeB1NaKvXAJJ/DW/f6Zu74JKBXkd0pjWzofAkXP9fvi6bYSwrK4sFCxYwfvz4gI996aWXGDt2LDExMVUQWc0RvoXatv+DbX+CAc9C26u8jkZERGquvcCFJR7HA2cMtHLOdQVeB5LNLDOQY2uN5u18XxdpPHlAXB0YngIR4ftvW7XpcANcej389WXfLRT96HFo/kOvozhnWVlZzJo1q9KF2siRI0OiUCsoKCAyMjR/t0IzqoroeCOMeAs6DPY6EhERqdnWA5c459oC+4DbgTtL7uCcuwj4I3C3mW0P5NhapU0/GPMx5J/wOpLw0rh1jfjHvtrc9ibsXee7Ghlq6sb45lUItvcn+ca9BdP5XSC57ElqJ02axM6dO0lMTGTAgAGcd955LFq0iLy8PG655RamTp3K8ePHGT58OHv37qWwsJCnnnqKgwcPsn//fq655hri4uJYuXJlqecfN24c69ev59tvv2XYsGFMnToVgPXr1zNhwgSOHz9O/fr1+fjjj4mJieGxxx7jww8/xDnHvffey4MPPkibNm1ITU0lLi6O1NRUJk6cyKpVq3jmmWfYv38/u3fvJi4ujl/+8pfcfffdHD9+HIAZM2bQp08fAF544QVSUlKoU6cOycnJ3Hvvvdx2221s2LABgB07dnD77beTlpYWzNYHKlCoOefmAoOBQ2Z2xvyhzjmHb9rh64ETwCgz2xDsQEsJzFesiYiIVCEzK3DOPQB8iG+K/blmtsU5d5//+VeBp4HmwCxfWqTAzJLKOtaTNxIKnIP4JK+jkJoush60Ce56VnKm559/ns2bN7Np0yaWLVvGO++8w7p16zAzhgwZwqeffkpGRgYXXHAB7733HgBHjx4lNjaW6dOns3LlSuLiyl5K4bnnnqNZs2YUFhbSv39//va3v9GhQwdGjBjBwoULueKKKzh27BjR0dHMmTOH9PR0Nm7cSGRkJEeOHCk3/rS0NP7yl78QHR3NiRMnWL58OVFRUezYsYM77riD1NRU3n//fRYvXszatWuJiYnhyJEjNGvWjNjYWDZt2lS8xtuoUaOC1aynqcgVtd8CM4B5ZTyfDFziv/UEZvu/ioiI1AhmthRY+r1tr5a4PwYodQBRaceKiATVWa58VYdly5axbNkyunXrBkBOTg47duygX79+TJw4kccee4zBgwfTr1+/Cp9z0aJFzJkzh4KCAg4cOMDWrVtxztGqVSuuuOIKABo3bgzARx99xH333VfchbFZs/LHwA4ZMoTo6GgA8vPzeeCBB9i0aRMRERFs3769+LyjR48u7qJ56rxjxozhjTfeYPr06SxcuJB169ZV+H0FotxCzcw+dc61OcsuNwHzzMyANc65Js65VmZ2IFhBioiIiIhIaDIzJk+ezE9/+tMznktLS2Pp0qVMnjyZgQMH8vTT5a91l56ezrRp01i/fj1NmzZl1KhR5ObmYma4UiZhKWt7ZGQkRUW+LrC5ubmnPdegQYPi+y+++CItW7bkiy++oKioiKioqLOed+jQoUydOpVrr72W7t2707x583LfU2UEY3p+TT0sIiIiIlKLNGrUiOzsbACuu+465s6dS05ODgD79u3j0KFD7N+/n5iYGEaOHMnEiROLx3WVPLY0x44do0GDBsTGxnLw4EHef9+34kmHDh3Yv38/69evByA7O5uCggIGDhzIq6++SkGBb+mPU10f27RpUzx27A9/+EOZr3f06FFatWpFnTp1SElJobCwEICBAwcyd+5cTpw4cdp5o6KiuO666xg3bhyjR4+uROtVTDAKtYCmHvb32U9q0aJFEF5aRERERESqW/Pmzenbty+dO3dm+fLl3HnnnfTu3ZsuXbowbNgwsrOz+fLLL+nRoweJiYk899xzPPnkkwCMHTuW5ORkrrnmmlLPnZCQQLdu3ejUqRP33HMPffv2BaBevXosXLiQBx98kISEBAYMGEBubi5jxozhoosuomvXriQkJLBgwQIApkyZwoQJE+jXrx8RERFlvpfx48fz5ptv0qtXL7Zv3158tW3QoEEMGTKEpKQkEhMTmTZtWvExd911F845Bg6sumUgnK/HYjk7+bo+/rmMyUT+B1hlZr/zP/4H8KPyuj4mJSVZampqpYIWEZHw4pxLMzPNIlFBypEiUp5t27bRsWNHr8OotaZNm8bRo0d59tlnK3xMad+zs+XHYEzP/yfgAefc2/gmETmq8WkiIiIiIlIT3XLLLezcuZMVK1ZU6etUZHr+3wE/AuKcc3uBKUBdKJ7xaim+qfm/wjc9f9V11BQRERERkRqjZ8+e5OXlnbYtJSWFLl26eBRR+d59991qeZ2KzPp4RznPG3B/0CISEREREZFaYe3atV6HELKCMZmIiIiIiIhUs4rMNSGhoTLfKxVqIiIiIiJhJioqiszMTBVrYcDMyMzMLF6fraKCMZmIiIiIiIhUo/j4ePbu3YvWJg4PUVFRxMfHB3SMCjURERERkTBTt25d2rZt63UYUoXU9VFERERERCTEqFATEREREREJMSrUREREREREQozzaqYY51wG8PU5niYOOByEcGoTtVng1GaBU5sFrqa32cVm1sLrIMKFcqRn1GaBU5sFTm0WmJreXmXmR88KtWBwzqWaWZLXcYQTtVng1GaBU5sFTm0mwaafqcCpzQKnNguc2iwwtbm91PVRREREREQkxKhQExERERERCTHhXqjN8TqAMKQ2C5zaLHBqs8CpzSTY9DMVOLVZ4NRmgVObBabWtldYj1ETERERERGpicL9ipqIiIiIiEiNo0JNREREREQkxIRtoeacG+Sc+4dz7ivn3CSv4wl1zrkLnXMrnXPbnHNbnHMTvI4pHDjnIpxzG51zf/Y6lnDhnGvinHvHOfd3/89bb69jCmXOuYf8v5ObnXO/c85FeR2ThDflx8AoP1aecmRglB8DV9tzZFgWas65CGAmkAxcBtzhnLvM26hCXgHwn2bWEegF3K82q5AJwDavgwgzLwMfmFkHIAG1X5mcc62BnwFJZtYZiABu9zYqCWfKj5Wi/Fh5ypGBUX4MgHJkmBZqQA/gKzPbZWYngbeBmzyOKaSZ2QEz2+C/n43vj0Nrb6MKbc65eOAG4HWvYwkXzrnGwFXA/wKY2Ukzy/I0qNAXCUQ75yKBGGC/x/FIeFN+DJDyY+UoRwZG+bHSanWODNdCrTWwp8TjveiPaoU559oA3YC1HocS6l4CHgWKPI4jnPwAyADe8HeHed0518DroEKVme0DpgH/BA4AR81smbdRSZhTfjwHyo8BeQnlyEAoPwZIOTJ8CzVXyjatM1ABzrmGwB+A/zCzY17HE6qcc4OBQ2aW5nUsYSYSuByYbWbdgOOAxsiUwTnXFN/VjrbABUAD59xIb6OSMKf8WEnKjxWnHFkpyo8BUo4M30JtL3Bhicfx1LJLoZXhnKuLLwnNN7M/eh1PiOsLDHHO7cbXdeha59xb3oYUFvYCe83s1KfR7+BLTFK6fwPSzSzDzPKBPwJ9PI5JwpvyYyUoPwZMOTJwyo+Bq/U5MlwLtfXAJc65ts65evgGFv7J45hCmnPO4esXvc3MpnsdT6gzs8lmFm9mbfD9fK0ws1r1KU5lmNk3wB7n3KX+Tf2BrR6GFOr+CfRyzsX4f0f7o8Hlcm6UHwOk/Bg45cjAKT9WSq3PkZFeB1AZZlbgnHsA+BDfDDBzzWyLx2GFur7A3cCXzrlN/m2Pm9lS70KSGupBYL7/n8RdwGiP4wlZZrbWOfcOsAHfzHMbgTneRiXhTPmxUpQfpbooPwZAORKcmbqui4iIiIiIhJJw7fooIiIiIiJSY6lQExERERERCTEq1EREREREREKMCjUREREREZEQo0JNREREREQkxKhQEwmAc67QObepxG1SEM/dxjm3OVjnExERqS7KjyLBF5brqIl46FszS/Q6CBERkRCj/CgSZLqiJhIEzrndzrlfO+fW+W/t/Nsvds597Jz7m//rRf7tLZ1z7zrnvvDf+vhPFeGce805t8U5t8w5F+3f/2fOua3+87zt0dsUEREJiPKjSOWpUBMJTPT3unaMKPHcMTPrAcwAXvJvmwHMM7OuwHzgFf/2V4BPzCwBuBzY4t9+CTDTzDoBWcBQ//ZJQDf/ee6rmrcmIiJSacqPIkHmzMzrGETChnMux8walrJ9N3Ctme1yztUFvjGz5s65w0ArM8v3bz9gZnHOuQwg3szySpyjDbDczC7xP34MqGtmv3DOfQDkAIuBxWaWU8VvVUREpMKUH0WCT1fURILHyrhf1j6lyStxv5DvxpHeAMwEugNpzjmNLxURkXCh/ChSCSrURIJnRImvn/vvrwZu99+/C/iL//7HwDgA51yEc65xWSd1ztUBLjSzlcCjQBPgjE8tRUREQpTyo0gl6FMHkcBEO+c2lXj8gZmdmoK4vnNuLb4PQO7wb/sZMNc59wiQAYz2b58AzHHO/Tu+TwbHAQfKeM0I4C3nXCzggBfNLCtI70dERCQYlB9Fgkxj1ESCwN8HP8nMDnsdi4iISKhQfhSpPHV9FBERERERCTG6oiYiIiIiIhJidEVNREREREQkxKhQExERERERCTEq1EREREREREKMCjUREREREZEQo0JNREREREQkxPw/5TIqYzL10UAAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 1080x504 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "# Plot our ViT model's loss curves\n",
        "plot_loss_curves(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c370cae-9854-474c-be05-51dabe62c204",
      "metadata": {
        "id": "0c370cae-9854-474c-be05-51dabe62c204"
      },
      "source": [
        "Hmm, it looks like our model's loss curves are all over the place.\n",
        "\n",
        "At least the loss looks like it's heading the right direction but the accuracy curves don't really show much promise.\n",
        "\n",
        "These results are likely because of the difference in data resources and training regime of our ViT model versus the ViT paper.\n",
        "\n",
        "It seems our model is [severely underfitting](learnpytorch.io/04_pytorch_custom_datasets/#82-how-to-deal-with-underfitting) (not achieving the results we'd like it to).\n",
        "\n",
        "How about we see if we can fix that by bringing in a pretrained ViT model?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ac9256f-90fb-4c75-8100-38977886aa80",
      "metadata": {
        "id": "3ac9256f-90fb-4c75-8100-38977886aa80"
      },
      "source": [
        "Woah!\n",
        "\n",
        "Those are some close to textbook looking (really good) loss curves (check out [04. PyTorch Custom Datasets section 8](https://www.learnpytorch.io/04_pytorch_custom_datasets/#8-what-should-an-ideal-loss-curve-look-like) for what an ideal loss curve should look like).\n",
        "\n",
        "That's the power of transfer learning!\n",
        "\n",
        "We managed to get outstanding results with the *same* model architecture, except our custom implementation was trained from scratch (worse performance) and this feature extractor model has the power of pretrained weights from ImageNet behind it.\n",
        "\n",
        "What do you think?\n",
        "\n",
        "Would our feature extractor model improve more if you kept training it?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eab07548-3b1c-43a3-9f8d-02672ef1f47c",
      "metadata": {
        "id": "eab07548-3b1c-43a3-9f8d-02672ef1f47c"
      },
      "source": [
        "### 10.6 Save feature extractor ViT model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0fd00943-01aa-4ef4-b366-3cb859a25b6f",
      "metadata": {
        "id": "0fd00943-01aa-4ef4-b366-3cb859a25b6f",
        "outputId": "2c6c06f2-7bbe-4848-8ff2-d2230f909371",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'save_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-41c4105788bf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m save_model(model=compiled_model,\n\u001b[0m\u001b[1;32m      5\u001b[0m                  \u001b[0mtarget_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"models\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                  model_name=\"08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth\")\n",
            "\u001b[0;31mNameError\u001b[0m: name 'save_model' is not defined"
          ]
        }
      ],
      "source": [
        "# Save the model\n",
        "\n",
        "\n",
        "save_model(model=compiled_model,\n",
        "                 target_dir=\"models\",\n",
        "                 model_name=\"vit_from_scratch.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f52ef12c-b88e-4796-84eb-981491a84334",
      "metadata": {
        "id": "f52ef12c-b88e-4796-84eb-981491a84334",
        "outputId": "7ef88212-52f4-4f39-9e25-8dda3469f749"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pretrained ViT feature extractor model size: 327 MB\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Get the model size in bytes then convert to megabytes\n",
        "pretrained_vit_model_size = Path(\"models/08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly)\n",
        "print(f\"Pretrained ViT feature extractor model size: {pretrained_vit_model_size} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip freeze > requirements.txt\n"
      ],
      "metadata": {
        "id": "kVGn6pMnt_PR"
      },
      "id": "kVGn6pMnt_PR",
      "execution_count": 3,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "110a4ad9b3b23ac6a757cfb6c77f1e39e8d0496598f07ec14a944919c025e818"
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
